## p-values, an intro

### P-values from permutation tests

+ In experimental situations a large p-value (large tail proportion) means that the luck of the randomisation quite often produces group differences as large or even larger than what we've got in our data.
   + A small p-value means that the luck of the randomisation draw hardly ever produces group differences as large as we've got in our data.
   + **Statistical significance does not imply practical significance.**
   + **Statistical significance says nothing about the size of treatment differences.** To estimate the sizes of differences you need confidence intervals.
   
###  `r emo::ji('scream')` p-values `r emo::ji('scream')`

**[The ASA Statement on p-Values: Context, Process, and Purpose](https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108)**

> "Good statistical practice, as an essential component of good scientific practice, emphasizes principles of good study design and conduct, a variety of numerical and graphical summaries of data, understanding of the phenomenon under study, interpretation of results in context, complete reporting and proper logical and quantitative understanding of what data summaries mean. No single index should substitute for scientific reasoning." `r tufte::quote_footer('--- ASA Statement on p-Values')`


**What is a p-Value?**

Informally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value

+ **p-values** can indicate how incompatible the data are with a specified statistical model

+ p-values **do not** measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone

+ Scientific conclusions and business or policy decisions **should not** be based only on whether a p-value passes a specific threshold

+ Proper inference requires **full** reporting and transparency

+ A p-value, or statistical significance, does **not** measure the size of an effect or the importance of a result

+ By itself, a p-value does **not** provide a good measure of evidence regarding a model or hypothesis


### Type I and Type II errors


**Type I** error (false positive): declare a difference (i.e., reject $H_0$) when there is no difference (i.e. $H_0$ is true). Risk of the Type I error is determined by the *level of significance* (which we set!) (i.e., $\alpha =\text{ P(Type I error)} = \text{P(false positive)}$.

![](https://raw.githubusercontent.com/allisonhorst/stats-illustrations/master/other-stats-artwork/type_1_errors.png)

**Type II** error (false negative): difference not declared (i.e., $H_0$ not rejected) when there is a difference (i.e., $H_0$ is false). Let $\beta =$ P(do not reject $H_0$ when $H_0$ is false); so, $1-\beta$ = P(reject $H_0$ when $H_0$ is false) = P(a true positive), which is the statistical **power** of the test.

![](https://raw.githubusercontent.com/allisonhorst/stats-illustrations/master/other-stats-artwork/type_2_errors.png) 


**Each** time we carry out a hypothesis test the probability we get a false positive result (type I error) is given by $\alpha$ (the *level of significance* we choose).

When we have **multiple comparisons** to make we should then control the **Type I** error rate across the entire *family* of tests under consideration, i.e., control the Family-Wise Error Rate (FWER); this ensures that the risk of making at least one **Type I** error among the family of comparisons in the experiment is $\alpha$.


|State of Nature  | Don't reject $H_0$ | reject $H_0$ |
|---              |---                |---            |
| $H_0$ is true |  `r emo::ji("check")` | Type I error  |
| $H_0$ is false  | Type II error  | `r emo::ji("check")` |
