[["index.html", "Advanced Biological Data Analysis Nau mai, haere mai. Welcome to BIOSCI 738", " Advanced Biological Data Analysis University of Auckland Lecturer: Dr. C. M. Jones-Todd Nau mai, haere mai. Welcome to BIOSCI 738 Artwork by @allison_horst With thanks to Prof. Chris Triggs, Dr Kathy Ruggiero, and Prof. James Russell who all freely gave me their previous iterations of the course and told me to do what I wanted with their notes. I did, and they may well find sections of this material hauntingly familiar! Credit also to all the other fantastic people who've shared materials or have made notes freely available online. I have endeavoured to link all materials, and more, where appropriate. "],["useful-information-to-set-you-up-for-your-semester.html", "Useful information to set you up for your semester", " Useful information to set you up for your semester Course outline This is a postgraduate course geared towards students of biology, ecology, and environmental science. It is suited to students with an interest in (bio)statistics who would like to equip themselves with the know-how to be able to correctly prepare experiments, analyse data, interpret their results and draw valid conclusions. The statistical concepts and methods taught in this course will provide students with the tools to make and evaluate scientific discoveries as well as propose and justify decisions based on data. The course builds on assumed knowledge of some fundamental statistical concepts. It is expected that students are comfortable with the statistical content covered in a typical stage 2 biostatistics course (e.g., BIOSCI220). This course will use the programming language R (through RStudio) and students are expected to be familiar with the basics of data import, manipulation, and visualisation using R. If you are unfamiliar with R it is expected that you will prepare accordingly prior to lectures and labs. The course will also introduce you to version control (via git and GitHub); no previous experience with these systems is expected. Learning Outcomes By the end of this course you will be able to: Discuss and critically evaluate the provenance of data and create informative visualisations. Develop and demonstrate effective Rprogramming and version control skills. Create and maintain a reproducible project directory. Describe, analyse and interpret different types of experimental designs identifying the potential sources of variation. Formulate an appropriate hypothesis associated with an experimental design. Perform, interpret, and critique multivariate data techniques. Communicate statistical concepts and experimental outcomes clearly using language appropriate for both a scientific and non-scientific audience. Perform, interpret, and critique a range of statistical regression techniques. Course summary Lectures this semester are in weeks (2--11) on Mondays 2-4pm. Labs are held on Fridays (weeks 2--11) 2--3pm Office hours: Book an online one-to-one session or contact me directly at c.jonestodd@auckland.ac.nz Each week some lecture material will be recorded and uploaded to CANVAS. It is expected that you will have watched these prior to coming along to the in-person lecture "],["module-1.html", "Module 1", " Module 1 Learning objectives Define the difference between R and RStudio Explain what an R function is; describe what an argument to an R function is Explain what an R package is; distinguish between the functions install.packages() and library() Use the appropriate R function to read in a data file Explain the importance of reproducibility in terms of scientific research Use the functionality offered by git and GitHub through RStudio Carry out and interpret the outputs of basic exploratory data analysis using in-built R functions Create and communicate informative data visualisations using R Discuss and critique data visualisations Define and discuss MƒÅori Data Sovereignty principles Define data sovereignty and explain this in relation to a researcher's obligation when collecting, displaying, and analysing data Other resources R for Biologists R for Beginners Happy Git and GitHub for the useR Why data sovereignty matters Indigenous Data Sovereignty and Policy Principles of MƒÅori Data Sovereignty Good data practices for Indigenous data sovereignty and governance. Ten Simple Rules for Better Figures Elegant Graphics for Data Analysis Using ggplot2 to communicate your results Graphs in the media "],["r-and-rstudio.html", "R and RStudio", " R and RStudio &quot;If R were an airplane, RStudio would be the airport...&quot; --- Julie Lowndes, Introduction to RStudio Awesomeness Alternatively, R is the pheromone to RStudio's PDA. &lt;/&gt; R is a language, specifically, a programming language; it's the way you can speak to your computer to ask it to carry out certain computations. RStudio is an integrated development environment (IDE). This means it is basically an interface, albeit a fancy one, that makes it easier to communicate with your computer in the language R. The main benefit is the additional features it has that enable you to more efficiently speak R. Note R and RStudio are two different pieces of software; for this course you are expected to download both. As you'd expect, the PDA depends on the pheromones (i.e., RStudio depends on R) so you have to download R to use RStudio! Why use R It's free It's open source A general-purpose of programming language Written by statisticians (here in Auckland!) It's available for all operating systems (Windows, Linux, and Mac) There is a huge online support network It's extremely flexible; if you can code it you can do it! 15,000+ packages available! ... Why use RStudio It speaks nicely to R Tab completion Debugging capabilities There is a huge online support network Offers many other features and tools to make your workflow with R easier It facilitates reproducibility ... Installing R and RStudio Software Carpentry install R &amp; RStudio Mac Software Carpentry install R &amp; RStudio Windows NOTE: RStudio depends on R so there is an order you should follow when you download these software! Download and install R by following these instructions. Make sure you choose the correct operating system. Download and install RStudio by going here choosing RStudio Desktop Open Source License Free and following instructions. Check all is working Open up RStudio from your computer menu, the icon will look something like this (DO NOT use this icon , this is a link to R and will only open a very basic interface) Wait a little and you should see RStudio open up to something similar to the screenshot below Pay close attention to the notes in the screenshot and familiarise yourself with the terms. Finally, in the Console next to the prompt type 1:10 and press enter on your keyboard. Your computer should say something back you (in the Console)! What do you think you were asking it to do? Does the output make sense?1 Getting started As in step 3. above open up RStudio from your computer menu, the icon will look something like this . Using the diagram above identify the different panes: Console where you directly type command in and communicate with your computer (via the language R). Environment pane Files pane Some terminology Running code: the act of telling R to perform an act by giving it commands in the console. Objects: where values are saved in (see later for creating an object. Script: a text file containing a set of commands and comments. Comments: notes written within a Script to better document/explain what's happening R errors üò± data &lt;- read.csv(&quot;data_file_not_in_my_working_directory.csv&quot;) ## Error in file(file, &quot;rt&quot;): cannot open the connection library(some_library_I_have_not_installed) some_function_I_spelled_worng(x = x) ## Error in some_function_I_spelled_worng(x = x): could not find function &quot;some_function_I_spelled_worng&quot; an_object_I_have_not_created ## Error in eval(expr, envir, enclos): object &#39;an_object_I_have_not_created&#39; not found What do you think the issues are here üòâ R Scripts (a .r file) Go File &gt; New File &gt; R Script to open up a new Script If you had only three panes showing before, a new (fourth) pane should open up in the top left of RStudio. This file will have a .r extension and is where you can write, edit, and save the R commands you write. It's a dedicated text editor for your R code (very useful if you want to save your code to run at a later date). The main difference between typing your code into a Script vs Console is that you edit it and save it for later! Remember though the Console is the pane where you communicate with your computer so all code you write will have to be Run here. There are two ways of running a line of code you've written in your Script Ensure your cursor is on the line of code you want to run, hold down Ctrl and press Enter. Ensure your cursor is on the line of code you want to run, then use your mouse to click the Run button (it has a green arrow next to it) on the top right of the Script pane. Type 1:10 in your Script and practice running this line of code using both methods above. Note that if you've Run the code successfully then your computer will speak back to you each time via the Console Writing Comments Comments are notes to yourself (future or present) or to someone else and are, typically, written interspersed in your code. Now, the comments you write will typically be in a language your computer doesn't understand (e.g., English). So that you can write yourself notes in your Script you need to tell your computer using the R language to ignore them. To do this precede any note you write with #, see below. The # is R for ignore anything after this character. ## IGNORE ME ## I&#39;m a comment ## I repeat I&#39;m a comment ## I am not a cat ## OK let&#39;s run some code 2 + 2 ## [1] 4 ## Hmm maybe I should check this ## @kareem_carr ;-) Now remember when you want to leave your R session you'll need to Save your Script to use it again. To do this go File &gt; Save As and name your file what you wish (remember too to choose a relevant folder on your computer, or as recommended use the .Rproj set-up as above). Change RStudio's appearance Go to Tools &gt; Global Options &gt; Appearance You should have seen the numbers 1 to 10 printed out as a sequence.‚Ü© "],["reproducible-research.html", "Reproducible research", " Reproducible research &quot;Reproducibility, also known as replicability and repeatability, is a major principle underpinning the scientific method. For the findings of a study to be reproducible means that results obtained by an experiment or an observational study or in a statistical analysis of a data set should be achieved again with a high degree of reliability when the study is replicated. ... With a narrower scope, reproducibility has been introduced in computational sciences: Any results should be documented by making all data and code available in such a way that the computations can be executed again with identical results.&quot; --- Reprodicibility, Wikipedia Project-oriented workflow: good practice Properly managing data and other research outputs should starts at the beginning of your project. It should then and continue throughout! To setup an RStudio .RProj project go to Project (top right) &gt; New Project &gt; Create Project Best-practice tips Be organised: set up each each assignment/university course as a project Ensure your project is self-contained. A project is a folder that should contain all the relevant files and all paths should be relative to that project Always apply the 5 Cs: be Clear, Concise, Consistent, Correct, and Conformant. Naming files should be descriptive and consistent! Include a README file that summarises your project's structure Provide a detailed description of the data, data source(s), and how it will be used. Describe all the steps of data preprocessing. Provide a description and information about each new version of the data. Provide details about the software/code that is used for preprocessing the data. Include brief examples of how your code works. Generate a list of all scripts, how to run them, and in what order. When structuring your project directory consider the following: Put code and data in separate subdirectories. Plan to separate raw data from processed data. An example of a well-structured directory ProjectName/ README.MD Dataset/ Raw Data/ Processed Data/ Analysis (or Code or Scripts)/ Data cleaning/ Data preprocessing/ Output/ Graphs Tables Always start with a clean workspace Why? So your ex (code) can't come and mess up your life! To ensure this go to Tools &gt; Global Options and uncheck the highlighted options. ‚ö†Ô∏èWarning‚ö†Ô∏è If the first line of your R script is setwd(&quot;C:\\Users\\jenny\\path\\that\\only\\I\\have&quot;) I will come into your office and SET YOUR COMPUTER ON FIRE üî•. If the first line of your R script is rm(list = ls()) I will come into your office and SET YOUR COMPUTER ON FIRE üî•. --- Jenny Bryan, Tidyverse blog, workflow vs script Why? Because, this is not reproducible, does NOT create a fresh R process, makes your script vulnerable, and it will come back to bite you. Version control with git and GitHub Git is a version control system that manages the evolution of a set of files, called a repository (repo), in a structured way (think of Word's Track Changes). With Git you can track the changes you make to your project/code. You will always have a record of what you've worked on and can easily revert back to an older version if you make a mistake. GitHub is a hosting service that provides a home for your Git-based projects on the internet (think of Dropbox). In addition, GitHub offers functionality to use Git online via an easy-to-use interface. Both Git and GitHub can very easily be configured to work with RStudio. Below are some key terms you will undoubtedly hear when delving into the git--GitHub world. Repository (already mentioned) This where the work happens--think of it as your project folder. It should contain all of your project‚Äôs files etc. Cloning A repository on GitHub is stored remotely in the cloud. To create a local copy of this repository you can clone it and use Git to sync the two. Committing and pushing are how you can add the changes you made on your local machine to the remote repository in GitHub. You can make a commit when you have made milestone worthy changes to your project. You should also add a helpful commit message to remind future you, or your teammates, what the changes you made were (e.g., fixed the bug in my_function). Setting up Register an account with GitHub https://github.com. Choose the free option! Make sure you've got the latest version of R R.version.string ## [1] &quot;R version 4.1.2 (2021-11-01)&quot; Upgrade RStudio to the new preview version (optional) Install git: follow these instructions Get started Cloning a repository from GitHub using RStudio In GitHub, navigate to the Code tab of the repository and on the right side of the screen, click Clone or download. Click the Copy to clipboard icon to the right of the repository URL Open RStudio in your local environment Click File, New Project, Version Control, Git Paste the repository URL and enter TAB to move to the Project directory name field. 6.Click Create Project. Your Files pane should now look similar to this Commiting and pushing changes Open a file from your project directory (here I've opened the file README.md). Note that the Git pane (top right) is empty Make a change to your file and save. Now note that the Git pane (top right) is not empty: Check this file in the Git tab (it is now staged for commit). Click the Commit button. A new pane will open. Changes made to the file will be highlighted (additions in green and deletions in red). Now write your self an informative message in the top right of this pop-up: Click the Commit button below the message you've just written. A new pop up will let you know how things are going! You can then close both popups. Now you'll see RStudio has left you a little message in the Git tab, something similar to Your branch is ahead of origin/master by 1 commit. This means that you've made and commited your changes locally (i.e., on your computer) but you are yet to push these changes to GitHub. To push to GitHub press the Push button, A new pop up will let you know how things are going! You can then close this once it gives you the option to. "],["your-data-and-r.html", "Your data and R", " Your data and R Starting out with tidyverse Artwork by @allison_horst 'tidyverse' is a collection of R packages that all share underlying design philosophy, grammar, and data structures. They are specifically designed to make data wrangling, manipulation, visualisation, and analysis simpler. To install all the packages that belong to the tidyverse run ## request (download) the tidyverse packages from the centralised library ## you only need to do this once per computer :-) install.packages(&quot;tidyverse&quot;) To tell your computer to access the tidyverse functionality in your session run (Note you'll have to do this each time you start up an R session): ## Get the tidyverse packages from our local library library(tidyverse) ## alternatively (does essentially the same thing, ## but checks if the package is already loaded first) require(tidyverse) Note: If you want to call a function from a specific package then you can use the syntax package_name::function_name. Here the :: specifically tells your computer to use the function function_name from the package package_name. This is useful if you only want to use a single function from a package (i.e., you don't want to load them all), or want to explicitly tell your computer which package to load the function from if there are multiple functions with the same name. See readr::read_csv below for an example of using this syntax. You will also see this in a few places throughout the rest of the notes. Reading in data from a .csv file In this section we will be using dataset referring to the age and shell lengths of different P\\(\\overline{\\text{a}}\\)ua. The dataset contains the following variables Age of P\\(\\overline{\\text{a}}\\)ua in years (calculated from counting rings in the cone) Length of P\\(\\overline{\\text{a}}\\)ua shell in centimeters Species of P\\(\\overline{\\text{a}}\\)ua: Haliotis iris (typically found in NZ) and Haliotis australis (less commonly found in NZ) To read these data into RStudio you have two options: Download the paua.csv file from my online repository into a folder on your local computer. In the Environment pane click Import Dataset &gt; ** From Text (readr)** &gt; Browse &gt; Choose your file, remembering which folder you downloaded it to. this is where .Rproj is useful &gt; Another pane should pop up, check the data looks as you might expect &gt; Import Or, you can download directly using the URL (or a any other given path) paua &lt;- readr::read_csv(&quot;https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/paua.csv&quot;) Explore your data Let's have a look at your data in the Console paua ## # A tibble: 60 √ó 3 ## Species Length Age ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Haliotis iris 1.8 1.50 ## 2 Haliotis australis 5.4 11.9 ## 3 Haliotis australis 4.8 5.42 ## 4 Haliotis iris 5.75 4.50 ## 5 Haliotis iris 5.65 5.50 ## 6 Haliotis iris 2.8 2.50 ## 7 Haliotis australis 5.9 6.49 ## 8 Haliotis iris 3.75 5.00 ## 9 Haliotis australis 7.2 8.56 ## 10 Haliotis iris 4.25 5.50 ## # ‚Ä¶ with 50 more rows Use the glimpse() command for an alternative view glimpse(paua) ## Rows: 60 ## Columns: 3 ## $ Species &lt;chr&gt; &quot;Haliotis iris&quot;, &quot;Haliotis australis&quot;, &quot;Haliotis australis&quot;, &quot;‚Ä¶ ## $ Length &lt;dbl&gt; 1.80, 5.40, 4.80, 5.75, 5.65, 2.80, 5.90, 3.75, 7.20, 4.25, 6.‚Ä¶ ## $ Age &lt;dbl&gt; 1.497884, 11.877010, 5.416991, 4.497799, 5.500789, 2.500972, 6‚Ä¶ The pipe operator %&gt;% A nifty tidyverse tool is called the pipe operator %&gt;%. The pipe operator allows us to combine multiple operations in R into a single sequential chain of actions. Say you would like to perform a hypothetical sequence of operations on a hypothetical data frame x using hypothetical functions f(), g(), and h(): Take x then Use x as an input to a function f() then Use the output of this as an input to a function g() then Use the output of this as an input to a function h() So to calculate the mean Age of each Species in the paua dataset we would use paua %&gt;% group_by(Species) %&gt;% summarize(mean_age = mean(Age)) ## # A tibble: 2 √ó 2 ## Species mean_age ## &lt;chr&gt; &lt;dbl&gt; ## 1 Haliotis australis 7.55 ## 2 Haliotis iris 4.40 You would read the sequence above as: Take the paua data.frame then Use this and apply the group_by() function to group by Species Use this output and apply the summarize() function to calculate the mean Age of each group (Species), calling the resulting number mean_age Or to describe my daily routine... I %&gt;% wake_up(time = &quot;later than I should&quot;) %&gt;% give(who = &quot;Watson&quot; , what = &quot;medication&quot;) %&gt;% make(who= &quot;myself&quot;, what = &quot;coffee&quot;) %&gt;% drink() %&gt;% try(remember_what_I_have_on(date = &quot;today&quot;)) Have a go at writing your own! "],["data-wrangling.html", "Data wrangling", " Data wrangling Illustrations from the Openscapes blog Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst &quot;Tidy datasets are all alike, but every messy dataset is messy in its own way.&quot; --- Hadley Wickham &quot;Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets.&quot; --- Hadley Wickham, Tidy data There are three interrelated rules which make a dataset tidy: Each variable must have its own column Each observation must have its own row Each value must have its own cell Why ensure that your data is tidy? 1) Consistency: using a consistent format aids learning and reproducibility, and 2) simplicity: it's a format that is well understood by R! Introuducing the Palmer penguins These data are included in the R package palmerpenguins; to access the data and follow along with this, and following, sections you should first install the package using install.packages(&quot;palmerpenguins&quot;) Then remember to load the package, using library(palmerpenguins), into your workspace each new R session in which you wish to use these data. Let's first have a look at the data: library(palmerpenguins) ## contains some nice penguin data penguins ## # A tibble: 344 √ó 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen NA NA NA NA ## 5 Adelie Torgersen 36.7 19.3 193 3450 ## 6 Adelie Torgersen 39.3 20.6 190 3650 ## 7 Adelie Torgersen 38.9 17.8 181 3625 ## 8 Adelie Torgersen 39.2 19.6 195 4675 ## 9 Adelie Torgersen 34.1 18.1 193 3475 ## 10 Adelie Torgersen 42 20.2 190 4250 ## # ‚Ä¶ with 334 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; So, what does this show us? A tibble: 344 x 8: A tibble is a specific kind of data frame in R. The penguin dataset has 344 rows (i.e., 344 different observations). Here, each observation corresponds to a penguin. 8 columns corresponding to 3 variables describing each observation. species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, sex, and year are the different variables of this dataset. We then have a preview of the first 10 rows of observations corresponding to the first 10 penguins. ``... with 334 more rows indicates there are 334 more rows to see, but these have not been printed (likely as it would clog our screen) To learn more about the penguins read the paper that talks all about the data collection. Common dataframe manipulations in the tidyverse, using dplyr and tidyr Even from these first few rows of data we can see that there are some NA values. Let's count the number of NAs. Remember the %&gt;% operator? Here we're going to be introduced to a few new things the apply() function, the is.na() function, and how R deals with logical values! library(tidyverse) penguins %&gt;% apply(.,2,is.na) %&gt;% apply(.,2,sum) ## species island bill_length_mm bill_depth_mm ## 0 0 2 2 ## flipper_length_mm body_mass_g sex year ## 2 2 11 0 There's lot going on in that code! Let's break it down Take penguins then Use penguins as an input to the apply() function (this is specified as the first argument using the .) Now the apply() function takes 3 arguments: the data object you want it to apply something to (in our case penguins) the margin you want to apply that something to; 1 stands for rows and 2 stands for columns, and the function you want it to apply (in our case is.na()). So the second line of code is asking R to apply the is.na() function over the columns of penguins is.na() asks for each value it's fed is it an NA value; it returns a TRUE if so and a FALSE otherwise The output from the first apply() is then fed to the second apply() (using the .). The sum() function then add them up! R treats a TRUE as a 1 and a FALSE as a 0. So how many NAs do you think there are! Doesn't help much. To Now we know there are NA values throughout the data let's remove then and create a new NA free version called penguins_nafree. There is a really handy tidyverse (dplyr) function for this! penguins_nafree &lt;- penguins %&gt;% drop_na() penguins_nafree ## # A tibble: 333 √ó 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen 36.7 19.3 193 3450 ## 5 Adelie Torgersen 39.3 20.6 190 3650 ## 6 Adelie Torgersen 38.9 17.8 181 3625 ## 7 Adelie Torgersen 39.2 19.6 195 4675 ## 8 Adelie Torgersen 41.1 17.6 182 3200 ## 9 Adelie Torgersen 38.6 21.2 191 3800 ## 10 Adelie Torgersen 34.6 21.1 198 4400 ## # ‚Ä¶ with 323 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; Below are some other useful manipulation functions; have a look at the outputs and run them yourselves and see if you can work out what they're doing. filter(penguins_nafree, island == &quot;Torgersen&quot; ) ## # A tibble: 47 √ó 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen 36.7 19.3 193 3450 ## 5 Adelie Torgersen 39.3 20.6 190 3650 ## 6 Adelie Torgersen 38.9 17.8 181 3625 ## 7 Adelie Torgersen 39.2 19.6 195 4675 ## 8 Adelie Torgersen 41.1 17.6 182 3200 ## 9 Adelie Torgersen 38.6 21.2 191 3800 ## 10 Adelie Torgersen 34.6 21.1 198 4400 ## # ‚Ä¶ with 37 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; summarise(penguins_nafree, avgerage_bill_length = mean(bill_length_mm)) ## # A tibble: 1 √ó 1 ## avgerage_bill_length ## &lt;dbl&gt; ## 1 44.0 group_by(penguins_nafree, species) ## # A tibble: 333 √ó 8 ## # Groups: species [3] ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen 36.7 19.3 193 3450 ## 5 Adelie Torgersen 39.3 20.6 190 3650 ## 6 Adelie Torgersen 38.9 17.8 181 3625 ## 7 Adelie Torgersen 39.2 19.6 195 4675 ## 8 Adelie Torgersen 41.1 17.6 182 3200 ## 9 Adelie Torgersen 38.6 21.2 191 3800 ## 10 Adelie Torgersen 34.6 21.1 198 4400 ## # ‚Ä¶ with 323 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; Often we want to summarise variables by different groups (factors). Below we Take the penguins_nafree data then Use this and apply the group_by() function to group by species Use this output and apply the summarize() function to calculate the mean (using (mean()) bill length (bill_length_mm) of each group (species), calling the resulting number avgerage_bill_length penguins_nafree %&gt;% group_by(species) %&gt;% summarise(avgerage_bill_length = mean(bill_length_mm)) ## # A tibble: 3 √ó 2 ## species avgerage_bill_length ## &lt;fct&gt; &lt;dbl&gt; ## 1 Adelie 38.8 ## 2 Chinstrap 48.8 ## 3 Gentoo 47.6 We can also group by multiple factors, for example, penguins_nafree %&gt;% group_by(island,species) %&gt;% summarise(avgerage_bill_length = mean(bill_length_mm)) ## # A tibble: 5 √ó 3 ## # Groups: island [3] ## island species avgerage_bill_length ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 Biscoe Adelie 39.0 ## 2 Biscoe Gentoo 47.6 ## 3 Dream Adelie 38.5 ## 4 Dream Chinstrap 48.8 ## 5 Torgersen Adelie 39.0 What about getting more complicated? I suggest you run the code below one pipe at a time to work out what each function is doing and data it is acting on. penguins_nafree %&gt;% filter(., sex != &quot;male&quot;) %&gt;% dplyr::select(c(&quot;species&quot;, &quot;island&quot;, &quot;body_mass_g&quot;)) %&gt;% group_by(species, island) %&gt;% summarise(total_mass_g = sum(body_mass_g)) %&gt;% pivot_wider(names_from = c(island), values_from = total_mass_g) ## # A tibble: 3 √ó 4 ## # Groups: species [3] ## species Biscoe Dream Torgersen ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie 74125 90300 81500 ## 2 Chinstrap NA 119925 NA ## 3 Gentoo 271425 NA NA "],["data-visualiation-data-viz.html", "Data visualiation (data viz)", " Data visualiation (data viz) &quot;...have obligations in that we have a great deal of power over how people ultimately make use of data, both in the patterns they see and the conclusions they draw.&quot; --- Michael Correll, Ethical Dimensions of Visualization Research &quot;Clutter and confusion are not attributes of data - they are shortcomings of design.&quot; --- Edward Tufte Exploratory plots Exploratory plots are just for you, they focus solely on data exploration, don't have to look pretty, just needs to get to the point, explore and discover new data facets, and help formulate new questions. For example, Explanatory plots Explanatory plots are mainly for others. These are the most common kind of graph used in scientific publications. They should have a clear purpose, be designed for the audience, be easy to read (this covers a lot of things), not distort the data, help guide the reader to a particular conclusion, answer a specific question, and support an outlined decision. For example, Plots by Cedric Scherer and mentioned on this blog Ten Simple Rules for Better Figures &quot;Scientific visualization is classically defined as the process of graphically displaying scientific data. However, this process is far from direct or automatic. There are so many different ways to represent the same data: scatter plots, linear plots, bar plots, and pie charts, to name just a few. Furthermore, the same data, using the same type of plot, may be perceived very differently depending on who is looking at the figure. A more accurate definition for scientific visualization would be a graphical interface between people and data.&quot; --- Nicolas P. Rougier, Michael Droettboom, Philip E. Bourne, Ten Simple Rules for Better Figures &quot;message and readability of the figure is the most important aspect while beauty is only an option&quot; --- Nicolas P. Rougier, Michael Droettboom, Philip E. Bourne, Ten Simple Rules for Better Figures Know Your Audience Identify Your Message Adapt the Figure to the Support Medium Captions Are Not Optional Do Not Trust the Defaults Use Color Effectively Do Not Mislead the Reader There are formulas to measure how misleading a graph is! Avoid Chartjunk Message Trumps Beauty Get the Right Tool I'm an advocate for R üòâ ggplot2 ggplot2 is an R package for producing statistical, or data, graphics; it has an underlying grammar based on the Grammar of Graphics Every ggplot2 plot has three key components: data, A set of aesthetic mappings between variables in the data and visual properties, and At least one layer which describes how to render each observation. Layers are usually created with a geom function. Examples Scatter plot using geom_point() ggplot(penguins,aes(x = body_mass_g, y = flipper_length_mm)) + ## data &amp; aesthetics geom_point() + ## geom geom_smooth(method = &#39;lm&#39;, se = FALSE) ## statistics (linear regression line) Boxplot using geom_boxplot() ggplot(penguins,aes(x = species, y = flipper_length_mm)) + ## data &amp; aesthetics geom_boxplot() + ## geom ggtitle(&quot;Flipper length (mm) by species&quot;) + ylab(&quot;Flipper length (mm)&quot;) + xlab(&quot;Species&quot;) + theme_dark() ## theme Scatter plot specifying color using geom_point() ggplot(penguins,aes(x = body_mass_g, y = flipper_length_mm, color = species)) + ## data and aesthetics geom_point() + ## geom geom_smooth(method = &#39;lm&#39;, se = FALSE) ## statistic (linear regression line without intervals) The Good, the Bad, and the Ugly... box &lt;- ggplot(penguins,aes(x = species, y = flipper_length_mm)) + ## data &amp; aesthetics geom_boxplot() + ## geom ggtitle(&quot;Flipper length (mm) by species&quot;) + ylab(&quot;Flipper length (mm)&quot;) + xlab(&quot;Species&quot;) + theme_dark() ## theme box jitter &lt;- ggplot(penguins,aes(x = species, y = flipper_length_mm)) + ## data &amp; aesthetics geom_jitter() + ## geom ggtitle(&quot;Flipper length (mm) by species&quot;) + ylab(&quot;Flipper length (mm)&quot;) + xlab(&quot;Species&quot;) + theme_dark() ## theme jitter violin &lt;- ggplot(penguins,aes(x = species, y = flipper_length_mm)) + ## data &amp; aesthetics geom_violin() + ## geom ggtitle(&quot;Flipper length (mm) by species&quot;) + ylab(&quot;Flipper length (mm)&quot;) + xlab(&quot;Species&quot;) + theme_dark() ## theme violin ggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm)) + geom_point() + geom_smooth(method = &quot;lm&quot;, col = &quot;blue&quot;, se = FALSE) ggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm, col = species)) + geom_point(size = 2, alpha = 0.5) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + facet_grid(~ sex) + theme_bw() + labs(title = &quot;Flipper Length and Body Mass, by Sex &amp; Species&quot;, subtitle = paste0(nrow(penguins), &quot; of the Palmer Penguins&quot;), x = &quot;Body Mass (g)&quot;, y = &quot;Flipper Length (mm)&quot;) penguins_nafree &lt;- penguins %&gt;% drop_na() ggplot(penguins_nafree, aes(x = body_mass_g, y = flipper_length_mm, col = species)) + geom_point(size = 2, alpha = 0.5) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + facet_grid(~ sex) + theme_bw() + labs(title = &quot;Flipper Length and Body Mass, by Sex &amp; Species&quot;, subtitle = paste0(nrow(penguins_nafree), &quot; of the Palmer Penguins&quot;), x = &quot;Body Mass (g)&quot;, y = &quot;Flipper Length (mm)&quot;) "],["mƒÅori-data-sovereignty-principles.html", "MƒÅori Data Sovereignty principles", " MƒÅori Data Sovereignty principles ''Data sovereignty is the idea that data are subject to the laws and governance structures within the nation it is collected'' &quot;MƒÅori Data Sovereignty has emerged as a critical policy issue as Aotearoa New Zealand develops world-leading linked administrative data resources.&quot; --- Andrew Sporle, Maui Hudson, Kiri West. Chapter 5, Indigenous Data Sovereignty and Policy &quot;For Indigenous peoples, historical encounters with statistics have been fraught, and none more so than when involving official data produced as part of colonial attempts at statecraft.&quot; --- Lovett, R., Lee, V., Kukutai, T., Cormack, D., Rainie, S.C. and Walker, J., 2019. Good data practices for Indigenous data sovereignty and governance. Good data, pp.26-36. &quot;MƒÅori Data Sovereignty has emerged as a critical policy issue as Aotearoa New Zealand develops world-leading linked administrative data resources.&quot; --- Andrew Sporle, Maui Hudson, Kiri West. Chapter 5, Indigenous Data Sovereignty and Policy ‚ÄúMƒÅori data refers to data produced by MƒÅori or that is about MƒÅori and the environments we have relationships with.&quot; --- Te Mana Raraunga Charter Data is a ‚Äúpotential taonga, something precious that needs to be maintained, in relation to its utility‚Äù --- Dr W. Edwards, TMR website Te Tiriti o Waitangi/Treaty of Waitangi obliges the Government to actively protect taonga, consult with MƒÅori in respect of taonga, give effect to the principle of partnership and recognize MƒÅori rangatiratanga over taonga. Factors that relate to how communities might recognize the taonga nature of any dataset include provenance of the data: does the dataset come from a significant MƒÅori source? opportunity for the data: could the dataset support MƒÅori aspirations for their people or their whenua (land)? utility of the data: does the dataset have multiple uses? MƒÅori Data Sovereignty principles to inform the recognition of MƒÅori rights and interests in data, and the ethical use of data to enhance MƒÅori well-being: Rangatiratanga, authority MƒÅori have an inherent right to exercise control over MƒÅori data and MƒÅori data ecosystems. This right includes, but is not limited to, the creation, collection, access, analysis, interpretation, management, security, dissemination, use and reuse of MƒÅori data. Decisions about the physical and virtual storage of MƒÅori data shall enhance control for current and future generations. Whenever possible, MƒÅori data shall be stored in Aotearoa New Zealand. MƒÅori have the right to data that is relevant and empowers sustainable self-determination and effective self-governance Whakapapa, relationships All data has a whakapapa (genealogy). Accurate metadata should, at minimum, provide information about the provenance of the data, the purpose(s) for its collection, the context of its collection, and the parties involved. The ability to disaggregate MƒÅori data increases its relevance for MƒÅori communities and iwi. MƒÅori data shall be collected and coded using categories that prioritise MƒÅori needs and aspirations. Current decision-making over data can have long-term consequences, good and bad, for future generations of MƒÅori. A key goal of MƒÅori data governance should be to protect against future harm. Whanaungatanga, obligations Individuals‚Äô rights (including privacy rights), risks and benefits in relation to data need to be balanced with those of the groups of which they are a part. In some contexts, collective MƒÅori rights will prevail over those of individuals. Individuals and organisations responsible for the creation, collection, analysis, management, access, security or dissemination of MƒÅori data are accountable to the communities, groups and individuals from whom the data derive Kotahitanga, collective benefit Data ecosystems shall be designed and function in ways that enable MƒÅori to derive individual and collective benefit. Build capacity. MƒÅori Data Sovereignty librarys the development of a MƒÅori workforce to enable the creation, collection, management, security, governance and application of data. Connections between MƒÅori and other Indigenous peoples shall be supported to enable the sharing of strategies, resources and ideas in relation to data, and the attainment of common goals. Manaakitanga, reciprocity The collection, use and interpretation of data shall uphold the dignity of MƒÅori communities, groups and individuals. Data analysis that stigmatises or blames MƒÅori can result in collective and individual harm and should be actively avoided. Free, prior and informed consent shall underpin the collection and use of all data from or about MƒÅori. Less defined types of consent shall be balanced by stronger governance arrangements. Kaitiakitanga, guardianship MƒÅori data shall be stored and transferred in such a way that it enables and reinforces the capacity of MƒÅori to exercise kaitiakitanga over MƒÅori data. Ethics. Tikanga, kawa (protocols) and mƒÅtauranga (knowledge) shall underpin the protection, access and use of MƒÅori data. MƒÅori shall decide which MƒÅori data shall be controlled (tapu) or open (noa) access. "],["module-2.html", "Module 2", " Module 2 Learning objectives List the aims, write out the appropriate null and alternative hypothesis using statistical notation for a permutation (randomization) test Write R code to carry out a permutation test List the aims, write out the appropriate null and alternative hypothesis using statistical notation for a bootstrap procedure Write R code to carry out bootstapping Carry out and interpret tests for the existence of relationships between explanatory variables and the response in a linear model Write R code to fit a linear model with a single continuous explanatory variable Write R code to fit a linear model with a continuous explanatory variable and a factor explanatory variable Interpret estimated effects with reference to confidence intervals from linear regression models. Specifically the interpretation of the intercept the effect of a factor the effect of a one-unit increase in a numeric variable the effect of an x-unit increase in a numeric variable Make a point prediction of the response for a new observation Write R code to fit a linear model with interaction terms in the explanatory variables Interpret estimated effects with reference to confidence intervals from linear regression models. Specifically the interpretation of main effects in a model with an interaction the effect of one variable when others are included in the model Other resources Exploring interactions with continuous predictors in regression models The ASA Statement on p-Values: Context, Process, and Purpose Power Analysis | Introduction in R "],["key-satistical-concepts.html", "Key satistical concepts", " Key satistical concepts Statistical inference Statistical inference is the process of deducing properties of an underlying distribution by analysis of data. The word inference means conclusions or decisions. Statistical inference is about drawing conclusions and making decisions based on observed data. Data, or observations, typically arise from some underlying process. It is the underlying process we are interested in, not the observations themselves. Sometimes we call the underlying process the population or mechanism of interest. The data are only a sample from this population or mechanism. We cannot possibly observe every outcome of the process, so we have to make do with the sample that we have observed. The data give us imperfect insight into the population of interest. The role of statistical inference is to use this imperfect data to draw conclusions about the population of interest, while simultaneously giving an honest reflection of the uncertainty in our conclusions. Hypothesis testing Hypothesis testing is a form of statistical inference that uses data from a sample to draw conclusions about a population parameter or a population probability distribution. For example, imagine I toss a coin ten times and get nine heads... Questions: How unlikely is that? Is the coin fair? Possible outcomes Probability distribution "],["bootstrap-resampling.html", "Bootstrap resampling", " Bootstrap resampling A sampling distribution shows us what would happen if we took very many samples under the same conditions. The bootstrap is a procedure for finding the (approximate) sampling distribution from just one sample. The original sample represents the distribution of the population from which it was drawn. Resamples, taken with replacement from the original sample are representative of what we would get from drawing many samples from the population (the distribution of the statistics calculated from each resample is known as the bootstrap distribution of the statistic). The bootstrap distribution of a statistic represents that statistic's sampling distribution. Example: constructing bootstrap confidence intervals Old faithful is a gyser located in Yellowstone National Park, Wyoming. Below is a histogram of the duration of 299 consecutive eruptions. Clearly the distribution is bimodal (has two modes)! library(tidyverse) ggplot(data = MASS::geyser, aes(x = duration)) + geom_histogram() + xlab(&quot;Duration of eruptions (m)&quot;) + theme_linedraw() Step 1: Calculating the observed mean eruption duration time: mean &lt;- MASS::geyser %&gt;% summarise(mean = mean(duration)) mean ## mean ## 1 3.460814 Step 2: Construct bootstrap distribution ## Number of times I want to bootstrap nreps &lt;- 1000 ## initialize empty array to hold results bootstrap_means &lt;- numeric(nreps) set.seed(1234) ## *****Remove this line for actual analyses***** ## This means that each run with produce the same results and ## agree with the printout that I show. for (i in 1:nreps) { ## bootstrap. note with replacement bootstrap_sample &lt;- sample(MASS::geyser$duration, replace = TRUE) ## bootstrapped mean resample bootstrap_means[i] &lt;- mean(bootstrap_sample) } Step 3: Collate the sampling distribution ## results results &lt;- data.frame(bootstrap_means = bootstrap_means) ggplot(data = results, aes(x = bootstrap_means)) + geom_histogram() + geom_vline(xintercept = as.numeric(mean)) + ggtitle(&quot;Sampling distribution of the mean&quot;) + xlab(&quot;Bootstrap means&quot;) + ylab(&quot;&quot;) + theme_classic() Step 4: Calculate quantities of interest from the sampling distribution The bootstrap estimate of bias is the difference between the mean of the bootstrap distribution and the value of the statistic in the original sample: bias &lt;- as.numeric(mean) - mean(results$bootstrap_means) bias ## [1] 0.001200111 The bootstrap standard error of a statistic is the standard deviation of its bootstrap distribution: sd(results$bootstrap_means) ## [1] 0.06740607 ## compare to SEM of original data MASS::geyser %&gt;% summarise(sem = sd(duration)/sqrt(length(duration))) ## sem ## 1 0.06638498 A Bootstrap \\(t\\) confidence interval. If, for a sample of size \\(n\\), the bootstrap distribution is approximately Normal and the estimate of bias is small then an approximate \\(C\\) confidence for the parameter corresponding to the statistic is: \\[\\text{statistic} \\pm t^* \\text{SE}_\\text{bootstrap}\\] where \\(t*\\) is the critical value of the \\(t_{n-1}\\) distribution with area \\(C\\) between \\(-t^*\\) and \\(t^*\\). For \\(C = 0.95\\): as.numeric(mean) + c(-1,1) * qt(0.975,298)*sd(results$bootstrap_means) ## [1] 3.328162 3.593466 So our 95% confidence interval is 3.3 to 3.6. A bootstrap \\(percentile\\) confidence interval. Use the bootstrap distribution itself to determine the limits of the confidence interval by taking the limits of the sorted, central \\(C\\) bulk of the distribution. For \\(C = 0.95\\): sort(results$bootstrap_means)[c(25,975)] ## [1] 3.328428 3.591081 "],["permutation-and-randomisation-tests.html", "Permutation and randomisation tests", " Permutation and randomisation tests A permutation test is a statistical significance test where the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistic under all possible rearrangements of the observed data points. This is also known as an exact test. Now, often finding all possible combinations is hugely computationally expensive so we harness the power of simulation and carry out a randomisation test, which randomly simulates (for a given number of repetitions) possible values of the test statistic under the null hypothesis to obtain its approximate distribution. The basic approach to a randomisation tests is straightforward: Choose a statistic to measure the effect in question (e.g., differences between group means) Calculate that test statistic on the observed data. Note this metric can be anything you wish Construct the sampling distribution that this statistic would have if the effect were not present in the population (i.e., the distribution under the Null hypothesis, \\(H_0\\)): For chosen number of times shuffle the data labels calculate the test statistic for the reshuffled data and retain Find the location of your observed statistic in the sampling distribution. The location of observed statistic in sampling distribution is informative: if in the main body of the distribution then the observed statistic could easily have occurred by chance if in the tail of the distribution then the observed statistic would rarely occur by chance and there is evidence that something other than chance is operating. Calculate the proportion of times your reshuffled statistics equal or exceed the observed. This p-value is the probability that we observe a statistic at least as ‚Äúextreme‚Äù as the one we observed State the strength of evidence against the null on the basis of this probability. A permutation test: Jackal mandible lengths Below are data specifying mandible lengths (mm) for golden jackals (Canis aureus) of each sex from the British Museum Mandible length (mm) Biological sex 120 Male 107 Male 110 Male 116 Male 114 Male 111 Male 113 Male 117 Male 114 Male 112 Male 110 Female 111 Female 107 Female 108 Female 110 Female 105 Female 107 Female 106 Female 111 Female 111 Female Scientific question: Are the jaw lengths of jackals the same, on average, in both sexes? Null hypothesis: The average jaw lengths in male jackals the same as for females Test statistic: Difference of sample means Let us first calculate the observed test statistic: ## the data jackal &lt;- data.frame(mandible_length_mm = c(120, 107, 110, 116, 114, 111, 113, 117, 114, 112, 110, 111, 107, 108, 110, 105, 107, 106, 111, 111), sex = rep(c(&quot;Male&quot;,&quot;Female&quot;), each = 10)) ## observed statistic jackal_mean_diff &lt;- jackal %&gt;% group_by(sex) %&gt;% summarise(mean = mean(mandible_length_mm)) %&gt;% summarise(diff = diff(mean)) %&gt;% as.numeric() jackal_mean_diff ## [1] 4.8 Now we use the command combn(x, m) to generate all possible combinations of the elements of x taken m at a time. For our data we have 20 elements in total with two groups of 10 elements each, therefore, we use combn(20,10). We can use these combinations to generate all possible combinations and calculate the proportion of times the test statistic calculated under the null hypothesis is as least as extreme as the one observed (the p-value): combinations &lt;- combn(20,10) ## Do the permutations permtest_combinations &lt;- apply(combinations, 2, function(x) mean(jackal$mandible_length_mm[x]) - mean(jackal$mandible_length_mm[-x])) ## Full Permutation test p.value p_val &lt;- length(permtest_combinations[abs(permtest_combinations) &gt;= jackal_mean_diff]) / choose(20,10) p_val ## [1] 0.003334127 Rather than considering all possible combinations we might rather use 100 random permutations under the null hypothesis. Here, we sample without replacement: ## set up matrix random_perm &lt;- apply(matrix(0, nrow = 99, ncol = 1), 1, function(x) sample(20)) random_mean_diff &lt;- apply(random_perm, 2, function(x){ z &lt;- jackal$mandible_length_mm[x] mean(z[jackal$sex == &quot;Male&quot;]) - mean(z[jackal$sex == &quot;Female&quot;]) }) ## add the observed random_mean_diff &lt;- c(random_mean_diff, jackal_mean_diff) random_p.value &lt;- length(random_mean_diff[abs(random_mean_diff) &gt;= jackal_mean_diff]) / 100 ## note the abs() random_p.value ## [1] 0.01 How does this compare to the exact (permutation) results above? Try increasing the number of times you randomise? What happens as you increase it? A randomisation test: PƒÅua shell length Remember the PƒÅua data from the previous module? The dataset contains the following variables Age of P\\(\\overline{\\text{a}}\\)ua in years (calculated from counting rings in the cone) Length of P\\(\\overline{\\text{a}}\\)ua shell in centimeters Species of P\\(\\overline{\\text{a}}\\)ua: Haliotis iris (typically found in NZ) and Haliotis australis (less commonly found in NZ) One question we may want to ask is if on average the shell length differs between Species? library(tidyverse) paua &lt;- read_csv(&quot;https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/paua.csv&quot;) Scientific question: Are the shell lengths of shells the same in both species? Null hypothesis: The distribution of shell lengths in Haliotis iris the same as in Haliotis australis Test statistic: Difference of sample means But because the data are skewed and we've likely got non-constant variances we may be better off adopting a randomization test (this time using a for loop!), rather than a parametric t-test 1. Choose a statistic that measures the effect of interest (in this case the differences between means). ## observed differences in means diff_in_means &lt;- (paua %&gt;% group_by(Species) %&gt;% summarise(mean = mean(Length)) %&gt;% summarise(diff = diff(mean)))$diff diff_in_means ## [1] -0.9569444 2. Construct the sampling distribution that this statistic would have if the effect were not present in the population (i.e., the distribution under \\(H_0\\)) . ## Number of times I want to randomise nreps &lt;- 999 ## initialize empty array to hold results randomisation_difference_mean &lt;- numeric(nreps) set.seed(1234) ## *****Remove this line for actual analyses***** ## This means that each run with produce the same results and ## agree with the printout that I show. for (i in 1:nreps) { ## the observations data &lt;- data.frame(value = paua$Length) ## randomise labels data$random_labels &lt;-sample(paua$Species, replace = FALSE) ## randomised differences in mean randomisation_difference_mean[i] &lt;- data %&gt;% group_by(random_labels) %&gt;% summarise(mean = mean(value)) %&gt;% summarise(diff = diff(mean)) %&gt;% as.numeric() } ## results, join randomised stats with the observed value results &lt;- data.frame(results = c(randomisation_difference_mean, diff_in_means)) 3. Locate the observed statistic (i.e., from our observed random sample) in the sampling distribution Let's calculate how many randomised differences in means are as least as extreme as the one we observed. Note that we library the absolute value as dealing with a two tailed test. n_exceed &lt;- sum(abs(results$results) &gt;= abs(diff_in_means)) n_exceed ## [1] 2 ## proportion n_exceed/nreps ## [1] 0.002002002 What do you conclude from this proportion? How does this tie in with the distribution of the test statistic under the null hypothesis shown below? NOTE: We can extend the randomization test to make inference about any sample statistic (not just the mean) "],["resampling-procedures-the-differences.html", "Resampling procedures, the differences", " Resampling procedures, the differences Resampling methods are any of a variety of methods for doing one of the following Estimating the precision of sample statistics (e.g., bootstrapping) Performing significance tests (e.g., permutation/exact/randomisation tests) Validating models (e.g., bootstrapping, cross validation) Permutation vs bootstrap test The permutation test exploits symmetry under the null hypothesis. A full permutation test p-value is exact, conditional on data values in the combined sample. A bootstrap estimates the probability mechanism that generated the samples under the null hypothesis. A bootstrap does not rely on any special symmetry or assumption or exchangeability. "],["hypothesis-testing-key-terms.html", "Hypothesis testing: key terms", " Hypothesis testing: key terms üò± p-values üò± &quot;Good statistical practice, as an essential component of good scientific practice, emphasizes principles of good study design and conduct, a variety of numerical and graphical summaries of data, understanding of the phenomenon under study, interpretation of results in context, complete reporting and proper logical and quantitative understanding of what data summaries mean. No single index should substitute for scientific reasoning.&quot; --- ASA Statement on p-Values In experimental situations a large tail proportion (large p-value) means that the luck of the randomisation quite often produces test statistics as large or even larger than what we've got in our data. A small tail proportion (small p-value) means that the luck of the randomisation draw hardly ever produces test statistics as large as we've got in our data. What is a p-Value? Informally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value. P-values can indicate how incompatible the data are with a specified statistical model, but they do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone. Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold. A p-value, or statistical significance, does not measure the size of an effect or the importance of a result. Statistical significance does not imply practical significance. Statistical significance says nothing about the size of treatment differences. By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis Type I and Type II errors &quot;Type I Zoom error: you think people can hear you, but you‚Äôre actually on mute. Type II Zoom error: you think your muted, but actually people can hear you.&quot; --- @CynPeacock, Twitter Type I A Type I error (false positive): declare a difference (i.e., reject \\(H_0\\)) when there is no difference (i.e. \\(H_0\\) is true). Risk of the Type I error is determined by the level of significance (which we set!) (i.e., \\(\\alpha =\\text{ P(Type I error)} = \\text{P(false positive)}\\). Type II A Type II error (false negative): difference not declared (i.e., \\(H_0\\) not rejected) when there is a difference (i.e., \\(H_0\\) is false). Let \\(\\beta =\\) P(do not reject \\(H_0\\) when \\(H_0\\) is false); so, \\(1-\\beta\\) = P(reject \\(H_0\\) when \\(H_0\\) is false) = P(a true positive), which is the statistical power of the test. Family-Wise Error Rate (FWER) Each time we carry out a hypothesis test the probability we get a false positive result (type I error) is given by \\(\\alpha\\) (the level of significance we choose). When we have multiple comparisons to make we should then control the Type I error rate across the entire family of tests under consideration, i.e., control the Family-Wise Error Rate (FWER); this ensures that the risk of making at least one Type I error among the family of comparisons in the experiment is \\(\\alpha\\). More on this in later chapters. State of Nature Don't reject \\(H_0\\) reject \\(H_0\\) \\(H_0\\) is true ‚úÖ Type I error \\(H_0\\) is false Type II error ‚úÖ "],["parametric-hypothesis-tests.html", "Parametric hypothesis tests", " Parametric hypothesis tests Recall, the P\\(\\overline{\\text{a}}\\)ua dataset. It contains the following variables Age of P\\(\\overline{\\text{a}}\\)ua in years (calculated from counting rings in the cone) Length of P\\(\\overline{\\text{a}}\\)ua shell in centimeters Species of P\\(\\overline{\\text{a}}\\)ua: Haliotis iris (typically found in NZ) and Haliotis australis (less commonly found in NZ) library(tidyverse) paua &lt;- read_csv(&quot;https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/paua.csv&quot;) glimpse(paua) ## Rows: 60 ## Columns: 3 ## $ Species &lt;chr&gt; &quot;Haliotis iris&quot;, &quot;Haliotis australis&quot;, &quot;Haliotis australis&quot;, &quot;‚Ä¶ ## $ Length &lt;dbl&gt; 1.80, 5.40, 4.80, 5.75, 5.65, 2.80, 5.90, 3.75, 7.20, 4.25, 6.‚Ä¶ ## $ Age &lt;dbl&gt; 1.497884, 11.877010, 5.416991, 4.497799, 5.500789, 2.500972, 6‚Ä¶ One-Sample t-test Using a violin plot we can look at the distribution of shell Length. We can calculate the average Length of all shells in our sample paua %&gt;% summarise(average_length = mean(Length)) ## # A tibble: 1 √ó 1 ## average_length ## &lt;dbl&gt; ## 1 5.19 What about drawing inference? Do we believe that the average length of P\\(\\overline{\\text{a}}\\)ua shells is, say, 5cm? We know our sample average, but can we make any claims based on this one number? How do we reflect our uncertainty about the population mean? Remember, it's the population we want to make inference on based on our sample! Enter the Standard Error of the Mean (SEM), \\[\\text{SEM} = \\frac{\\sigma}{\\sqrt{n}}, \\] where \\(\\sigma = \\sqrt{\\frac{\\Sigma_{i = 1}^n(x_i - \\bar{x})^2}{n-1}}\\) (\\(i = 1,...,n\\)) is the standard deviation (SD) of the sample, \\(n\\) is the sample size, and \\(\\bar{x}\\) is the sample mean. Calculating \\(\\Sigma_{i = 1}^n(x_i - \\bar{x})^2, i = 1,...,n\\) by hand. In, relatively, plain English this sum is the sum squared differences of the distances between the \\(i^{th}\\) observation and the sample mean \\(\\bar{x}\\) (denoted \\(\\mu_x\\) in the GIF below) So using the example values in the GIF ## our sample of values x &lt;- c(1,2,3,5,6,9) ## sample mean sample_mean &lt;- mean(x) sample_mean ## [1] 4.333333 ## distance from mean for each value distance_from_mean &lt;- x - sample_mean distance_from_mean ## [1] -3.3333333 -2.3333333 -1.3333333 0.6666667 1.6666667 4.6666667 ## squared distance from mean for each value squared_distance_from_mean &lt;- distance_from_mean^2 squared_distance_from_mean ## [1] 11.1111111 5.4444444 1.7777778 0.4444444 2.7777778 21.7777778 ## sum of the squared distances sum(squared_distance_from_mean) ## [1] 43.33333 Calculating SD and SEM Now what about the SD? Remember it's the \\(\\sqrt{\\frac{\\Sigma_{i = 1}^n(x_i - \\bar{x})^2}{n-1}}\\) so = \\(\\sqrt{\\frac{43.3333333}{n-1}}\\) = \\(\\sqrt{\\frac{43.3333333}{6-1}}\\) = \\(\\sqrt{\\frac{43.3333333}{5}}\\) = 2.9439203. Or we could just use R's sd() function sd(x) ## [1] 2.94392 So the SEM is \\(\\frac{\\text{SD}}{\\sqrt{n}}\\) = \\(\\frac{2.9439203}{\\sqrt{6}}\\) In R sd(x)/sqrt(length(x)) ## [1] 1.20185 For the paua data we can simply use the in-built functions in R to calculate the SEM sem &lt;- paua %&gt;% summarise(mean = mean(Length), sem = sd(Length)/sqrt(length(Length))) sem ## # A tibble: 1 √ó 2 ## mean sem ## &lt;dbl&gt; &lt;dbl&gt; ## 1 5.19 0.155 Visualising the uncertainty Recall that the SEM is a measure of uncertainty about the mean. So we can use it to express our uncertainty visually. Typically \\(\\pm\\) twice the SEM is the interval used: Why error bars that are \\(\\pm\\) twice the SEM? This is approximately the 95% confidence interval for the population mean (see lecture) The exact 95% CI is given by \\(\\bar{x}\\) (mean) \\(\\pm\\) \\(t_{df,1 - \\alpha/2}\\) \\(\\times\\) SEM df = degrees of freedom (in this situation df = n - 1) \\(\\alpha\\) = level of significance Each mean has its own confidence interval whose width depends on the SEM for that mean When the df (more on these later) are large (e.g. 30 or greater) and \\(\\alpha\\) = 0.05 \\(t_{df,1 - \\alpha/2}\\) = \\(t_{large,0.975}\\) \\(\\approx\\) 2. Hence, the 95% confidence interval for the population mean is approximately \\(\\bar{x}\\) (mean) \\(\\pm\\) 2 \\(\\times\\) SEM Back to our hypothesis test Question: Do we believe that the average length of P\\(\\overline{\\text{a}}\\)ua shells is 5cm? Formalizing into a hypothesis test: Null hypothesis: On average P\\(\\overline{\\text{a}}\\)ua shells are 5cm long Alternative hypothesis: On average P\\(\\overline{\\text{a}}\\)ua shells are not 5cm long Notationally: \\(H_0: \\mu = 5\\) vs \\(H_1: \\mu \\neq 5\\) (\\(\\mu\\) is the proposed mean) Calculating a statistic (we use a t-statistic) t-statistic \\(= \\frac{\\bar{x}- \\mu}{\\text{SEM}}\\) = \\(\\frac{5.1925 - 5}{0.155351}\\) = 1.239 \\(\\bar{x}\\) is the sample mean \\(\\mu\\) is the theoretical value (proposed mean) The corresponding p-value Recall that a p-value is the probability under a specified statistical model that a statistical summary of the data would be equal to or more extreme than its observed value So in this case it's the probability, under the null hypothesis (\\(\\mu = 5\\)), that we would observe a statistic as least as extreme as we did. Under our null hypothesis the distribution of the t-statistic is as below. The one calculated from our hypothesis test was 1.2391. Now, remember that our alternative hypotheses was \\(H_1: \\mu \\neq 5\\) so we have to consider both sides of the inequality; hence, anything as least as extreme is either \\(&gt; 1.2391\\) or \\(&lt; -1.2391\\) to our observed statistic (vertical lines). Anything as least as extreme is therefore given by the grey shaded areas. We can calculate the p-value using the pt() function (where q is our calculated t-statistic, and df are the degrees of freedom from above): 2*(1 - pt(q = 1.2391,df = 59)) ## [1] 0.2202152 Or we could do all of the above in one step using R t.test(paua$Length, mu = 5 ) ## ## One Sample t-test ## ## data: paua$Length ## t = 1.2391, df = 59, p-value = 0.2202 ## alternative hypothesis: true mean is not equal to 5 ## 95 percent confidence interval: ## 4.881643 5.503357 ## sample estimates: ## mean of x ## 5.1925 Recall, that the p-value gives the probability that under our null hypothesis we observe anything as least as extreme as what we did (hence the \\(\\times 2\\), think of the grey shaded area in the graph). This probability is \\(\\sim\\) 22%. Do you think what we've observed is likely under the null hypothesis? Does this plot help? The proposed mean is shown by the blue horizontal line; the dashed line shows the sample mean and the dotted lines \\(\\pm\\) the SEM. Differences between two means Calculating the differences between species means: Haliotis australis average - Haliotis iris average = \\(\\mu_{\\text{Haliotis australis}} - \\mu_{\\text{Haliotis iris}}\\) = 5.767 - 4.81 = 0.957. Doesn't really tell us much... As above the average values are all well and good, but what about variation? Recall the SEM from the one-sample t-test? The same idea holds here, although the calculation is a little bit more complicated (as we have to think about the number of observations in each group). But from the two group SEMs we can calculate the Standard Error of the Difference between two means, SED. Independent samples t-test using t.test() Question: Do we believe that on average the length of P\\(\\overline{\\text{a}}\\)ua shells are equal between species? Formalizing into a hypothesis test: Null hypothesis: On average the species' shells are the same length Alternative hypothesis: they aren't! Notationally: \\(H_0: \\mu_{\\text{Haliotis iris}} - \\mu_{\\text{Haliotis australis}} = 0\\) vs \\(H_1: \\mu_{\\text{Haliotis iris}} \\neq \\mu_{\\text{Haliotis australis}},\\) where \\(\\mu_{j}\\) is the average length for species, \\(j =\\) (Haliotis iris, Haliotis australis). Let us now calculate the test statistic: t-statistic = \\(\\frac{\\bar{x}_{\\text{difference}} - \\mu}{\\text{SED}}\\) = \\(\\frac{\\bar{x}_{\\text{difference}} - 0}{\\text{SED}}n\\) where \\(\\bar{x}_{\\text{difference}}\\) is the differences between the species` averages. Calculations area a little bit more tricky here so let's use R: test &lt;- t.test(Length ~ Species, data = paua) ## printing out the result test ## ## Welch Two Sample t-test ## ## data: Length by Species ## t = 3.5404, df = 57.955, p-value = 0.0007957 ## alternative hypothesis: true difference in means between group Haliotis australis and group Haliotis iris is not equal to 0 ## 95 percent confidence interval: ## 0.4158802 1.4980086 ## sample estimates: ## mean in group Haliotis australis mean in group Haliotis iris ## 5.766667 4.809722 test$p.value ## [1] 0.0007956853 Listed are the t-statistic, t = 3.5403636 and the p-value, p-value = 7.9610^{-4} for the hypothesis test outlined above. What would you conclude? One-Way Analysis of Variance (ANOVA) What if we had more than two groups? In this section we consider the following data of the calculated logAUC for 12 rats subjected to three different treatments (Surgery), C, P, and S: Surgery Rat logAUC C 1 8.49 C 2 8.20 C 3 9.08 C 4 8.07 P 1 10.24 P 2 7.72 P 3 9.34 P 4 8.50 S 1 11.31 S 2 12.69 S 3 11.37 S 4 10.82 To read the data directly into R you can use readr::read_csv(&quot;https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/crd_rats_data.csv&quot;) Between group SS (SSB) The idea: Assess distances between treatment (surgical condition) means relative to our uncertainty about the actual (true) treatment means. Add up the differences: -1.192 + -0.703 + 1.895 = 0. This is always the case! So adding up the differences: -1.192 + -0.703 + 1.895 = 0. Not a great way to measure distances! Sums of Squares? -1.192^2 + -0.703^2 + 1.895^2 add up the squared differences? but... there are 4 observations in each group (treatment) 4\\(\\times\\)(-1.192)^2 + 4\\(\\times\\)(-0.703)^2 + 4\\(\\times\\)(1.895)^2 This is the Between Groups Sums of Squares or the Between group SS (SSB) So the Between group SS (SSB) = 22.02635 Adding up the differences: -1.192 + -0.703 + 1.895 = 0. This is always the case and that itself gives us information... We only need to know two of the values to work out the third! So we have only 2 bits of unique information; SSB degrees of freedom = 2 Within group SS (SSW) The Within group SS (SSW) arises from the same idea: To assess distances between treatment (surgical condition) means relative to our uncertainty about the actual (true) treatment means. Procedure: Observation - Treatment mean Square the difference Add them up! Within group SS (SSW) unexplained variance F-statistic Recall the Between group SS (SSB) = 22.02635 So mean SSB = 22.02635 / 2 The within group SS (SSW) = 6.059075 Here we have 3\\(\\times\\) 3 bits of unique information: within groups degrees of freedom is 9. So mean SSW = 6.059/9 Consider the ratio \\({\\frac{{\\text{variation due to treatments}}}{{\\text{unexplained variance}}}} = {\\frac{{\\text{ mean between-group variability}}}{{\\text{mean within-group variability}}}}\\) \\(=\\frac{\\text{mean SSB}}{\\text{mean SSW}}\\) \\(=\\frac{\\text{MSB}}{\\text{MSW}}\\) = \\(=\\frac{\\text{experimental variance}}{\\text{error variance}}\\) 16.3586975 This is the F-statistic! Degrees of freedom (DF) Essentially statistical currency (i.e., unique bits of information). So in the example above we have 3 treatment groups and if we know the mean of two we know the third (i.e., 2 unique bits of info) so SSB df = 2. Now, for SSW df We have 12 observations (4 in each group); we know the treatment means so if we have three of those observed values in each group we know the fourth: 12 - 3 = 9 (i.e., number of observations - number of df lost due to knowing the cell means). Inference Hypothesis: We test the Null hypothesis, \\(H_0\\), population (Surgery) means are the same on average verses the alternative hypothesis, \\(H_1\\), that at least one differs from the others! Under our null hypothesis the distribution of the F-statistic is as below. The value calculated from our hypothesis test was 16.359. This observed test statistic is shown by the vertical line. To calculate the relevant p-value we can use (1 - pf(q = 16.351,df1 = 2, df2 = 9)) ## [1] 0.001007825 Thus, the probability of getting an F-statistic at least as extreme as the one we observe (think of the area under the tails of the curve below) p-value Pr(&gt;F)= 0.001 tells us we have sufficient evidence to reject \\(H_0\\) at the 1% level of significance. Alternatively, we could do this in one step using aov(): summary(aov(logAUC ~ Surgery, data = rats)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Surgery 2 22.026 11.013 16.36 0.00101 ** ## Residuals 9 6.059 0.673 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],["linear-regression.html", "Linear regression", " Linear regression Some mathematical notation Let's consider a linear regression with a simple explanatory variable: \\[Y_i = \\alpha + \\beta_1x_i + \\epsilon_i\\] where \\[\\epsilon_i \\sim \\text{Normal}(0,\\sigma^2).\\] Here for observation \\(i\\) \\(Y_i\\) is the value of the response \\(x_i\\) is the value of the explanatory variable \\(\\epsilon_i\\) is the error term: the difference between \\(Y_i\\) and its expected value \\(\\alpha\\) is the intercept term (a parameter to be estimated), and \\(\\beta_1\\) is the slope: coefficient of the explanatory variable (a parameter to be estimated) Modeling Bill Depth Remember the penguins from the first module? Key assumptions Independence There is a linear relationship between the response and the explanatory variables The residuals have constant variance The residuals are normally distributed library(tidyverse) library(palmerpenguins) penguins_nafree &lt;- penguins %&gt;% drop_na() ggplot(data = penguins_nafree, aes(x = bill_depth_mm)) + geom_histogram() + theme_classic() + xlab(&quot;Bill depth (mm)&quot;) First off let's fit a null (intercept only) model. This in old money would be called a one sample t-test. slm_null &lt;- lm(bill_depth_mm ~ 1, data = penguins_nafree) summary(slm_null)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 17.16486 0.1079134 159.0614 1.965076e-315 Model formula This model, from above, is simply \\[Y_i = \\alpha + \\epsilon_i.\\] Here for observation \\(i\\) \\(Y_i\\) is the value of the response (bill_depth_mm) and \\(\\alpha\\) is a parameter to be estimated (typically called the intercept). Inference The (Intercept) term, 17.1648649, tells us the (estimated) average value of the response (bill_depth_mm), see penguins_nafree %&gt;% summarise(average_bill_depth = mean(bill_depth_mm)) ## # A tibble: 1 √ó 1 ## average_bill_depth ## &lt;dbl&gt; ## 1 17.2 The SEM (Std. Error) = 0.1079134. The hypothesis being tested is \\(H_0:\\) ((Intercept) ) \\(\\text{mean}_{\\text{`average_bill_depth`}} = 0\\) vs. \\(H_1:\\) ((Intercept)) \\(\\text{mean}_{\\text{`average_bill_depth`}} \\neq 0\\) The t-statistic is given by t value = Estimate / Std. Error = 159.0614207 The p-value is given byPr (&gt;|t|) = 1.965076110^{-315}. So the probability of observing a t-statistic as least as extreme given under the null hypothesis (average bill depth = 0) given our data is 1.965076110^{-315}, pretty strong evidence against the null hypothesis I'd say! Single continuous variable Does bill_length_mm help explain some of the variation in bill_depth_mm? p1 &lt;- ggplot(data = penguins_nafree, aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + ylab(&quot;Bill depth (mm)&quot;) + xlab(&quot;Bill length (mm)&quot;) + theme_classic() p1 slm &lt;- lm(bill_depth_mm ~ bill_length_mm, data = penguins_nafree) Model formula This model is simply \\[Y_i = \\alpha + \\beta_1x_i + \\epsilon_i\\] where for observation \\(i\\) \\(Y_i\\) is the value of the response (bill_depth_mm) and \\(x_i\\) is the value of the explanatory variable (bill_length_mm); As above \\(\\alpha\\) and \\(\\beta_1\\) are parameters to be estimated. We could also write this model as \\[ \\begin{aligned} \\operatorname{bill\\_depth\\_mm} &amp;= \\alpha + \\beta_{1}(\\operatorname{bill\\_length\\_mm}) + \\epsilon \\end{aligned} \\] Fitted model As before we can get out estimated parameters (here \\(\\alpha\\) and \\(\\beta_1\\)) using summary(slm)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.78664867 0.85417308 24.335406 1.026904e-75 ## bill_length_mm -0.08232675 0.01926835 -4.272642 2.528290e-05 Here, the (Intercept): Estimate (\\(\\alpha\\) above) gives us the estimated average bill depth (mm) given the estimated relationship bill length (mm) and bill length. The bill_length_mm : Estimate (\\(\\beta_1\\) above) is the slope associated with bill length (mm). So, here for every 1mm increase in bill length we estimated a 0.082mm decrease (or a -0.082mm increase) in bill depth. ## calculate predicted values penguins_nafree$pred_vals &lt;- predict(slm) ## plot ggplot(data = penguins_nafree, aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + ylab(&quot;Bill depth (mm)&quot;) + xlab(&quot;Bill length (mm)&quot;) + theme_classic() + geom_line(aes(y = pred_vals)) One factor and a continuous variable Adding species; remember species is a factor variable! p2 &lt;- ggplot(data = penguins_nafree, aes(y = bill_depth_mm, x = bill_length_mm, color = species)) + geom_point() + ylab(&quot;Bill depth (mm)&quot;) + xlab(&quot;Bill length (mm)&quot;) + theme_classic() p2 slm_sp &lt;- lm(bill_depth_mm ~ bill_length_mm + species, data = penguins_nafree) Model formula Now we have two explanatory variables, so our model formula becomes \\[Y_i = \\beta_0 + \\beta_1z_i + \\beta_2x_i + \\epsilon_i\\] \\[\\epsilon_i \\sim \\text{Normal}(0,\\sigma^2)\\] where for observation \\(i\\) \\(Y_i\\) is the value of the response (bill_depth_mm) \\(z_i\\) is one explanatory variable (bill_length_mm say) \\(x_i\\) is another explanatory variable (species say) \\(\\epsilon_i\\) is the error term: the difference between \\(Y_i\\) and its expected value \\(\\alpha\\), \\(\\beta_1\\), and \\(\\beta_2\\) are all parameters to be estimated. Remember though that when we have factor explanatory variables (e.g., species) we have to use dummy variables, see lecture. Here the Adelie group are the baseline (R does this alphabetically, to change this see previous chapter). So model formula is \\[ \\begin{aligned} \\operatorname{bill\\_depth\\_mm} &amp;= \\alpha + \\beta_{1}(\\operatorname{bill\\_length\\_mm}) + \\beta_{2}(\\operatorname{species}_{\\operatorname{Chinstrap}}) + \\beta_{3}(\\operatorname{species}_{\\operatorname{Gentoo}})\\ + \\\\ &amp;\\quad \\epsilon \\end{aligned} \\] Fitted model summary(slm_sp)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.5652616 0.69092642 15.291442 2.977289e-40 ## bill_length_mm 0.2004431 0.01767974 11.337449 2.258955e-25 ## speciesChinstrap -1.9330779 0.22571878 -8.564099 4.259893e-16 ## speciesGentoo -5.1033153 0.19439523 -26.252267 1.043789e-82 Simpson's paradox... look how the slope associated with bill length (coefficient of bill_length_mm) has switched direction from the model above! Why do you think this is? Here, the (Intercept): Estimate gives us the estimated average bill depth (mm) of the Adelie penguins given the estimated relationship between bill length and bill depth. Technically, this is the estimated bill depth (mm) for Adelie penguins with zero bill length. That is clearly a nonsense way to interpret this as that would be an impossible situation in practice! I would recommend as thinking of this as the y-shift (i.e., height) of the fitted line. The bill_length_mm : Estimate (\\(\\beta_1\\) above) is the slope associated with bill length (mm). So, here for every 1mm increase in bill length we estimated a 0.2mm increase in bill depth. What about the coefficient of the other species levels? Look at the plot below, these values give the shift (up or down) of the parallel lines from the Adelie level. So given the estimated relationship between bill depth and bill length these coefficients are the estimated change from the baseline. ## calculate predicted values penguins_nafree$pred_vals &lt;- predict(slm_sp) ## plot ggplot(data = penguins_nafree, aes(y = bill_depth_mm, x = bill_length_mm, color = species)) + geom_point() + ylab(&quot;Bill depth (mm)&quot;) + xlab(&quot;Bill length (mm)&quot;) + theme_classic() + geom_line(aes(y = pred_vals)) Interactions Recall the (additive) model formula from above \\[Y_i = \\beta_0 + \\beta_1z_i + \\beta_2x_i + \\epsilon_i\\] but what about interactions between variables? For example, \\[Y_i = \\beta_0 + \\beta_1z_i + \\beta_2x_i + \\beta_3z_ix_i + \\epsilon_i\\] Note: to include interaction effects in our model by using either the * or : syntax in our model formula. For example, : denotes the interaction of the variables to its left and right, and * means to include all main effects and interactions, so a*b is the same as a + b + a:b. To specify a model with additive and interaction effects we use slm_int &lt;- lm(bill_depth_mm ~ bill_length_mm*species, data = penguins_nafree) Model formula The model formula is then \\[ \\begin{aligned} \\operatorname{bill\\_depth\\_mm} &amp;= \\alpha + \\beta_{1}(\\operatorname{bill\\_length\\_mm}) + \\beta_{2}(\\operatorname{species}_{\\operatorname{Chinstrap}}) + \\beta_{3}(\\operatorname{species}_{\\operatorname{Gentoo}})\\ + \\\\ &amp;\\quad \\beta_{4}(\\operatorname{bill\\_length\\_mm} \\times \\operatorname{species}_{\\operatorname{Chinstrap}}) + \\beta_{5}(\\operatorname{bill\\_length\\_mm} \\times \\operatorname{species}_{\\operatorname{Gentoo}}) + \\epsilon \\end{aligned} \\] Fitted model summary(slm_int)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.48770713 1.15987305 9.9042797 2.135979e-20 ## bill_length_mm 0.17668344 0.02980564 5.9278518 7.793199e-09 ## speciesChinstrap -3.91856701 2.06730876 -1.8954919 5.890889e-02 ## speciesGentoo -6.36675118 1.77989710 -3.5770333 4.000274e-04 ## bill_length_mm:speciesChinstrap 0.04552828 0.04594283 0.9909769 3.224296e-01 ## bill_length_mm:speciesGentoo 0.03092816 0.04111608 0.7522157 4.524625e-01 This can be written out: \\[ \\begin{aligned} \\operatorname{\\widehat{bill\\_depth\\_mm}} &amp;= 11.49 + 0.18(\\operatorname{bill\\_length\\_mm}) - 3.92(\\operatorname{species}_{\\operatorname{Chinstrap}}) - 6.37(\\operatorname{species}_{\\operatorname{Gentoo}})\\ + \\\\ &amp;\\quad 0.05(\\operatorname{bill\\_length\\_mm} \\times \\operatorname{species}_{\\operatorname{Chinstrap}}) + 0.03(\\operatorname{bill\\_length\\_mm} \\times \\operatorname{species}_{\\operatorname{Gentoo}}) \\end{aligned} \\] The interaction terms (i.e., bill_length_mm:speciesChinstrap and bill_length_mm:speciesGentoo) specify the species specific slopes given the other variables in the model. We can use our dummy variable trick again to interpret the coefficients correctly. In this instance we have one factor explanatory variable, Species, and one continuous explanatory variable, bill length (mm). As before, Adelie is our baseline reference. Let's assume we're talking about the Adelie penguins, then our equation becomes (using the dummy variable technique) \\[\\widehat{\\text{bill_depth_mm}} = 11.488 + (0.177 \\times \\text{bill_length_mm}).\\] So, the (Intercept): Estimate term (\\(\\hat{\\alpha}\\)), again, specifies the height of the Adelie fitted line and the main effect of bill_length_mm: Estimate (\\(\\hat{\\beta_1}\\)) estimates the relationship (slope) between bill length (mm) and bill depth (mm) for the Adelie penguin. So, here for every 1mm increase in bill length (mm) for the Adelie penguins we estimate, on average, a 0.177mm increase in bill depth (mm). Now, what about Gentoo penguins? Our equation then becomes \\[\\widehat{\\text{bill_depth_mm}} = 11.488 + (0.177 \\times \\text{bill_length_mm}) + (-6.367) + (0.031 \\times \\text{bill_length_mm}).\\] which simplifies to \\[\\widehat{\\text{bill_depth_mm}} = 5.121 + (0.208 \\times \\text{bill_length_mm}).\\] The estimated Gentoo-specific intercept term (y-axis line height) is therefore \\(\\hat{\\alpha} + \\hat{\\beta_3} = 11.488 + (-6.367) = 5.121.\\) The Gentoo-specific bill length (mm) slope is then \\(\\hat{\\beta_1} + \\hat{\\beta_5} = 0.177 + 0.031 = 0.208.\\) So, for every 1mm increase in bill length (mm) for the Gentoo penguins we estimate, on average, a 0.208mm increase in bill depth (mm), a slightly steeper slope than for the estimated Adelie relationship (i.e., we estimate that as Gentoo bills get longer their depth increases at a, slightly, greater rate than those of Adelie penguins). In summary, the main effect of species (i.e., speciesChinstrap: Estimate and speciesGentoo:Estimate ) again give the shift (up or down) of the lines from the Adelie level. However, these lines are no longer parallel (i.e., each species of penguin has a different estimated relationship between bill length and bill depth)! But, now we've specified this all singing and dancing interaction model we might ask are the non-parallel lines non-parallel enough to reject the parallel line model? Look at the plot below; what do you think? "],["model-formula-syntax-in-r.html", "Model formula syntax in R", " Model formula syntax in R In R to specify the model you want to fit you typically create a model formula object; this is usually then passed as the first argument to the model fitting function (e.g., lm()). Some notes on syntax: Consider the model formula example y ~ x + z + x:z. There is a lot going on here: The variable to the left of ~ specifies the response, everything to the right specify the explanatory variables + indicated to include the variable to the left of it and to the right of it (it does not mean they should be summed) : denotes the interaction of the variables to its left and right Additional, some other symbols have special meanings in model formula: * means to include all main effects and interactions, so a*b is the same as a + b + a:b ^ is used to include main effects and interactions up to a specified level. For example, (a + b + c)^2 is equivalent to a + b + c + a:b + a:c + b:c (note (a + b + c)^3 would also add a:b:c) - excludes terms that might otherwise be included. For example, -1 excludes the intercept otherwise included by default, and a*b - b would produce a + a:b Mathematical functions can also be directly used in the model formula to transform a variable directly (e.g., y ~ exp(x) + log(z) + x:z). One thing that may seem counter intuitive is in creating polynomial expressions (e.g., x2). Here the expression y ~ x^2 does not relate to squaring the explanatory variable x (this is to do with the syntax ^ you see above. To include x^2 as a term in our model we have to use the I() (the &quot;as-is&quot; operator). For example, y ~ I(x^2)). Other possible models Let's assume that we have the same data types as in the previous section, specifically, a continuous response (\\(y\\)) and one factor (\\(f\\), with two levels \\(A\\) and \\(B\\)) and one continuous (\\(x\\)) explanatory variable. Assuming that \\(A\\) is the baseline for \\(f\\) the possible models are depicted below. Note that models III and V are forced to have the same intercept for both levels of \\(f\\). In addition, when you have no main effect of \\(x\\), models IV and V, then the model is forced to have no effect of \\(x\\) for the baseline level of \\(f\\) (in this case \\(A\\)). Traditional name Model formula R code Simple regression \\(Y \\sim X_{continuous}\\) lm(Y ~ X) One-way ANOVA \\(Y \\sim X_{categorical}\\) lm(Y ~ X) Two-way ANOVA \\(Y \\sim X1_{categorical} + X2_{categorical}\\) lm(Y ~ X1 + X2) ANCOVA \\(Y \\sim X1_{continuous} + X2_{categorical}\\) lm(Y ~ X1 + X2) Multiple regression \\(Y \\sim X1_{continuous} + X2_{continuous}\\) lm(Y ~ X1 + X2) Factorial ANOVA \\(Y \\sim X1_{categorical} * X2_{categorical}\\) lm(Y ~ X1 * X2) or lm(Y ~ X1 + X2 + X1:X2) "],["model-diagnostics-for-a-linear-model.html", "Model diagnostics for a linear model", " Model diagnostics for a linear model Remember that it is always is imperative that we check the underlying assumptions of our model! If our assumptions are not met then basically the maths falls over and we can't reliably draw inference from the model (e.g., can't trust the parameter estimates etc.). Two of the most important assumption are: equal variances (homogeneity of variance), and normality of residuals. Let's look at the fit of the slm model (single continuous explanatory variable) gglm::gglm(slm_sp) # Plot the four main diagnostic plots Residuals vs Fitted plot You are basically looking for no pattern or structure in your residuals (e.g., a &quot;starry&quot; night). You definitely don't want to see is the scatter increasing around the zero line (dashed line) as the fitted values get bigger (e.g., think of a trumpet, a wedge of cheese, or even a slice of pizza) which would indicate unequal variances (heteroscedacity). Normal quantile-quantile (QQ) plot This plot shows the sorted residuals versus expected order statistics from a standard normal distribution. Samples should be close to a line; points moving away from 45 degree line at the tails suggest the data are from a skewed distribution. Scale-Location plot (\\(\\sqrt{\\text{|standardized residuals vs Fitted|}}\\)) Another way to check the homoskedasticity (constant-variance) assumption. We want the line to be roughly horizontal. If this is the case then the average magnitude of the standardized residuals isn't changing much as a function of the fitted values. We'd also like the spread around the line not to vary much with the fitted values; then the variability of magnitudes doesn't vary much as a function of the fitted values. Residuals vs Leverage plot (standardized residuals vs Leverage) This can help detect outliers in a linear regression model. For linear regression model leverage measures how sensitive a fitted value is to a change in the true response. We're looking at how the spread of standardized residuals changes as the leverage. This can also be used to detect heteroskedasticity and non-linearity: the spread of standardized residuals shouldn't change as a function of leverage. In addition, points with high leverage may be influential: that is, deleting them would change the model a lot. Do you think the residuals are Normally distributed (look at the QQ plot)? Think of what this model is, do you think it's the best we can do? Marginal predictions If the residuals do not show a violation of our model assumptions (let's just pretend the plots above don't show anything for concern) we can produce margins plots, which show the marginal effect of a predictor, holding other variables constant. The ggpredict() function from the ggeffects package can be used for this: ggeffects::ggpredict(slm_sp, terms = &quot;species&quot;) ## # Predicted values of bill_depth_mm ## ## species | Predicted | 95% CI ## -------------------------------------- ## Adelie | 19.38 | [19.15, 19.62] ## Chinstrap | 17.45 | [17.17, 17.73] ## Gentoo | 14.28 | [14.07, 14.49] ## ## Adjusted for: ## * bill_length_mm = 43.99 We can even plot the marginal effects: ggeffects::ggpredict(slm_sp, terms = &quot;species&quot;) %&gt;% ggplot(aes(x = x, y = predicted, label = round(predicted, 3))) + geom_point() + geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .5) + geom_text(hjust = -.2) + ylab( &quot;Predicted bill depth (mm)&quot;) + xlab(&quot;&quot;) + ggtitle(&quot;Fitted means for each species with 95% CIs&quot;, subtitle = &quot;Adjusted for bill_length_mm = 43.99&quot;) + theme_classic() Model selection Are the non-parallel lines non-parallel enough to reject the parallel line model? Now we can compare nested linear models by hypothesis testing. Luckily the R function anova() automates this. Yes the same idea as we've previously learnt about ANOVA! We essentially perform an F-ratio test between the nested models! By nested we mean that one model is a subset of the other (i.e., where some coefficients have been fixed at zero). For example, \\[Y_i = \\beta_0 + \\beta_1z_i + \\epsilon_i\\] is a nested version of \\[Y_i = \\beta_0 + \\beta_1z_i + \\beta_2x_i + \\epsilon_i\\] where \\(\\beta_2\\) has been fixed to zero. As an example consider testing the single explanatory variable model slm against the same model with species included as a variable slm_sp. To carry out the appropriate hypothesis test in R we can run anova(slm,slm_sp) ## Analysis of Variance Table ## ## Model 1: bill_depth_mm ~ bill_length_mm ## Model 2: bill_depth_mm ~ bill_length_mm + species ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 331 1220.16 ## 2 329 299.62 2 920.55 505.41 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 As you'll see the anova() function takes the two model objects (slm and slm_sp) each as arguments. It returns an ANOVA testing whether the more complex model (slm_sp) is just as good at capturing the variation in the data as the simpler model (slm). The returned p-value should be interpreted as in any other hypothesis test. i.e., the probability of observing a statistic as least as extreme under our null hypothesis (here that each model is as good at capturing the variation in the data). What would we conclude here? I'd say we have pretty strong evidence against the models being equally good! I'd definitely plump for slm_sp over slm, looking back at the plots above does this make sense? Now what about slm_int vs slm_sp? anova(slm_sp,slm_int) ## Analysis of Variance Table ## ## Model 1: bill_depth_mm ~ bill_length_mm + species ## Model 2: bill_depth_mm ~ bill_length_mm * species ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 329 299.62 ## 2 327 298.62 2 0.99284 0.5436 0.5812 So it seems both models are just as good at capturing the variation in our data: we're happy with the parallel lines! Another way we might compare models is by using the Akaike information criterion (AIC) (you'll see more of this later in the course). AIC is an estimator of out-of-sample prediction error and can be used as a metric to choose between competing models. Between nested models we're looking for the smallest AIC (i.e., smallest out-of-sample prediction error). Typically, a difference of 4 or more is considered to indicate an improvement; this should not be taken as writ however, using multiple comparison techniques is advised. R already has an AIC() function that can be used directly on your lm() model object(s). For example, AIC(slm,slm_sp,slm_int) ## df AIC ## slm 3 1383.4462 ## slm_sp 5 919.8347 ## slm_int 7 922.7294 This backs up what our ANOVA suggested model slm_sp as that preferred! As always it's important to do a sanity check! Does this make sense? Have a look at the outputs from these models and see what you think. Just because we've chosen a model (the best of a bad bunch perhaps) this doesn't let us off the hook. We should check our assumptions gglm::gglm(slm_sp) # Plot the four main diagnostic plots Residuals vs Fitted plot: equal spread? Doesn't look too trumpety! Normal quantile-quantile (QQ) plot: skewed? Maybe slightly right skewed (deviation upwards from the right tail) Scale-Location plot: equal spared? I'd say so. Residuals vs Leverage: ? Maybe a couple of points with high leverage. "],["inference-for-a-linear-model.html", "Inference for a linear model", " Inference for a linear model After all that what do estimated parameters mean? Assuming our model assumptions are met we can draw inference based on the estimated parameter values. Point prediction Using the slm_sp model we can make a point prediction for the expected bill depth (mm) for Gentoo penguins with a bill length of 50mm. Recall the model equation is \\[ \\begin{aligned} \\operatorname{bill\\_depth\\_mm} &amp;= \\alpha + \\beta_{1}(\\operatorname{bill\\_length\\_mm})\\ + \\\\ &amp;\\quad \\beta_{2}(\\operatorname{species}_{\\operatorname{Chinstrap}}) + \\beta_{3}(\\operatorname{species}_{\\operatorname{Gentoo}})\\ + \\\\ &amp;\\quad \\epsilon \\end{aligned} \\] The fitted equation is given as \\[ \\begin{aligned} \\operatorname{\\widehat{bill\\_depth\\_mm}} &amp;= 10.57 + 0.2(\\operatorname{bill\\_length\\_mm})\\ - \\\\ &amp;\\quad 1.93(\\operatorname{species}_{\\operatorname{Chinstrap}}) - 5.1(\\operatorname{species}_{\\operatorname{Gentoo}}) \\end{aligned} \\] We can then simply substitute in the values: \\[\\widehat{\\text{bill depth}} = \\hat{\\alpha} + \\hat{\\beta_1}*50 + \\hat{\\beta_3}*1\\] \\[\\downarrow\\] \\[\\widehat{\\text{bill depth}} = 10.56 + 0.20*50 - 5.10*1\\] \\[\\downarrow\\] \\[15.47\\text{mm}\\] Rather than by hand we can do this easily in R ## create new data frame with data we want to predict to ## the names have to match those in our original data frame newdata &lt;- data.frame(species = &quot;Gentoo&quot;,bill_length_mm = 50) ## use predict() function predict(slm_sp, newdata = newdata) ## more accurate than our by hand version! ## 1 ## 15.4841 What does this look like on a plot? Confidence intervals for parameters For the chosen slm_sp model we can get these simply by using confint(). By default the 95% intervals are returned: cis &lt;- confint(slm_sp) cis ## 2.5 % 97.5 % ## (Intercept) 9.2060707 11.9244526 ## bill_length_mm 0.1656635 0.2352227 ## speciesChinstrap -2.3771120 -1.4890438 ## speciesGentoo -5.4857298 -4.7209009 So this tells us that For every 1mm increase in bill length we estimate the expected bill depth to increases between 0.166 and 0.235 mm We estimate that the expected bill depth of a Chinstrap penguin is between 1.5 and 2.4 mm shallower than the Adelie penguin "],["module-3.html", "Module 3", " Module 3 Learning objectives Identify the experimental unit(s) and the observational unit(s) for a given experimental design List and describe the three main principals of experimental design randomization replication blocking Calculate Sums of Squares (between and within groups) given the observations Define and state the appropriate degrees of freedom in a one-way ANOVA scenario Calculate the F-statistics given the appropriate Sums of Squares and degrees of freedom Interpret and discuss a given p-value in the context of a stated hypothesis test Explain between group and within group variation Communicate statistical concepts and experimental outcomes clearly using language appropriate for both a scientific and non-scientific audience Describe a Completely Randomised (experimental) Design Carry out linear regression in R with one categorical explanatory variable (one-way ANOVA) and draw the appropriate inference Communicate statistical concepts and experimental outcomes clearly using language appropriate for both a scientific and non-scientific audience Describe and discuss factorial experiments with both equal and unequal replication Carry out linear regression in R with two categorical explanatory variables and an interaction (two-way ANOVA with interaction) and draw the appropriate inference Calculate the marginal means for a balanced and unbalanced design Describe a Randomized Complete Block Design (RCBD) and a Split-plot Design Carry out analysis of the above designs in R using aov(), lm(), and lmer() and discuss and compare the three Define a fixed and random effect in the context of experimental design Describe and discuss split-plot experimental design Describe and discuss repeated measures experiments Write R code to visualise repeated measures data Carry out appropriate analysis in R and draw the appropriate inference Discuss and critique methods for controlling errors in hypothesis testing, for example Fisher‚Äôs LSD and the Bonferroni Correction Detail and draw inference form multiple comparison procedures such as Tukey‚Äôs HSD and Dunnett's test Describe the family wise error rate (FWER) and false discover rate (FDR) in the context of multiple comparisons Calculate the marginal means for a balanced and unbalanced design Communicate statistical concepts and experimental outcomes clearly using language appropriate for both a scientific and non-scientific audience Other resources Glass, David J. Experimental Design for Biologists. Second ed. 2014. Print. Welham, S. J. Statistical Methods in Biology : Design and Analysis of Experiments and Regression. 2015. Print. Fisher, Ronald Aylmer. The Design of Experiments. 8th ed. Edinburgh: Oliver &amp; Boyd, 1966. Print. O &amp; B Paperbacks. Analysis of variance with unbalanced data: an update for ecology &amp; evolution - Hector - 2010 - Journal of Animal Ecology "],["introduction-to-the-design-and-analysis-of-experiments.html", "Introduction to the design and analysis of experiments", " Introduction to the design and analysis of experiments &quot;A useful property of a test of significance is that it exerts a sobering influence on the type of experimenter who jumps to conclusions on scanty data, and who might otherwise try to make everyone excited about some sensational treatment effect that can well be ascribed to the ordinary variation in [their] experiment.&quot; --- Gertrude Mary Cox Key phrases *An experiment is a procedure (or set of actions) where a researcher intentionally changes some factor/treatment/variable to observe the effect of their actions. As mentioned above, the collection of observational data is not experimentation. An experimental unit is the smallest portion of experimental material which is independently perturbed. This is the item under study for which some variable (treatment) is changed. For example this could be a human subject or an agricultural plot. An observational unit (or subsample) is the smallest unit on which a response is measured. If the experimental unit is split after the treatment has been applied (e.g., multiple samples taken from one human subject) then this sample is called a subsample or observational unit. If one measurement is made on each experimental unit then the observational unit = the experimental unit. If multiple measurements are made on each subject (e.g., human) then each experimental unit has &gt;1 observational unit. This is then pseudo- or technical replication (see below). A treatment (or independent variable or factor or treatment factor) is an experimental condition independently applied to an experimental unit. It is one of the variables that is controlled by the researcher during the experiment (e.g., drug type). The values of the treatments within a set are called levels. The dependent variable or response is the output (or thing) that is measured after an experiment. This is what the researcher measures and assesses if changing the treatment(s) (i.e., independent variable(s)) induces any change. An effect is the change in the response variable caused by the controlled changes in the independent variable. Whether the magnitude of the effect (it's size) is significant (or or any practical interest) is determined by the researcher after carrying out some appropriate analyses. The three key principles: R. A. Fisher's work in the area of experimental design is, perhaps, the most well known in the field. The principles he devised we still abide by today, see below. Note, however, to give a balanced view of the celebrated mathematician many of his views (on eugenics and race in particular) are abhorrent to many. I would urge you to read up on his work in this area and come to your own conclusions. Replication Biological replication: each treatment is independently applied to each of several humans, animals or plants. Why? To generalize results to population. Technical replication: two or more samples from the same biological source which are independently processed. Why? Advantageous if processing steps introduce a lot of variation; increases the precision with which comparisons of relative abundances between treatments are made. Pseudo-replication: one sample from the same biological source, divided into two or more aliquots which are independently measured. Why? Advantageous for noisy measuring instruments; increases the precision with which comparisons of relative abundances between treatments are made. Randomization The main reason to randomise allocation of treatment to experimental units is to protect against bias. We, typically, wish to plan the experiment in such a way that the variations caused by extraneous factors can all be combined under the general heading of &quot;chance&quot;. Doing so ensures that each treatment has the same probability of getting good (or bad) units and thus avoids systematic bias. Random allocation can cancel out population bias; it ensures that any other possible causes for the experimental results are split equally between groups. Typically statistical analysis assumes that observations are independent. This is almost never strictly true in practice but randomisation means that our estimates will behave as if they were based on independent observations. Blocking Blocking helps control variability by making treatment groups more alike. Experimental units are divided into subsets (called blocks) so that units within the same block are more similar than units from different subsets or blocks. Blocking is a technique for dealing with nuisance factors. A nuisance factor is a factor that has some effect on the response, but is of no interest. Setting up an experiment When setting up and experiment and/or considering experimental data there are questions we should always answer the following questions. What are the objectives of the study? Why are you carrying out this experiment? What is the scientific question? What specific hypothesis do you hope to test/investigate? What are the experimental units? What characteristics of the experimental units can be measured? How large is an meaningful effect size in this context? How many treatments are to be studied and what are they? How does the experimenter apply the treatments to the available experimental units and then observe the responses? What are the potential sources of variation? Can you control for the unwanted sources or variation? Can you name all likely sources? Can the resulting design be analyzed or can the desired comparisons be made? **Can you describe and detail the appropriate analysis?* A simple example: Charlotte's coffee Last Christmas I was given a gift set of three types of coffee beans. I want to know which makes the darkest coffee; to do this I measure the opacity after the coffee is made. Three types of coffee beans are: Arabica , Liberica , and Robusta . 12 identical cups are chosen and sets of four cups are randomly allocated one of three treatments. Four sets of each type of coffee beans are ground and cups made (in the same way) resulting in a total of 12 cups of coffee, see below. Samples are taken from each cup and the coffee colour is measured. Experiment design: Scientific question: Does coffee colour differ between bean types of coffee beans? There are 3 treatments (types of coffee beans): Arabica, Liberica, and Robusta. In this case the experimental unit would be the coffee cup as each one is allocated a different bean type (treatment). The observational unit changes depending on the scenario (i.e., what and how samples are taken). For example, if a single ml of liquid is taken from each cup and one measurement is taken per ml taken \\(\\rightarrow\\) the observational unit would be the cup; if single ml of liquid is taken from each cup and two subsamples are then taken from each ml, then if measurements are taken per subsample \\(\\rightarrow\\) then the observational unit would be a subsample; if four \\(\\times\\) 1 ml of liquid were taken from each cup and from each ml a measurement is taken \\(\\rightarrow\\) then the observational unity would be each 1ml sample. "],["some-basic-experimental-designs.html", "Some basic experimental designs", " Some basic experimental designs Completely randomised design (CRD) Let's consider a completely randomised design with one treatment factor (e.g., coffee bean type). Here, \\(n\\) experimental units (e.g., cups) are divided randomly into \\(t\\) groups. Random allocation can be achieved by simply drawing lots from a hat! To be more rigorous, though, we could use R's sample() function (have a go yourself and see if you can work out how to wield sample()). Each group is then given one treatment level (one of the treatment factors). As we have defined only one treatment factor all other known independent variables are kept constant so as to not bias any effects (e.g., cup type and type in Charlotte's‚òï experiment). An illustration of a CRD with one tratment factor and three treatment levels (A, B, &amp; C) Designing a CRD using R Let's assume you want to set up an experiment similar to the coffee one above using R to do the random allocation of treatments for you. ## create a character vector of bean types beans &lt;- rep(c(&quot;Arabica&quot;,&quot;Liberica&quot;, &quot;Robusta&quot;), each = 4) beans ## [1] &quot;Arabica&quot; &quot;Arabica&quot; &quot;Arabica&quot; &quot;Arabica&quot; &quot;Liberica&quot; &quot;Liberica&quot; ## [7] &quot;Liberica&quot; &quot;Liberica&quot; &quot;Robusta&quot; &quot;Robusta&quot; &quot;Robusta&quot; &quot;Robusta&quot; ## randomly sample the character vector to give the order of coffees set.seed(1234) ## this is ONLY for consistency, remove if doing this yourself allocation &lt;- sample(beans, 12) allocation ## [1] &quot;Robusta&quot; &quot;Robusta&quot; &quot;Liberica&quot; &quot;Liberica&quot; &quot;Arabica&quot; &quot;Liberica&quot; ## [7] &quot;Arabica&quot; &quot;Robusta&quot; &quot;Arabica&quot; &quot;Liberica&quot; &quot;Robusta&quot; &quot;Arabica&quot; Having run the code above your CRD plan is as follows Cup Bean 1 Robusta 2 Robusta 3 Liberica 4 Liberica 5 Arabica 6 Liberica 7 Arabica 8 Robusta 9 Arabica 10 Liberica 11 Robusta 12 Arabica Randomised complete block design (RCBD) Let's consider a randomised complete block design with one treatment factor (e.g., coffee bean type). If the treatment factor has \\(t\\) levels there will be \\(b\\) blocks that each contain \\(t\\) experimental units resulting in a total of \\(t\\times b\\) experimental units. For example, let's imagine that for the coffee experiment we had two cup types: mugs and heatproof glasses. We might consider the type of receptacle to have an effect on the coffee colour measured, however, we are not interested in this. Therefore, to negate this we block by cup type. This means that any effect due to the blocking factor (cup type) is accounted for by the blocking. For a blocked design we want the \\(t\\) experimental units within each block should be as homogeneous as possible (as similar as possible, so that there is unlikely to be unwanted variation coming into the experiment this way). The variation between blocks (the groups of experimental units) should be large enough (i.e., blocking factors different enough) so that conclusions can be drawn. Allocation of treatments to experimental units is done randomly (i.e., treatments are randomly assigned to units) within each block. An illustration of a CRD with one tratment factor, three treatment levels (A, B, &amp; C), and three blocks (rows) Designing a RCBD using R Let's assume you want to set up an experiment similar to the coffee one above; however, now you are in the situation where you have two types of cups (six mugs and six heatproof glasses). Below we use R to do the random allocation of treatments within each block (cup type) for you. Here we have \\(t = 3\\) treatments (bean types) and \\(b = 2\\) blocks (cup types) so we will have \\(t \\times b = 6\\) experimental units in total. set.seed(432) ## this is ONLY for consistency, remove if doing this yourself plan &lt;- data.frame(Beans = rep(c(&quot;Arabica&quot;,&quot;Liberica&quot;, &quot;Robusta&quot;), times = 2), Block = rep(c(&quot;Mug&quot;, &quot;Glass&quot;), each = 3)) %&gt;% ## combine experiment variables group_by(Block) %&gt;% ## group by blocking factor dplyr::sample_n(3) plan ## # A tibble: 6 √ó 2 ## # Groups: Block [2] ## Beans Block ## &lt;chr&gt; &lt;chr&gt; ## 1 Robusta Glass ## 2 Arabica Glass ## 3 Liberica Glass ## 4 Robusta Mug ## 5 Arabica Mug ## 6 Liberica Mug Having run the code above your RCBD plan is as follows Cup Beans Block 1 Robusta Glass 2 Arabica Glass 3 Liberica Glass 1 Robusta Mug 2 Arabica Mug 3 Liberica Mug Factorial design A factorial experiment is one where there are two or more sets of (factor) treatments. Rather than studying each factor separately all combinations of the treatment factors are considered. Factorial designs enable us to infer any interaction effects, which may be present. An interaction effect is one where the effect of one variable depends on the value of another variable (i.e., the effect of one treatment factor on the response variable will change depending on the value of a second treatment factor.) For example, we could add another treatment to the coffee experiment above: grinder type (manual or electric ). So, now we have two treatments, bean type (with three levels) and cup type (with two levels). Recall that in a factorial experiment we want to study all combinations of the levels of each factor: Manual Electric Arabica A.M A.E Liberica L.M L. E Robusta R.M R. E Note is a factorial design has equal numbers of replicates in each group then it is said to be a balanced design; if this is not the case then it is unbalanced. Now, our question about the colour of coffee would be Does the colour of the difference in the colour of the coffee between the beans wall depend on the grinder used? Possible outcomes Interactions between two factor treatments If an interaction effect exists the effect of one factor on the response will change depending on the level of the other factor: The plot above is called an interaction plot, think back to the previous module. Creating such a plot is often very useful when drawing inference; in this instance we can see that the colour of the coffee changes depending on the type of coffee bean used, however, this relationship differs depending on the type of grinder used. For example, Liberica beans produce darker coffee than the other two beans when the electric grinder is used, but weaker coffee when the manual grinder is used. "],["modelling-experimental-data.html", "Modelling experimental data", " Modelling experimental data A completely randomised design (CRD) as a linear model As we've seen in the previous module that we can write a linear model with a single explanatory variable as \\[Y_i = \\alpha + \\beta_1x_i + \\epsilon_i\\] When dealing with factor variables we use dummy variables and can write the above as \\[Y_{ik} = \\alpha + \\tau_k + \\epsilon_{ik}\\] where \\(\\tau_k\\) is called an effect and represents the difference between the overall average, \\(\\alpha\\), and the average at the \\(k_{th}\\) treatment level. The errors \\(\\epsilon_{ik}\\) are again assumed to be normally distributed and independent due to the randomisation (i.e., \\(\\epsilon_{ik} \\sim N(0, \\sigma^2)\\). Or you might think of the model as \\[Y_{ik} = \\mu_k + \\epsilon_{ik}\\] where \\(Y_{ik}\\) is the response (i.e., observed coffee opacity) for the \\(i^{th}\\) experimental unit (i.e., coffee cup) subjected to the \\(k^{th}\\) level of the treatment factor (i.e., coffee type). Here \\(\\mu_k\\) are the different (cell) means for each level of the treatment factor. See below for an illustration of this for three factor treatment levels (as in the coffee example above). Analysis of a CRD in R In this section we again consider the data containing logAUC values for 12 rats subjected to three different treatments (Surgery), C, P, and S: Surgery Rat logAUC C 1 8.49 C 2 8.20 C 3 9.08 C 4 8.07 P 1 10.24 P 2 7.72 P 3 9.34 P 4 8.50 S 1 11.31 S 2 12.69 S 3 11.37 S 4 10.82 To read the data directly into R you can use readr::read_csv(&quot;https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/crd_rats_data.csv&quot;) We could analyse these data using aov(): rats_aov &lt;- aov(logAUC ~ Surgery, data = rats) summary(rats_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Surgery 2 22.026 11.013 16.36 0.00101 ** ## Residuals 9 6.059 0.673 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Hypothesis: We test the Null hypothesis, \\(H_0\\), population (Surgery) means are the same on average verses the alternative hypothesis, \\(H_1\\), that at least one differs from the others! Probability of getting an F-statistic at least as extreme as the one we observe (think of the area under the tails of the curve below) p-value Pr(&gt;F)= 0.001 tells us we have sufficient evidence to reject \\(H_0\\) at the 1% level of significance Alternatively, we could use lm(): rats_lm &lt;- lm(logAUC ~ Surgery, data = rats) summary(rats_lm)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.4600 0.4102531 20.6214144 6.930903e-09 ## SurgeryP 0.4900 0.5801856 0.8445574 4.202408e-01 ## SurgeryS 3.0875 0.5801856 5.3215734 4.799872e-04 So, what does this tell us about which pairs of means are different? To carry out a pair-wise comparisons of means we can use two-sample t-tests, calculating our observed t-value where \\(\\text{t-value} = \\frac{\\text{Sample Difference}_{ij} - \\text{Difference assuming } H_0 \\text{ is true}_{ij}}{\\text{SE of } \\text{Sample Difference}_{ij}}\\). Here, \\(\\text{Sample Difference}_{ij}\\) = Difference between pair of sample means. We can then compute the p-value for observed t-value. The output above has already done this for us: (Intercept) = \\(\\text{mean}_C\\) = 8.46 SE of (Intercept) = SE of \\(\\text{mean}_C\\) = SEM = 0.4102531 \\(\\text{Surgery}_P\\) = \\(\\text{mean}_P\\) ‚Äì \\(\\text{mean}_C\\) = 0.49 SE of \\(\\text{Surgery}_P\\) = SE of (\\(\\text{mean}_P\\) - \\(\\text{mean}_C\\) ) = SED = 0.5801856 Hypotheses being tested The t value and Pr (&gt;|t|) are the t-statistic and p-value for testing the null hypotheses listed below 1. Mean abundance is zero for C population 2. No difference between the population means of P and C 3. No difference between the population means of S and C We're interested in 2 and 3, but not necessarily 1! F-test: anova(rats_lm) ## Analysis of Variance Table ## ## Response: logAUC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Surgery 2 22.0263 11.0132 16.359 0.001006 ** ## Residuals 9 6.0591 0.6732 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The same as aov() in fact aov() is calling lm() in the background. Diagnostic plots Carrying out any linear regression recall that we have some key assumptions Independence There is a linear relationship between the response and the explanatory variables The residuals have constant variance The residuals are normally distributed gglm::gglm(rats_lm) # Plot the four main diagnostic plots What do you think? Look back at module 2. A Factorial experiment (as a CRD) Equal replications (balanced design) Data Global metabolic profiling and comparison of relative abundances of proteins (logAUC) in the inner and outer left ventricle (innerLV and outerLV) wall of diabetic and healthy male Wistar rats. To read the data directly into R you can use readr::read_csv(&quot;https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/factorial_expt.csv&quot;) Organ Diabetic Healthy innerLV n = 3 n = 3 outerLV n = 3 n = 3 Disease Organ Animal Sample logAUC Healthy innerLV 1 1 9.40 Healthy outerLV 2 2 8.83 Healthy innerLV 3 1 10.33 Healthy outerLV 4 2 10.49 Healthy innerLV 5 1 9.74 Healthy outerLV 6 2 10.98 Diabetic innerLV 7 1 7.92 Diabetic outerLV 8 2 9.37 Diabetic innerLV 9 1 8.69 Diabetic outerLV 10 2 11.31 Diabetic innerLV 11 1 7.01 Diabetic outerLV 12 2 9.29 Fitting models with interactions using lm() ## change to factors (saves errors later on) factorial$Disease &lt;- as.factor(factorial$Disease) factorial$Organ &lt;- as.factor(factorial$Organ) fac_lm &lt;- lm(logAUC ~ Disease*Organ, data = factorial) summary(fac_lm)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.873333 0.5405835 14.564508 4.841215e-07 ## DiseaseHealthy 1.950000 0.7645006 2.550685 3.413826e-02 ## OrganouterLV 2.116667 0.7645006 2.768692 2.434579e-02 ## DiseaseHealthy:OrganouterLV -1.840000 1.0811671 -1.701865 1.271934e-01 So, the full model is \\[ \\begin{aligned} \\operatorname{\\widehat{logAUC}} &amp;= 7.87 + 1.95(\\operatorname{Disease}_{\\operatorname{Healthy}}) + 2.12(\\operatorname{Organ}_{\\operatorname{outerLV}}) - 1.84(\\operatorname{Disease}_{\\operatorname{Healthy}} \\times \\operatorname{Organ}_{\\operatorname{outerLV}}) \\end{aligned} \\] The three gobal null hypotheses being tested are \\(H_0: \\hat{\\mu}_{\\text{Diabetic}} = \\hat{\\mu}_{\\text{Healthy}}\\) \\(H_0: \\hat{\\mu}_{\\text{innerLV}} = \\hat{\\mu}_{\\text{outerLV}}\\) \\(H_0: \\hat{\\mu}_{\\text{Diabetic,innerLV}} = \\hat{\\mu}_{\\text{Diabetic,outerLV}} = \\hat{\\mu}_{\\text{Healthy,innerLV}} = \\hat{\\mu}_{\\text{Healthy,outerLV}}\\) anova(fac_lm) ## Analysis of Variance Table ## ## Response: logAUC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Disease 1 3.1827 3.1827 3.6304 0.09320 . ## Organ 1 4.2960 4.2960 4.9003 0.05775 . ## Disease:Organ 1 2.5392 2.5392 2.8963 0.12719 ## Residuals 8 7.0135 0.8767 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 What conclusions do you draw? Note with a balanced design ordering of term doesn't matter. For example, fac_lm &lt;- lm(logAUC ~ Disease*Organ, data = factorial) anova(fac_lm) ## Analysis of Variance Table ## ## Response: logAUC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Disease 1 3.1827 3.1827 3.6304 0.09320 . ## Organ 1 4.2960 4.2960 4.9003 0.05775 . ## Disease:Organ 1 2.5392 2.5392 2.8963 0.12719 ## Residuals 8 7.0135 0.8767 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 fac_lm_2 &lt;- lm(logAUC ~ Organ*Disease, data = factorial) anova(fac_lm_2) ## Analysis of Variance Table ## ## Response: logAUC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Organ 1 4.2960 4.2960 4.9003 0.05775 . ## Disease 1 3.1827 3.1827 3.6304 0.09320 . ## Organ:Disease 1 2.5392 2.5392 2.8963 0.12719 ## Residuals 8 7.0135 0.8767 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Unqual replications (unbalanced design) Here, we consider a subset of the data above. Organ Diabetic Healthy innerLV n = 3 n = 1 outerLV n = 2 n = 2 Disease Organ Animal Sample logAUC Healthy outerLV 4 2 10.49 Healthy innerLV 5 1 9.74 Healthy outerLV 6 2 10.98 Diabetic innerLV 7 1 7.92 Diabetic outerLV 8 2 9.37 Diabetic innerLV 9 1 8.69 Diabetic innerLV 11 1 7.01 Diabetic outerLV 12 2 9.29 Fitting models with interactions using lm() Note: order matters. For example, fac_lm &lt;- lm(logAUC ~ Disease*Organ, data = unbalanced_nafree) anova(fac_lm) ## Analysis of Variance Table ## ## Response: logAUC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Disease 1 7.1102 7.1102 18.4955 0.01264 * ## Organ 1 3.1149 3.1149 8.1027 0.04656 * ## Disease:Organ 1 0.0913 0.0913 0.2376 0.65145 ## Residuals 4 1.5377 0.3844 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 fac_lm_2 &lt;- lm(logAUC ~ Organ*Disease, data = unbalanced_nafree) anova(fac_lm_2) ## Analysis of Variance Table ## ## Response: logAUC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Organ 1 5.7291 5.7291 14.9029 0.01814 * ## Disease 1 4.4960 4.4960 11.6953 0.02678 * ## Organ:Disease 1 0.0913 0.0913 0.2376 0.65145 ## Residuals 4 1.5377 0.3844 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The three global null hypotheses being tested are (essentially) the same \\(H_0: \\hat{\\mu}_{\\text{Diabetic}} = \\hat{\\mu}_{\\text{Healthy}}\\) \\(H_0: \\hat{\\mu}_{\\text{innerLV}} = \\hat{\\mu}_{\\text{outerLV}}\\) \\(H_0: \\hat{\\mu}_{\\text{Diabetic,innerLV}} = \\hat{\\mu}_{\\text{Diabetic,outerLV}} = \\hat{\\mu}_{\\text{Healthy,innerLV}} = \\hat{\\mu}_{\\text{Healthy,outerLV}}\\) However, now the order the terms affects the estimation. Look carefully at the anova() outputs above; what to you notice about the sums of squares (Sum Sq) values? For a balanced two-factor experiment (e.g., in the previous section) partitioning of the sums of squares (\\(SS\\)) is additive: \\[SS_\\text{Treatment} = SS_\\text{Disease} + SS_\\text{Organ} + SS_\\text{Disease:Organ}.\\] The order in which the main effects are added to the model does not matter. However, for an unbalanced design this is not the case as the sums of squares are calculated sequentially. Note that sequentially calculated \\(SS\\) are known as Type I \\(SS\\). Sums of squares (\\(SS\\)) Sequential (Type I \\(SS\\)) As a term enters the model its \\(SS\\) is calculated, which is then subtracted from the total \\(SS\\). This then reduces the available \\(SS\\) for the next term entering the model. When treatment combinations in a factorial experiment are unequally replicated, their effects are not mutually independent, so that the order in which terms enter the model matters. Considering the data above if we include Disease as a main effect first (i.e., Disease*Organ) then the \\(SS_\\text{Disease}\\) is calculated first ignoring the Organ main effect. Here, some Organ information is confounded with Disease information (i.e., variation due to Organ confounded by the variation due to Disease). Then \\(SS_\\text{Organ}\\) are calculated having been adjusted for the Diease main effect. This now only contains Organ information (i.e., variation due to Organ effect) since all the Disease information was eliminated in previous step. Finally, \\(SS_\\text{Disease:Organ}\\) are calculated adjusted for both \\(SS_\\text{Disease}\\) and \\(SS_\\text{Organ}\\). Here, there is no information left relating to Disease or Organ main effects. What does this look like? For Disease*Organ we calculate \\(SS_\\text{Disease}\\) ignoring any Organ effect (if you are unsure what each line of code is doing I suggest you run it line by line to see what's being done at each step). unbalanced_nafree %&gt;% mutate(grand_mean = mean(logAUC)) %&gt;% group_by(Disease) %&gt;% summarise(n = n(), treatment_mean = mean(logAUC), grand_mean = mean(grand_mean)) %&gt;% mutate(ss_disease = n * (treatment_mean - grand_mean)^2) %&gt;% pull(ss_disease) %&gt;% sum() ## [1] 7.110201 This matches, as we'd expect, \\(SS_\\text{Disease}\\) calculated from the Disease*Organ model above. But, that about the sequential \\(SS\\), a simple way to think about this is in terms of sequential models: ## Null model. fit.null &lt;- lm(logAUC ~ 1, data = unbalanced_nafree) ## Only Factor Disease. fit.a &lt;- lm(logAUC ~ Disease, data = unbalanced_nafree) ## Factors Disease and Organ. fit.ab &lt;- lm(logAUC ~ Disease + Organ, data = unbalanced_nafree) ## Factors Disease and Organ with interaction. fit.abi &lt;- lm(logAUC ~ Disease*Organ, data = unbalanced_nafree) ## ANOVA table, as above anova(fit.abi) ## Analysis of Variance Table ## ## Response: logAUC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Disease 1 7.1102 7.1102 18.4955 0.01264 * ## Organ 1 3.1149 3.1149 8.1027 0.04656 * ## Disease:Organ 1 0.0913 0.0913 0.2376 0.65145 ## Residuals 4 1.5377 0.3844 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## First line. sum(residuals(fit.null)^2) - sum(residuals(fit.a)^2) ## [1] 7.110201 ## Second line. sum(residuals(fit.a)^2) - sum(residuals(fit.ab)^2) ## [1] 3.114926 ## Third line. sum(residuals(fit.ab)^2) - sum(residuals(fit.abi)^2) ## [1] 0.09134405 ## Final line. sum(residuals(fit.abi)^2) ## [1] 1.537717 Type II \\(SS\\) Rather than calculating \\(SS\\) sequentially we can calculate the \\(SS\\) for a given effect adjusting for all other effects listed in the model. This means that the \\(SS_\\text{A}\\) and \\(SS_\\text{B}\\) main effects will both be adjusted for each other (since neither contains the other), but will not be adjusted for \\(SS_\\text{A:B}\\) (since it contains both A and B). \\(SS_\\text{A:B}\\) will be adjusted for both main effects. In our example above \\(SS_\\text{Disease}\\) and \\(SS_\\text{Organ}\\) main effects will both be adjusted for each other, but will not be adjusted for \\(SS_\\text{A:B}\\) and \\(SS_\\text{Disease:Organ}\\) will be adjusted for both main effects. We can calculate the Type II \\(SS\\) table in R buy either Extracting the main effect rows that have been adjusted for the other model terms from two Type I \\(SS\\) tables (each one having the terms specified in a different order): ## Type II Organ SS anova(fac_lm)[2, ] ## Analysis of Variance Table ## ## Response: logAUC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Organ 1 3.1149 3.1149 8.1027 0.04656 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Type II Disease SS anova(fac_lm_2)[2, ] ## Analysis of Variance Table ## ## Response: logAUC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Disease 1 4.496 4.496 11.695 0.02678 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Type II Organ:Disease/Disease:Organ anova(fac_lm_2)[3, ] ## Analysis of Variance Table ## ## Response: logAUC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Organ:Disease 1 0.091344 0.091344 0.2376 0.6514 or, Using an inbuilt R function (does not matter which order the model has the terms specified): car::Anova(fac_lm, type = 2) ## Anova Table (Type II tests) ## ## Response: logAUC ## Sum Sq Df F value Pr(&gt;F) ## Disease 4.4960 1 11.6953 0.02678 * ## Organ 3.1149 1 8.1027 0.04656 * ## Disease:Organ 0.0913 1 0.2376 0.65145 ## Residuals 1.5377 4 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note: Type III \\(SS\\) and beyond are not covered in this course but I would recommend reading this recent paper for an intuitive overview. Marginal means Balanced design Table 1: logAUC data from a balanced design Disease Organ logAUC Healthy innerLV 9.40 Healthy outerLV 8.83 Healthy innerLV 10.33 Healthy outerLV 10.49 Healthy innerLV 9.74 Healthy outerLV 10.98 Diabetic innerLV 7.92 Diabetic outerLV 9.37 Diabetic innerLV 8.69 Diabetic outerLV 11.31 Diabetic innerLV 7.01 Diabetic outerLV 9.29 Grand mean 9.447 Table 1: Cell means Disease Organ log AUC mean Diabetic innerLV 7.873 Diabetic outerLV 9.990 Healthy innerLV 9.823 Healthy outerLV 10.100 Table 1: Marginal (organ) mean Organ log AUC mean innerLV 8.848 outerLV 10.045 Table 1: Marginal (disease) mean Disease log AUC mean Diabetic 8.932 Healthy 9.962 Unbalanced design (Unequal replication due to missing data) Table 2: logAUC data from an unbalanced design Disease Organ logAUC Healthy innerLV NA Healthy outerLV NA Healthy innerLV NA Healthy outerLV 10.49 Healthy innerLV 9.74 Healthy outerLV 10.98 Diabetic innerLV 7.92 Diabetic outerLV 9.37 Diabetic innerLV 8.69 Diabetic outerLV NA Diabetic innerLV 7.01 Diabetic outerLV 9.29 Grand mean 9.186 Table 3: Cell means Disease Organ log AUC mean Diabetic innerLV 7.873 Diabetic outerLV 9.330 Healthy innerLV 9.740 Healthy outerLV 10.735 Everything is as we'd expect up until now, but what about the marginal means? The, perhaps, most obvious way would be to do the following (i.e., ignore subgroups, hence give all observations equal weight)? unbalanced_nafree %&gt;% dplyr::select(c(Disease, logAUC)) %&gt;% group_by(Disease) %&gt;% summarise(Mean = mean(logAUC)) ## # A tibble: 2 √ó 2 ## Disease Mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 Diabetic 8.46 ## 2 Healthy 10.4 However, as a result of this the means are biased towards groups with greater replication! To avoid this we give the subgroups (Organ) cell means equal weight (this is sometimes called the least squares mean): unbalanced_nafree %&gt;% dplyr::select(c(Disease, Organ, logAUC)) %&gt;% group_by(Organ, Disease) %&gt;% mutate(n = n()) %&gt;% ## count observations in each group ## then calculate weighted mean based on the no. observations summarise(weighted_mean = weighted.mean(logAUC, w = 1/n)) %&gt;% group_by(Disease) %&gt;% ## calculate mean of weighted means summarise(mean = mean(weighted_mean)) ## # A tibble: 2 √ó 2 ## Disease mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 Diabetic 8.60 ## 2 Healthy 10.2 Now see if you can do the same across Organ to get Table 4: Marginal (organ) means Organ mean innerLV 8.807 outerLV 10.032 What does all of this tell us? When calculating marginal means for unbalanced designs we need to be careful! Luckily there are inbuilt functions in R that do this correctly for us! See the next section for an example using the predictmeans function from the predictmeans package. "],["multiple-comparisons.html", "Multiple comparisons", " Multiple comparisons Recall that each time we carry out a hypothesis test the probability we get a false positive result (type I error) is given by \\(\\alpha\\) (the level of significance we choose). When we have multiple comparisons to make we should then control the Type I error rate across the entire family of tests under consideration, i.e., control the Family-Wise Error Rate (FWER); this ensures that the risk of making at least one Type I error among the family of comparisons in the experiment is \\(\\alpha\\). State of Nature Don't reject \\(H_0\\) reject \\(H_0\\) \\(H_0\\) is true ‚úÖ Type I error \\(H_0\\) is false Type II error ‚úÖ The familywise error rate (FWER) is the risk of making at least one Type I error among the family of comparisons in the experiment. Now let's consider carrying out \\(m\\) independent t-tests and let for any single test, let Pr(commit a Type 1 error) \\(= \\alpha_c\\) be the per comparison error rate (PCER). So for a single test the probability a correct decision is made is \\(1 - \\alpha_c\\). Therefore for \\(m\\) independent t-tests the probability of committing no Type I errors is \\((1 - \\alpha_c)^m\\) and the probability of committing at least one Type I error is \\(1 -(1 - \\alpha_c)^m = \\alpha_F\\) which is the upper limit of the FWER. The False Discovery Rate (FDR) controls the expected (mean) proportion of false discoveries among the \\(R\\) (out of \\(m\\)) hypotheses declared significant. Consider testing \\(m\\) null hypotheses with corresponding p-values \\(P_1, P_2,...,P_m\\); we then order then so that \\(P_{(1)} &lt; P_{(2)} &lt;...&lt;P_{(m)}\\) (where \\(P_{(i)}\\) is the \\(i^{th}\\) largest \\(i=1,...,m\\)). The \\(i^{th}\\) ordered p-value is calculated as \\(\\frac{i}{m}q^*\\) and the \\(i^{th}\\) null hypothesis is rejected if \\(P_i \\leq \\frac{i}{m}q^*\\) Adjustments for multiple testing Calculating 95% confidence intervals for pairwise comparisons is similar to the procedure discussed in module 2: \\[\\text{95% CI} = \\text{estimate} \\pm (\\text{scale factor} \\times \\text{standard error}_\\text{of estimate}).\\] For the 95% CI for pairwise comparisons of means this becomes \\[\\text{95% CI} = \\text{difference} \\pm (\\text{scale factor} \\times \\text{SED}),\\] where SED is the standard error of the difference (which depends on group replication). The choice of scale factor depends on what adjustments we might want to make. Below we cover three common adjustments. Specifically where (scale factor √ó SED) is the 1) Fisher correction LSD, 2) Bonferroni correction LSD, and 3) Tukey's HSD. Fisher‚Äôs Least Significant Difference (LSD) To calculate the LSD the \\(t\\)-distribution is used, specifically the \\[t_{\\alpha = \\frac{\\alpha_c}{2}, \\text{df} = N - m}\\] distribution, where \\(N\\) is the number of observations and \\(m\\) is the number of treatment groups. To obtain the critical value for \\(\\alpha_c = 0.05\\), \\(N = 12\\) and \\(m = 3\\) the following code can be used in R. qt(p = 0.05/2,df = 12 - 3, lower.tail = FALSE) ## [1] 2.262157 Fisher's LSD is then \\[t_{\\alpha = \\frac{\\alpha_c}{2}, \\text{df} = N - m} \\times \\text{SED}.\\] Here we carry out post-hoc tests only if the ANOVA F-test is significant. If so declare significant \\(100\\alpha\\%\\) any pairwise difference &gt; LSD. This does not control the FWER. Bonferroni correction To calculate Bonferroni correction the \\(t\\)-distribution is used, specifically the \\[t_{\\alpha = \\frac{\\alpha_c}{2 \\times k}, \\text{df} = N - m}\\] distribution, where \\(N\\) is the number of observations, \\(m\\) is the number of treatment groups, and \\(k = {m \\choose 2}\\) is the number of pairwise comparisons being made. To obtain the critical value for \\(\\alpha_c = 0.05\\), \\(N = 12\\) and \\(m = 3\\) the following code can be used in R. qt(p = 0.05/(2 * choose(3,2)),df = 12 - 3, lower.tail = FALSE) ## [1] 2.933324 Bonferroni's LSD is then \\[t_{\\alpha = \\frac{\\alpha_c}{2 \\times k}, \\text{df} = N - m} \\times \\text{SED}.\\] Here we reject the \\(H_0\\) for which the p-value, p-val, is p-val \\(&lt; \\alpha_c = \\frac{\\alpha_f}{n_c}\\) where \\(\\alpha_f\\) is the FWER and \\(n_c\\) is the number of pairwise comparisons. However, this makes no assumptions about independence between tests. Tukey‚Äôs Honest Significant Difference (HSD) This compares the mean of every treatment with the mean of every other treatment and uses a studentised range distribution compared with a \\(t\\)-distribution for Fisher's LSD and the Bonferroni correction. Therefore to calculate the HSD the studentised range distribution is used, specifically the \\[q_{ 1- \\alpha_c, m, \\text{df} = N - m}\\] distribution, where \\(N\\) is the number of observations, \\(m\\) is the number of treatment groups, and \\(q_{\\alpha,m,\\text{df}}\\) quantile of the studendised distribution. To obtain the critical value for \\(\\alpha_c = 0.05\\), \\(N = 12\\) and \\(m = 3\\) the following code can be used in R. 1 - qtukey(p = 1 - 0.05, nmeans = 3, df = 12 - 3) ## [1] -2.948492 Tukey's HSD is given by \\[\\frac{q_{ 1 - \\alpha_c, m, \\text{df} = N - m}}{\\sqrt{2}} \\times \\sqrt{\\frac{2\\hat{\\sigma}^2}{n}}.\\] Here \\(\\hat{\\sigma}^2\\) is the residual mean square error and \\(n\\) is the assumed equal number of replicates in each group. Classification of multiple hypothesis tests Suppose we have a number \\(m\\) of null hypotheses, \\(H_1, H_2, ..., H_m\\). Using the traditional parlance we reject the null hypothesis if the test is declared significant and have no evidence to reject the null hypothesis if the test is &quot;not significant&quot;. Now, summing each type of outcome over all \\(H_i (i = 1.,..,m)\\) yields the following random variables: Null hypothesis is true (H0) Alternative hypothesis is true (HA) Total Test is declared significant V S R Test is declared non-significant U T m - R Total \\(m_{0}\\) \\(m - m_0\\) m \\(m\\) is the total number hypotheses tested \\(m_{0}\\) is the number of true null hypotheses, an unknown parameter \\(m - m_0\\) is the number of true alternative hypotheses \\(V\\) is the number of false positives (Type I error) (also called false discoveries) \\(S\\) is the number of true positives (also called true discoveries) \\(T\\) is the number of false negatives (Type II error) \\(U\\) is the number of true negatives \\(R=V+S\\) is the number of rejected null hypotheses (also called discoveries, either true or false) Multiple comparison procedures Recall the CRD rats data Surgery Rat logAUC C 1 8.49 C 2 8.20 C 3 9.08 C 4 8.07 P 1 10.24 P 2 7.72 P 3 9.34 P 4 8.50 S 1 11.31 S 2 12.69 S 3 11.37 S 4 10.82 You can read this data directly into R using rats &lt;- readr::read_csv(&quot;https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/crd_rats_data.csv&quot;) To use predictmeans later on we have to ensure that the relevant variables are coded as factors: rats &lt;- rats %&gt;% mutate(Surgery = as.factor(Surgery)) We can fit a linear model using lm(): rats_lm &lt;- lm(logAUC ~ Surgery, data = rats) coef(rats_lm) ## (Intercept) SurgeryP SurgeryS ## 8.4600 0.4900 3.0875 Our fitted model is therefore \\[ \\begin{aligned} \\operatorname{\\widehat{logAUC}} &amp;= 8.46 + 0.49(\\operatorname{Surgery}_{\\operatorname{P}}) + 3.09(\\operatorname{Surgery}_{\\operatorname{S}}) \\end{aligned} \\] Using predictmeans By default Fisher's comparisons are made. pm &lt;- predictmeans::predictmeans(rats_lm , modelterm = &quot;Surgery&quot;, pairwise = TRUE, plot = FALSE) str(pm) ## List of 10 ## $ Predicted Means : &#39;xtabs&#39; num [1:3(1d)] 8.46 8.95 11.55 ## ..- attr(*, &quot;dimnames&quot;)=List of 1 ## .. ..$ Surgery: chr [1:3] &quot;C&quot; &quot;P&quot; &quot;S&quot; ## ..- attr(*, &quot;call&quot;)= language xtabs(formula = pm ~ ., data = mt[, c(&quot;pm&quot;, vars)], drop.unused.levels = TRUE) ## $ Standard Error of Means : Named num 0.41 ## ..- attr(*, &quot;names&quot;)= chr &quot;All means have the same Stder&quot; ## $ Standard Error of Differences: Named num [1:3] 0.58 0.58 0.58 ## ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Max.SED&quot; &quot;Min.SED&quot; &quot;Aveg.SED&quot; ## $ LSD : Named num [1:3] 1.31 1.31 1.31 ## ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Max.LSD&quot; &quot;Min.LSD&quot; &quot;Aveg.LSD&quot; ## ..- attr(*, &quot;Significant level&quot;)= num 0.05 ## ..- attr(*, &quot;Degree of freedom&quot;)= num 9 ## $ Pairwise LSDs : num [1:3, 1:3] 0 1.31 1.31 -0.49 0 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:3] &quot;C&quot; &quot;P&quot; &quot;S&quot; ## .. ..$ : chr [1:3] &quot;C&quot; &quot;P&quot; &quot;S&quot; ## ..- attr(*, &quot;Significant level&quot;)= num 0.05 ## ..- attr(*, &quot;Degree of freedom&quot;)= int 9 ## ..- attr(*, &quot;Note&quot;)= chr &quot;LSDs matrix has mean differences (row-col) above the diagonal, LSDs (adjusted by &#39;none&#39; method) below the diagonal&quot; ## $ Pairwise p-value : num [1:3, 1:3] 0 0.4202 0.0005 -0.8446 0 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:3] &quot;C&quot; &quot;P&quot; &quot;S&quot; ## .. ..$ : chr [1:3] &quot;C&quot; &quot;P&quot; &quot;S&quot; ## ..- attr(*, &quot;Degree of freedom&quot;)= int 9 ## ..- attr(*, &quot;Note&quot;)= chr &quot;The matrix has t-value above the diagonal, p-value (adjusted by &#39;none&#39; method) below the diagonal&quot; ## ..- attr(*, &quot;Letter-based representation of pairwise comparisons at significant level &#39;0.05&#39;&quot;)=&#39;data.frame&#39;: 3 obs. of 3 variables: ## .. ..$ Treatment: chr [1:3] &quot;S&quot; &quot;P&quot; &quot;C&quot; ## .. ..$ Mean : num [1:3] 11.55 8.95 8.46 ## .. ..$ Group : chr [1:3] &quot;A &quot; &quot; B&quot; &quot; B&quot; ## $ predictmeansPlot : NULL ## $ predictmeansBarPlot : NULL ## $ mean_table :&#39;data.frame&#39;: 3 obs. of 7 variables: ## ..$ Surgery : Factor w/ 3 levels &quot;C&quot;,&quot;P&quot;,&quot;S&quot;: 1 2 3 ## ..$ Predicted means: num [1:3] 8.46 8.95 11.55 ## ..$ Standard error : num [1:3] 0.41 0.41 0.41 ## ..$ Df : int [1:3] 9 9 9 ## ..$ LL of 95% CI : num [1:3] 7.53 8.02 10.62 ## ..$ UL of 95% CI : num [1:3] 9.39 9.88 12.48 ## ..$ LetterGrp : chr [1:3] &quot;A &quot; &quot;A &quot; &quot; B&quot; ## $ p_valueMatrix : NULL ## - attr(*, &quot;class&quot;)= chr &quot;pdmlist&quot; Let's look at some of the pm elements more closely: pm$mean_table ## Surgery Predicted means Standard error Df LL of 95% CI UL of 95% CI LetterGrp ## 1 C 8.4600 0.4102531 9 7.531943 9.388057 A ## 2 P 8.9500 0.4102531 9 8.021943 9.878057 A ## 3 S 11.5475 0.4102531 9 10.619443 12.475557 B This gives us the estimated group means and associated 95% CIs. Why are the standard errors all equal? But, what we'd like is the pairwise comparisons between groups. Information pertaining to this is also returned: print(pm$`Pairwise p-value`) ## C P S ## C 0.0000 -0.8446 -5.3216 ## P 0.4202 0.0000 -4.4770 ## S 0.0005 0.0015 0.0000 ## attr(,&quot;Degree of freedom&quot;) ## [1] 9 ## attr(,&quot;Note&quot;) ## [1] &quot;The matrix has t-value above the diagonal, p-value (adjusted by &#39;none&#39; method) below the diagonal&quot; ## attr(,&quot;Letter-based representation of pairwise comparisons at significant level &#39;0.05&#39;&quot;) ## Treatment Mean Group ## 1 S 11.5475 A ## 2 P 8.9500 B ## 3 C 8.4600 B This gives us the pairwise comparison statistic (the \\(t\\)-statistic in this case) on the upper diagonal and the associated p-value's on the lower diagonal. What about the 95%CI for the comparisons? Luckily predictmeans also returns the pairwise LSD values (Fisher's by default with \\(\\alpha_c = 0.05\\)): pm$`Pairwise LSDs` ## C P S ## C 0.00000 -0.49000 -3.0875 ## P 1.31247 0.00000 -2.5975 ## S 1.31247 1.31247 0.0000 ## attr(,&quot;Significant level&quot;) ## [1] 0.05 ## attr(,&quot;Degree of freedom&quot;) ## [1] 9 ## attr(,&quot;Note&quot;) ## [1] &quot;LSDs matrix has mean differences (row-col) above the diagonal, LSDs (adjusted by &#39;none&#39; method) below the diagonal&quot; Here, the upper diagonal matrix has the pairwise differences and the lower has the LSD values. Fisher‚Äôs LSD So, we have all the information to construct the Fisher LSD 95% CIs! You could extract all the information and construct a pairwise comparison table manually, or use a pre-written function (aren't I nice to provide one!): url &lt;- &quot;https://gist.github.com/cmjt/72f3941533a6bdad0701928cc2924b90&quot; devtools::source_gist(url, quiet = TRUE) comparisons(pm) ## Comparison Difference SED LSD lwr upr t p ## 1 C-P -0.490 0.58 1.312 -1.802 0.822 -0.845 0.4202 ## 2 C-S -3.087 0.58 1.312 -4.400 -1.775 -5.322 0.0005 ## 3 P-S -2.598 0.58 1.312 -3.910 -1.285 -4.477 0.0015 Bonferroni correction To use the Bonferroni correction we must now calculate and specify the adjusted \\(\\alpha_b = \\frac{\\alpha_b}{n_c}\\), in our case this is \\(\\frac{0.05}{{3 \\choose 2}}\\). We also specify adj = &quot;bonferroni&quot;: alpha.adj &lt;- 0.05/choose(3,2) bonferroni &lt;- predictmeans::predictmeans(rats_lm , modelterm = &quot;Surgery&quot;, adj = &quot;bonferroni&quot;, level = alpha.adj, pairwise = TRUE, plot = FALSE) comparisons(bonferroni) ## Comparison Difference SED LSD lwr upr t p ## 1 C-P -0.490 0.58 2.101 -2.591 1.611 -0.845 1.0000 ## 2 C-S -3.087 0.58 2.101 -5.189 -0.986 -5.322 0.0014 ## 3 P-S -2.598 0.58 2.101 -4.699 -0.496 -4.477 0.0046 Tukey‚Äôs Honest Significant Difference (HSD) Things are a bit more cumbersome when it comes to Tukey's HSD. We can specify adj = &quot;tukey&quot; in predictmeans: tukey &lt;- predictmeans::predictmeans(rats_lm , modelterm = &quot;Surgery&quot;, adj = &quot;tukey&quot;, level = alpha.adj, pairwise = TRUE, plot = FALSE) names(tukey) ## [1] &quot;Predicted Means&quot; &quot;Standard Error of Means&quot; ## [3] &quot;Standard Error of Differences&quot; &quot;LSD&quot; ## [5] &quot;Pairwise p-value&quot; &quot;predictmeansPlot&quot; ## [7] &quot;predictmeansBarPlot&quot; &quot;mean_table&quot; ## [9] &quot;p_valueMatrix&quot; However, the HSD values are not returned, so we cannot calculate out pairwise CIs. R has an inbuilt function TukeyHSD that does this for us, but as we can see below the HSD values are not returned. TukeyHSD(aov(logAUC~Surgery, data = rats)) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = logAUC ~ Surgery, data = rats) ## ## $Surgery ## diff lwr upr p adj ## P-C 0.4900 -1.1298813 2.109881 0.6863267 ## S-C 3.0875 1.4676187 4.707381 0.0012479 ## S-P 2.5975 0.9776187 4.217381 0.0039400 From above we saw that Tukey's HSD was given by \\[\\frac{q_{1-\\alpha_c, m, \\text{df} = N - m}}{\\sqrt{2}} \\times \\sqrt{\\frac{2\\hat{\\sigma}^2}{n}}.\\] We can calculate \\(q_{1 - \\alpha_c, m, \\text{df} = N - m}\\) for our data using qtukey(p = 1- 0.05, nmeans = 3, df = 12 - 3) ## [1] 3.948492 Assuming equal replicates (\\(n = 4\\), with \\(\\hat{\\sigma}^2\\) as above) we calculate the SED as sqrt(2 * anova(rats_lm)[2,3] / 4) ## [1] 0.5801856 Therefore, Tukey's HSD is \\(\\frac{3.9484922}{\\sqrt{2}}\\times 0.5801856 = 1.6198813.\\) To calculate the CIs we use \\(\\text{difference} \\pm \\text{HSD}\\): HSD &lt;- (qtukey(p = 1 - 0.05, nmeans = 3, df = 12 - 3)/sqrt(2))*sqrt(2 * anova(rats_lm)[2,3] / 4) all_diffs &lt;- outer(tukey$`Predicted Means`, tukey$`Predicted Means`, &quot;-&quot;) comparison_table &lt;- data.frame(differences = all_diffs[lower.tri(all_diffs)]) %&gt;% mutate(upper = differences + HSD, lower = differences - HSD, HSD = HSD) comparison_table ## differences upper lower HSD ## 1 0.4900 2.109881 -1.1298813 1.619881 ## 2 3.0875 4.707381 1.4676187 1.619881 ## 3 2.5975 4.217381 0.9776187 1.619881 This matches the output from TukeyHSD() above! What about the values returned by predictmeans()? print(tukey$`Pairwise p-value`) ## C P S ## C 0.0000 -0.8446 -5.3216 ## P 0.6863 0.0000 -4.4770 ## S 0.0012 0.0039 0.0000 ## attr(,&quot;Degree of freedom&quot;) ## [1] 9 ## attr(,&quot;Note&quot;) ## [1] &quot;The matrix has t-value above the diagonal, p-value (adjusted by &#39;tukey&#39; method) below the diagonal&quot; ## attr(,&quot;Letter-based representation of pairwise comparisons at significant level &#39;0.0166666666666667&#39;&quot;) ## Treatment Mean Group ## 1 S 11.5475 A ## 2 P 8.9500 B ## 3 C 8.4600 B There is an easier way to see the same pairwise contrasts using the emmeans function from the package of the same name and the pairs function: em &lt;- emmeans::emmeans(rats_lm, specs = &quot;Surgery&quot;) pairs(em, adjust = &quot;tukey&quot;) ## contrast estimate SE df t.ratio p.value ## C - P -0.49 0.58 9 -0.845 0.6863 ## C - S -3.09 0.58 9 -5.322 0.0012 ## P - S -2.60 0.58 9 -4.477 0.0039 ## ## P value adjustment: tukey method for comparing a family of 3 estimates This, gives us the same information as predictmeans but in an easier to read format (still no CIs though, we still have to calculate those ourselves!) However, the emmeanspackage facilitates some nice plotting of the pairwise comparisons: plot(em, comparisons = TRUE) + theme_bw() "],["linear-mixed-effect-models-lmms.html", "Linear mixed-effect models (LMMs)", " Linear mixed-effect models (LMMs) Recall, blocking helps control variability by making treatment groups more alike. Experimental units are divided into subsets (called blocks) so that units within the same block are more similar than units from different subsets or blocks. Blocking is a technique for dealing with nuisance factors. A nuisance factor is a factor that has some effect on the response, but is of no interest (e.g., age class). Fixed effects are terms (parameters) in a statistical model which are fixed, or non-random, quantities (e.g., treatment group's mean response). For the same treatment, we expect this quantity to be the same from experiment to experiment. Random effects are terms (parameters) in a statistical model which are considered as random quantities or variables (e.g., block id). Specifically, terms whose levels are a representative sample from a population, and where the variance of the population is of interest should be allocated as random. Setting a block as a random effect allows us to infer variation between blocks as well as the variation between experimental units within blocks. Key idea: Partition known sources of variation which are unimportant to key scientific question(s) to improve precision of comparisons between treatment means. A Randomised Controlled Block Design (RCBD) Run Surgery Rat logAUC8 1 C 1 9.24 1 P 2 8.81 1 S 3 10.75 2 C 4 3.89 2 P 5 8.62 2 S 6 10.24 3 C 7 8.42 3 P 8 9.93 3 S 9 11.68 4 C 10 8.77 4 P 11 10.86 4 S 12 13.05 You can read these data directly into R: rcbd &lt;- readr::read_csv(&quot;https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/rcbd.csv&quot;) To use predictmeans later on we have to ensure that the relevant variables are coded as factors: rcbd &lt;- rcbd %&gt;% mutate(Run = as.factor(Run)) %&gt;% mutate(Surgery = as.factor(Surgery)) Run as a fixed effect lm &lt;- lm(logAUC8 ~ Run + Surgery, data = rcbd) summary(lm) ## ## Call: ## lm(formula = logAUC8 ~ Run + Surgery, data = rcbd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.7517 -0.3683 -0.0900 0.4508 1.5817 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.6583 0.8506 9.004 0.000105 *** ## Run2 -2.0167 0.9822 -2.053 0.085854 . ## Run3 0.4100 0.9822 0.417 0.690882 ## Run4 1.2933 0.9822 1.317 0.235963 ## SurgeryP 1.9750 0.8506 2.322 0.059293 . ## SurgeryS 3.8500 0.8506 4.526 0.003991 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.203 on 6 degrees of freedom ## Multiple R-squared: 0.8449, Adjusted R-squared: 0.7157 ## F-statistic: 6.538 on 5 and 6 DF, p-value: 0.02034 Run as a random effect There are, confusingly, two ways of fitting the same model. For inference we require both! Option 1 uses the lmer function from the lme4 package: lmer4_mod &lt;- lme4::lmer(logAUC8 ~ Surgery + (1|Run), data = rcbd) summary(lmer4_mod) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: logAUC8 ~ Surgery + (1 | Run) ## Data: rcbd ## ## REML criterion at convergence: 37.2 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.8525 -0.2273 0.1772 0.4036 1.3309 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Run (Intercept) 1.479 1.216 ## Residual 1.447 1.203 ## Number of obs: 12, groups: Run, 4 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 7.5800 0.8552 8.863 ## SurgeryP 1.9750 0.8506 2.322 ## SurgeryS 3.8500 0.8506 4.526 ## ## Correlation of Fixed Effects: ## (Intr) SrgryP ## SurgeryP -0.497 ## SurgeryS -0.497 0.500 Option 2 uses the lmer function from the lmerTest package: lmerTest_mod &lt;- lmerTest::lmer(logAUC8 ~ Surgery + (1|Run), data = rcbd) summary(lmerTest_mod) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: logAUC8 ~ Surgery + (1 | Run) ## Data: rcbd ## ## REML criterion at convergence: 37.2 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.8525 -0.2273 0.1772 0.4036 1.3309 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Run (Intercept) 1.479 1.216 ## Residual 1.447 1.203 ## Number of obs: 12, groups: Run, 4 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 7.5800 0.8552 5.9567 8.863 0.000119 *** ## SurgeryP 1.9750 0.8506 6.0000 2.322 0.059293 . ## SurgeryS 3.8500 0.8506 6.0000 4.526 0.003991 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) SrgryP ## SurgeryP -0.497 ## SurgeryS -0.497 0.500 As you can see they give the same output! Why bother, you might ask?! This will become apparent later on. Inference about the random effects We have two variance components Between Groups (Runs) \\(\\hat{\\sigma^2}_{\\text{Run}}\\) = 1.479 Within Runs (between observations) \\(\\hat{\\sigma_2}\\) = 1.447 Note that aov() presents the same information, but in a different way: summary(aov(logAUC8 ~ Surgery + Error(Run), data = rcbd)) ## ## Error: Run ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals 3 17.65 5.883 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Surgery 2 29.652 14.826 10.25 0.0116 * ## Residuals 6 8.682 1.447 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Within Runs (Residuals) \\(\\hat{\\sigma}_2\\) = 1.447 (same as lmer) Between Run variance = \\(\\hat{\\sigma}^2\\) + \\(3\\:\\hat{\\sigma}^2_{\\text{Run}}\\) so \\(\\hat{\\sigma}^2_{\\text{Run}} = \\frac{5.883 - \\hat{\\sigma}^2 }{3} = \\frac{5.883 - 1.447}{3} = 1.479\\) Inference about the fixed effects Specifying Run as random effect changes our estimated baseline (i.e., Intercept coefficient) as now and effect due to Run is attributed to the structural component of the model. We can interpret the fixed effects of a LMM as we might for a linear model (now the Intercept estimate changes depending on Run). coefficients(lmer4_mod) ## $Run ## (Intercept) SurgeryP SurgeryS ## 1 7.639067 1.975 3.85 ## 2 6.118411 1.975 3.85 ## 3 7.948225 1.975 3.85 ## 4 8.614297 1.975 3.85 ## ## attr(,&quot;class&quot;) ## [1] &quot;coef.mer&quot; coefficients(lmerTest_mod) ## $Run ## (Intercept) SurgeryP SurgeryS ## 1 7.639067 1.975 3.85 ## 2 6.118411 1.975 3.85 ## 3 7.948225 1.975 3.85 ## 4 8.614297 1.975 3.85 ## ## attr(,&quot;class&quot;) ## [1] &quot;coef.mer&quot; What about an ANOVA table? NOTE that specifying the type of \\(SS\\) (e.g., Type I, II, or III) in an anova call only works with a model fitted using lmerTest::lmer: anova(lmerTest_mod, type = 1) ## Type I Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## Surgery 29.652 14.826 2 6 10.246 0.01162 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(lmerTest_mod, type = 2) ## Type II Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## Surgery 29.652 14.826 2 6 10.246 0.01162 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Now, as we only have a single fixed effect in our model (Surgery) the ANOVA Type I and Type II tables above are equivalent! Inference using predictmeans() (Note: output will be the same for the lmerTest model.) library(predictmeans) pm &lt;- predictmeans(lmer4_mod, modelterm = &quot;Surgery&quot;, pairwise = TRUE, plot = FALSE) Using the elements of the predictmeans object (as in the last section) we can extract the pairwise comparison information: pm$`Pairwise LSDs` ## C P S ## C 0.00000 -1.97500 -3.850 ## P 2.08132 0.00000 -1.875 ## S 2.08132 2.08132 0.000 ## attr(,&quot;Significant level&quot;) ## [1] 0.05 ## attr(,&quot;Degree of freedom&quot;) ## [1] 6 ## attr(,&quot;Note&quot;) ## [1] &quot;LSDs matrix has mean differences (row-col) above the diagonal, LSDs (adjusted by &#39;none&#39; method) below the diagonal&quot; print(pm$`Pairwise p-value`) ## C P S ## C 0.0000 -2.3219 -4.5263 ## P 0.0593 0.0000 -2.2043 ## S 0.0040 0.0697 0.0000 ## attr(,&quot;Degree of freedom&quot;) ## [1] 6 ## attr(,&quot;Note&quot;) ## [1] &quot;The matrix has t-value above the diagonal, p-value (adjusted by &#39;none&#39; method) below the diagonal&quot; ## attr(,&quot;Letter-based representation of pairwise comparisons at significant level &#39;0.05&#39;&quot;) ## Treatment Mean Group ## 1 S 11.430 A ## 2 P 9.555 AB ## 3 C 7.580 B Gives us 1) pairwise differences (upper diagonal) and the LSD values (lower diagonal), and 2) the pairwise comparison statistic (the \\(t\\)-statistic in this case) on the upper diagonal and the associated p-value's on the lower diagonal. So is there a pairwise difference in means? Let's organise the information in a table. ## Comparison Difference SED LSD lwr upr t p ## 1 C-P -1.975 0.851 2.081 -4.056 0.106 -2.322 0.0593 ## 2 C-S -3.850 0.851 2.081 -5.931 -1.769 -4.526 0.0040 ## 3 P-S -1.875 0.851 2.081 -3.956 0.206 -2.204 0.0697 Have a look at the CIs and p-values. What do you conclude? We could also plot the pairwise comparisons: A Split-plot design Disease Organ Animal Sample logAUC Healthy innerLV 1 1 9.40 Healthy outerLV 1 2 8.83 Healthy innerLV 2 1 10.33 Healthy outerLV 2 2 10.49 Healthy innerLV 3 1 9.74 Healthy outerLV 3 2 10.98 Diabetic innerLV 4 1 7.92 Diabetic outerLV 4 2 9.37 Diabetic innerLV 5 1 8.69 Diabetic outerLV 5 2 11.31 Diabetic innerLV 6 1 7.01 Diabetic outerLV 6 2 9.29 You can read these data directly into R: split_plot &lt;- readr::read_csv(&quot;https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/split_plot.csv&quot;) To use predictmeans later on we have to ensure that the relevant variables are coded as factors: split_plot &lt;- split_plot %&gt;% mutate(Animal = as.factor(Animal)) %&gt;% mutate(Disease = as.factor(Disease))%&gt;% mutate(Organ = as.factor(Organ)) Animal as a random effect Using aov() sp_aov &lt;- aov(logAUC ~ Disease*Organ + Error(Animal), data = split_plot) summary(sp_aov) ## ## Error: Animal ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Disease 1 3.183 3.183 2.187 0.213 ## Residuals 4 5.822 1.456 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Organ 1 4.296 4.296 14.423 0.0191 * ## Disease:Organ 1 2.539 2.539 8.525 0.0433 * ## Residuals 4 1.191 0.298 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Using lmer() (from lmeTest and lmer4) Recall that specifying the type of \\(SS\\) (e.g., Type I, II, or III) in an anova call only works with a model fitted using lmerTest::lmer. As we now have specified an interaction model then the type of \\(SS\\) calculated will have an effect on inference for unbalanced designs. sp_lmer &lt;- lmerTest::lmer(logAUC ~ Disease*Organ + (1|Animal), data = split_plot) anova(sp_lmer,type = 1) ## Type I Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## Disease 0.6513 0.6513 1 4 2.1866 0.21329 ## Organ 4.2960 4.2960 1 4 14.4227 0.01914 * ## Disease:Organ 2.5392 2.5392 1 4 8.5246 0.04326 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(sp_lmer,type = 2) ## Type II Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## Disease 0.6513 0.6513 1 4 2.1866 0.21329 ## Organ 4.2960 4.2960 1 4 14.4227 0.01914 * ## Disease:Organ 2.5392 2.5392 1 4 8.5246 0.04326 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note here, though, our design is balanced hence the ordering of terms in our model does not make a difference (no need to specify Type II \\(SS\\)) Pairwise comparison of time means When a design has blocking, to get summary stats using predictmeans you should fit the model using lme4::lmer(): Calling predictmeans alone produces plots: lmer &lt;- lme4::lmer(logAUC ~ Disease*Organ + (1|Animal), data = split_plot) predmeans &lt;- predictmeans::predictmeans(model = lmer ,modelterm = &quot;Disease:Organ&quot;, pairwise = TRUE) predmeans$predictmeansPlot Recall from previous sections that the LSD vale is essentially the buffer around the point estimate (the radius of the CI if you like), beyond the limit of which we might believe there to be a significant difference (a bit lax with terminology there!). How would be interpret the above plot? I would conclude that there is a big difference between Diabetic and Healthy for InnerLV wall, but no difference for the outerLV wall. We can construct the pairwise comparison table: comparisons(predmeans) ## Comparison Difference SED LSD lwr upr t ## 1 Diabetic:innerLV-Diabetic:outerLV -2.117 0.446 1.237 -3.354 -0.879 -4.750 ## 2 Diabetic:innerLV-Healthy:innerLV -1.950 0.764 2.123 -4.073 0.173 -2.551 ## 3 Diabetic:innerLV-Healthy:outerLV -2.227 0.764 2.123 -4.349 -0.104 -2.913 ## 4 Diabetic:outerLV-Healthy:innerLV 0.167 0.765 2.123 -1.956 2.289 0.218 ## 5 Diabetic:outerLV-Healthy:outerLV -0.110 0.764 2.123 -2.233 2.013 -0.144 ## 6 Healthy:innerLV-Healthy:outerLV -0.277 0.446 1.237 -1.514 0.961 -0.621 ## p ## 1 0.0090 ## 2 0.0633 ## 3 0.0436 ## 4 0.8381 ## 5 0.8925 ## 6 0.5683 What do the CI coverages indicate? We can also use emmeans to plot the pairwise comparisons em &lt;- emmeans::emmeans(lmer, ~Disease*Organ) plot(em, pairwise = TRUE) + theme_bw() From all the output above, what do you conclude about the interaction effect, if any! A repeated measures design Here we will use time points as factors. To read these data directly into R use liver &lt;- readr::read_csv(&quot;https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/repeated_measures_liver.csv&quot;) liver &lt;- liver %&gt;% mutate(Time = as.factor(Time)) %&gt;% mutate(Treatment = as.factor(Treatment)) liver ## # A tibble: 210 √ó 4 ## Animal Treatment Time Glucose ## &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 Control1 Control 0 1.60 ## 2 Control1 Control 5 1.28 ## 3 Control1 Control 10 1.60 ## 4 Control1 Control 15 1.58 ## 5 Control1 Control 20 1.28 ## 6 Control1 Control 25 1.44 ## 7 Control1 Control 30 1.18 ## 8 Control2 Control 0 0.84 ## 9 Control2 Control 5 0.64 ## 10 Control2 Control 10 0.7 ## # ‚Ä¶ with 200 more rows Below these data are plotted. Animal as a random effect Using aov() re_aov &lt;- aov(Glucose ~ Treatment*Time + Error(Animal),data = liver) summary(re_aov) ## ## Error: Animal ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Treatment 3 1.90 0.6335 1.407 0.263 ## Residuals 26 11.71 0.4503 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Time 6 0.7973 0.13289 8.732 0.0000000334 *** ## Treatment:Time 18 0.2539 0.01411 0.927 0.547 ## Residuals 156 2.3741 0.01522 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Using lmer() (from lmerTest and lme4) Recall that specifying the type of \\(SS\\) (e.g., Type I, II, or III) in an anova call only works with a model fitted using lmerTest::lmer. As we now have specified an interaction model then the type of \\(SS\\) calculated will have an effect on inference for unbalanced designs. re_lmer &lt;- lmerTest::lmer(Glucose ~ Treatment*Time + (1|Animal), data = liver) anova(re_lmer,type = 2) ## Type II Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## Treatment 0.06423 0.021409 3 26 1.4068 0.2632 ## Time 0.79731 0.132885 6 156 8.7318 0.00000003345 *** ## Treatment:Time 0.25390 0.014105 18 156 0.9269 0.5474 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Pairwise comparison of time means As above, when a design has blocking, to get summary stats using predictmeans you should fit the model using lme4::lmer(): re_lmer4 &lt;- lme4::lmer(Glucose ~ Treatment*Time + (1|Animal),data = liver) predmeans &lt;- predictmeans::predictmeans(model = re_lmer4 ,modelterm = &quot;Time&quot;, pairwise = TRUE, plot = TRUE) How would be interpret the above plot? What about the pairwise comparison table? comparisons(predmeans) ## Comparison Difference SED LSD lwr upr t p ## 1 0-5 0.058 0.032 0.063 -0.005 0.121 1.830 0.0691 ## 2 0-10 0.080 0.032 0.063 0.017 0.143 2.512 0.0130 ## 3 0-15 0.111 0.032 0.063 0.048 0.174 3.473 0.0007 ## 4 0-20 0.149 0.032 0.063 0.086 0.212 4.675 0.0000 ## 5 0-25 0.145 0.032 0.063 0.082 0.208 4.531 0.0000 ## 6 0-30 0.205 0.032 0.063 0.142 0.268 6.421 0.0000 ## 7 5-10 0.022 0.032 0.063 -0.041 0.085 0.682 0.4961 ## 8 5-15 0.052 0.032 0.063 -0.011 0.116 1.643 0.1025 ## 9 5-20 0.091 0.032 0.063 0.028 0.154 2.845 0.0050 ## 10 5-25 0.086 0.032 0.063 0.023 0.149 2.701 0.0077 ## 11 5-30 0.147 0.032 0.063 0.083 0.210 4.591 0.0000 ## 12 10-15 0.031 0.032 0.063 -0.032 0.094 0.960 0.3384 ## 13 10-20 0.069 0.032 0.063 0.006 0.132 2.163 0.0321 ## 14 10-25 0.064 0.032 0.063 0.001 0.127 2.018 0.0453 ## 15 10-30 0.125 0.032 0.063 0.062 0.188 3.909 0.0001 ## 16 15-20 0.038 0.032 0.063 -0.025 0.101 1.202 0.2310 ## 17 15-25 0.034 0.032 0.063 -0.029 0.097 1.058 0.2917 ## 18 15-30 0.094 0.032 0.063 0.031 0.157 2.948 0.0037 ## 19 20-25 -0.005 0.032 0.063 -0.068 0.058 -0.144 0.8853 ## 20 20-30 0.056 0.032 0.063 -0.007 0.119 1.746 0.0828 ## 21 25-30 0.060 0.032 0.063 -0.003 0.123 1.890 0.0606 What do the CI coverages indicate? We can also use emmeans to plot the pairwise comparisons em &lt;- emmeans::emmeans(re_lmer4, ~Treatment*Time) plot(em, pairwise = TRUE) + theme_bw() + facet_wrap(~Treatment) "],["module-4.html", "Module 4", " Module 4 Learning objectives Define, state, and critique the motivation(s) behind the three following estimation techniques least squares estimation maximum likelihood estimation Bayesian inference Carry out by-hand parameter estimation for the three following estimation techniques least squares estimation maximum likelihood estimation Bayesian inference Carry out parameter estimation in R for the following estimation techniques least squares estimation maximum likelihood estimation Other resources Basic differentiation rules Basic matrix operations An introduction to Bayesian thinking Beginner's Guide To Maximum Likelihood Estimation Bayesian statistics with R "],["least-squares-estimation.html", "Least Squares Estimation", " Least Squares Estimation Some basic matrix algebra This section is a recap only, if you need a more in-depth overiew of matrix algebra then use the extra materials provided at the start of this module. Matrices are commonly written in box brackets or parentheses and are typically denoted by upper case bold letters (e.g., A) with elements represented by the corresponding lower case indexed letters: \\[\\mathbf {A} = \\begin{bmatrix}a_{11}&amp;a_{12}&amp;\\cdots &amp;a_{1n}\\\\a_{21}&amp;a_{22}&amp;\\cdots &amp;a_{2n}\\\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots \\\\a_{m1}&amp;a_{m2}&amp;\\cdots &amp;a_{mn}\\end{bmatrix}\\] The entry in the \\(i^{th}\\) row and \\(j^{th}\\) column of a matrix A is often referred to as the \\((i,j)^{th}\\) entry of the matrix, and most commonly denoted as \\(a_{i,j}\\), or \\(a_{ij}\\). Matrix addition The sum A + B of two m-by-n matrices A and B: \\[\\begin{array}{rl} \\mathbf {A} + \\mathbf {B} &amp;= \\begin{bmatrix}a_{11}&amp;a_{12}&amp;\\cdots &amp;a_{1n}\\\\a_{21}&amp;a_{22}&amp;\\cdots &amp;a_{2n}\\\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots \\\\a_{m1}&amp;a_{m2}&amp;\\cdots &amp;a_{mn}\\end{bmatrix} + \\begin{bmatrix}b_{11}&amp;b_{12}&amp;\\cdots &amp;b_{1n}\\\\b_{21}&amp;b_{22}&amp;\\cdots &amp;b_{2n}\\\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots \\\\b_{m1}&amp;b_{m2}&amp;\\cdots &amp;b_{mn}\\end{bmatrix} \\\\ &amp;= \\begin{bmatrix}a_{11} +b_{11}&amp;a_{12}+b_{12}&amp;\\cdots &amp;a_{1n}+b_{1n}\\\\a_{21}+b_{21}&amp;a_{22}+b_{22}&amp;\\cdots &amp;a_{2n}+b_{2n}\\\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots \\\\a_{m1}+b_{m1}&amp;a_{m2}+b_{m2}&amp;\\cdots &amp;a_{mn}+b_{mn}\\end{bmatrix}\\end{array}\\] Scalar multiplication The product cA of a number c and a matrix A: \\[c\\times \\mathbf {A} = \\begin{bmatrix}c\\times a_{11}&amp;c\\times a_{12}&amp;\\cdots &amp;c\\times a_{1n}\\\\c\\times a_{21}&amp;c\\times a_{22}&amp;\\cdots &amp;c\\times a_{2n}\\\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots \\\\c\\times a_{m1}&amp;c\\times a_{m2}&amp;\\cdots &amp;c\\times a_{mn}\\end{bmatrix}\\] Matrix transposition The transpose of an m-by-n matrix A is the n-by-m matrix A\\(^T\\): \\[\\mathbf {A}^\\text{T} = \\begin{bmatrix}a_{11}&amp;a_{12}&amp;\\cdots &amp;a_{1n}\\\\a_{21}&amp;a_{22}&amp;\\cdots &amp;a_{2n}\\\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots \\\\a_{m1}&amp;a_{m2}&amp;\\cdots &amp;a_{mn}\\end{bmatrix}^\\text{T} = \\begin{bmatrix}a_{11}&amp;a_{21}&amp;\\cdots &amp;a_{m1}\\\\a_{12}&amp;a_{22}&amp;\\cdots &amp;a_{m2}\\\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots \\\\a_{1n}&amp;a_{2n}&amp;\\cdots &amp;a_{mn}\\end{bmatrix}\\] Matrix multiplication Multiplication of two matrices is defined if and only if (\\(iif\\)) the number of columns of the left matrix is the same as the number of rows of the right matrix. If A is an m-by-n matrix and B is an n-by-p matrix, then their matrix product AB is the m-by-p matrix whose entries are given by dot product of the corresponding row of A and the corresponding column of B: \\[[\\mathbf {AB} ]_{i,j}=a_{i,1}b_{1,j}+a_{i,2}b_{2,j}+\\cdots +a_{i,n}b_{n,j}=\\sum _{r=1}^{n}a_{i,r}b_{r,j}\\] Note Matrix multiplication satisfies the rules (AB)C = A(BC) (associativity), and (A + B)C = AC + BC as well as C(A + B) = CA + CB (left and right distributivity), whenever the size of the matrices is such that the various products are defined. The product AB may be defined without BA being defined, namely if A and B are m-by-n and n-by-k matrices, respectively, and m \\(\\neq\\) k. Even if both products are defined, they generally need not be equal, that is: AB \\(\\neq\\) BA. Linear least squares Recall the linear regression (with a simple explanatory variable) equation from Module 2: \\[Y_i = \\alpha + \\beta_1x_i + \\epsilon_i\\] where \\[\\epsilon_i \\sim \\text{Normal}(0,\\sigma^2).\\] Here for observation \\(i\\) \\(Y_i\\) is the value of the response \\(x_i\\) is the value of the explanatory variable \\(\\epsilon_i\\) is the error term: the difference between \\(Y_i\\) and its expected value \\(\\alpha\\) is the intercept term (a parameter to be estimated), and \\(\\beta_1\\) is the slope: coefficient of the explanatory variable (a parameter to be estimated) Recall, that the Euclidean distance between two points is calculated as \\[\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\\] The method of least squares estimation works by minimising the distances between each observation and the line of fit (i.e., the residuals). The line-of best fit is the line with the smallest residual sum (i.e., all the possible red lines in the figures)! Matrix representation of a CRD Let's consider the CRD outlined in the previous module, we can write the effects model using matrix representation: \\[\\boldsymbol{y} = \\boldsymbol{X\\beta} + \\boldsymbol{\\epsilon}\\] where \\(\\boldsymbol{y} = \\begin{bmatrix} y_{11} \\\\ y_{12} \\\\ y_{13} \\\\ y_{14} \\\\ y_{21} \\\\ y_{22} \\\\ y_{23} \\\\ y_{24} \\\\ y_{31} \\\\ y_{32} \\\\ y_{33} \\\\ y_{34} \\end{bmatrix}\\), \\(\\boldsymbol{X} = \\begin{bmatrix} 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}\\), \\(\\boldsymbol{\\beta} = \\begin{bmatrix} \\alpha \\\\ \\tau_1 \\\\ \\tau_2 \\\\ \\tau_3 \\end{bmatrix}\\), and \\(\\boldsymbol{\\epsilon} = \\begin{bmatrix} \\epsilon_{11} \\\\ \\epsilon_{12} \\\\ \\epsilon_{13} \\\\ \\epsilon_{14} \\\\ \\epsilon_{21} \\\\ \\epsilon_{22} \\\\ \\epsilon_{23} \\\\ \\epsilon_{24} \\\\ \\epsilon_{31} \\\\ \\epsilon_{32} \\\\ \\epsilon_{33} \\\\ \\epsilon_{34} \\end{bmatrix}.\\) where \\(\\boldsymbol{\\epsilon} \\sim \\text{MVN}(\\boldsymbol{0}, \\boldsymbol{\\sigma^2 I})\\). The least squares estimators of \\(\\boldsymbol{\\beta}\\) are the solutions to the \\[\\boldsymbol{X^{&#39;}X\\beta}=\\boldsymbol{X^{&#39;}y}\\]. Recall that for a factor variable we take the one level (the first factor) into the baseline (i.e., the standard) and hence then the coefficients we estimate are compared to it (i.e., the differences in the mean). This is to ensure that the matrix \\(\\boldsymbol{X}\\) is full rank. So \\[\\boldsymbol{X} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\end{bmatrix}\\] Now, with three factor levels the least squares estimators of \\(\\boldsymbol{\\beta}\\) are (note the hat to denote the estimates) \\[(\\boldsymbol{X^{&#39;}X})^{-1}\\boldsymbol{X^{&#39;}y} = \\boldsymbol{\\hat{\\beta}}= \\begin{bmatrix} \\hat{\\alpha} - \\hat{\\tau_1} \\\\ \\hat{\\tau_2} - \\hat{\\tau_1} \\\\ \\hat{\\tau_3} - \\hat{\\tau_1} \\end{bmatrix} \\] A numeric example Using the coffee data from the previous chapter we have Beans Strength Type 1 5.3 Type 1 6.0 Type 1 6.6 Type 1 4.9 Type 2 7.5 Type 2 7.1 Type 2 6.3 Type 2 7.6 Type 3 8.8 Type 3 6.9 Type 3 9.2 Type 3 10.3 Using our matrix algebra so \\(\\boldsymbol{X^{&#39;}X} = \\begin{bmatrix}12&amp;4&amp;4 \\\\4&amp;4&amp;0 \\\\4&amp;0&amp;4 \\\\\\end{bmatrix}\\), \\(\\boldsymbol{X^{&#39;}y} = \\begin{bmatrix}86.5 \\\\28.5 \\\\35.2 \\\\\\end{bmatrix}\\), and \\((\\boldsymbol{X^{&#39;}X})^{-1} = \\begin{bmatrix}0.25&amp;-0.25&amp;-0.25 \\\\-0.25&amp;0.5&amp;0.25 \\\\-0.25&amp;0.25&amp;0.5 \\\\\\end{bmatrix}.\\) Therefore, \\[\\boldsymbol{\\hat{\\beta}} = (\\boldsymbol{X^{&#39;}X})^{-1}\\boldsymbol{X^{&#39;}y} = \\begin{bmatrix} \\hat{\\alpha} - \\hat{\\tau_1} \\\\ \\hat{\\tau_2} - \\hat{\\tau_1} \\\\ \\hat{\\tau_3} - \\hat{\\tau_1} \\end{bmatrix} = \\begin{bmatrix}0.25&amp;-0.25&amp;-0.25 \\\\-0.25&amp;0.5&amp;0.25 \\\\-0.25&amp;0.25&amp;0.5 \\\\\\end{bmatrix} \\times \\begin{bmatrix}86.5 \\\\28.5 \\\\35.2 \\\\\\end{bmatrix} = \\begin{bmatrix}5.7 \\\\1.425 \\\\3.1 \\\\\\end{bmatrix}\\] Using lm in R Linear least squares estimation can be carried out in R by simply using the function lm(): mod &lt;- lm(Strength ~ Beans, data = df) summary(mod)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.700 0.4934994 11.550166 0.000001065454 ## BeansType 2 1.425 0.6979136 2.041800 0.071556522658 ## BeansType 3 3.100 0.6979136 4.441811 0.001619249110 "],["maximum-likelkihood-estimation.html", "Maximum likelkihood estimation", " Maximum likelkihood estimation Under the assumptions of a linear model then maximum likelihood estimation is equivalent to least squares. However, (as we'll see in a later module) we often need to be more flexible! Differeniation rules This section is a recap only, if you need a more in-depth overview of differentiation then use the extra materials provided at the start of this module. The constant factor rule \\[(af(x))&#39;=af&#39;(x)\\] The sum rule \\[(f(x)+g(x))&#39;=f&#39;(x)+g&#39;(x)\\] The subtraction rule \\[(f(x) - g(x))&#39;=f&#39;(x) - g&#39;(x)\\] The product rule For the functions \\(f\\) and \\(g\\), the derivative of the function \\(h(x) = f(x)g(x)\\) with respect to \\(x\\) is \\[h&#39;(x) = (fg)&#39;(x) = f&#39;(x)g(x) + f(x)g&#39;(x)\\] The chain rule The derivative of the function \\(h(x)=f(g(x))h(x)=f(g(x))\\) is \\[h&#39;(x)=f&#39;(g(x)\\cdot g&#39;(x)\\] In summary, for any functions \\(f\\) and \\(g\\) and any real numbers \\(a\\) and \\(b\\), the derivative of the function \\(h(x) = a f(x)+bg(x)\\) with respect to \\(x\\) is \\(h&#39;(x)=af&#39;(x) + bg&#39;(x).\\) Logarithm rules This section is a recap only, if you need a more in-depth overview of log rules then use the extra materials provided at the start of this module. Product rule: \\(\\text{log}(xy) = \\text{log}(x) + \\text{log}(y)\\) Quotient rule: \\(\\text{log}(x/y)=\\text{log}(x)‚àí\\text{log}(y)\\) Log of power: \\(\\text{log}(x^y)=y\\text{log}(x)\\) Log of e: \\(\\text{log}(e)=1\\) Log of one: \\(\\text{log}(1)=0\\) Log reciprocal: \\(\\text{log}(1/x)=‚àí\\text{log}(x)\\) Differentiating a log: \\(\\frac{\\delta \\text{log}(y)}{\\delta x} = \\frac{1}{y}\\frac{\\delta y}{\\delta x}.\\) Maximum likelihood estimation for a Binomial distribution The process of using observations to suggest a value for a parameter is called estimation. The value suggested is called the estimate of the parameter. Let us consider data from the published article Influence of predator identity on the strength of predator avoidance responses in lobsters.. The authors were interested in how a juvenile lobster's size was related to its vulnerability to predation. In total, 159 juvenile lobsters were collected from their natural habitat in the Gulf of Maine, USA, and the length of each lobster's carapace (upper shell) was measured to the nearest 3 mm. The lobsters were then tethered to the ocean floor for 24 hours. Any missing lobsters were assumed to have been consumed by a predator, while the surviving lobsters were released. library(tidyverse) data &lt;- read_csv(&quot;https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/lobster.csv&quot;) data %&gt;% mutate(survived = ifelse(survived == 0, &quot;consumed&quot;, &quot;alive&quot;)) %&gt;% group_by(survived) %&gt;% tally() %&gt;% pivot_wider(names_from = c(survived), values_from = n) ## # A tibble: 1 √ó 2 ## alive consumed ## &lt;int&gt; &lt;int&gt; ## 1 79 80 So, 79 of the small juvenile lobsters survived predation from a total of 159. We are interested in the probability of survival, \\(\\theta\\), for the general population. The obvious estimate is simply to take the ratio, \\(\\frac{\\text{number surviving juveniles}}{\\text{total number juveniles}} = \\frac{79}{159} = 0.4968553\\)!! There are, however, many situations where our common sense fails us. For example, what would we do if we had a regression-model situation as in Module 2 and would like to specify an alternative form for \\(\\theta\\), such as \\[\\theta = \\alpha + \\beta \\times (\\text{lobster species}).\\] How would we estimate the unknown intercept \\(\\alpha\\) and slope(s) \\(\\beta\\), assuming we had information on lobster species etc. For this we need a general framework for estimation that can be applied to any situation. The most useful and general method of obtaining parameter estimates is the method of maximum likelihood estimation. I mean the title of this section was a bit of a giveaway! The Likelihood The likelihood function, \\(L(\\theta ; x)\\) is a function of the parameter(s) \\(\\theta\\) for fixed data \\(x\\) and it gives the probability of a fixed observation \\(x\\) for every possible value of the parameter(s) \\(\\theta\\), \\(P(X = x)\\). From above, letting \\(S\\) be the number of lobsters alive after the 24 hours we can assume a Binomial distribution: \\[L(\\theta ; s) = P(S = s) = {n \\choose s} \\theta^s (1 - \\theta)^{n-s}.\\] To obtain the maximum likelihood estimate (i.e., the best guess for \\(\\theta\\) given the observed data) we need to differentiate the likelihood. That is, find it's rate of change given data \\(x\\), \\(\\frac{\\delta L}{\\delta \\theta}\\) . We are interested in the best guess for \\(\\theta\\); this occurs at the point when the rate of change of our likelihood is zero (i.e., \\(\\frac{\\delta L}{\\delta \\theta} = 0\\)) Using the product rule we differentiate \\(L(\\theta ; s)\\): \\[ \\begin{array}{cl} \\frac{\\delta L}{\\delta \\theta} &amp;= {n \\choose s}\\left( s\\theta^{s-1} (1 - \\theta)^{n-s} + \\theta^s (n-s) (1 - \\theta)^{n-s-1}(-1) \\right)\\\\ &amp;= (1 - \\theta)^{n-s-1}\\theta^{s-1}\\{s(1-\\theta) - (n-s)\\theta\\} \\\\ &amp;= (1 - \\theta)^{n-s-1}\\theta^{s-1}\\{s - s\\theta - n\\theta + s\\theta \\}\\\\ &amp;= (1 - \\theta)^{n-s-1}\\theta^{s-1}\\{s - n\\theta \\}. \\end{array}\\] Setting \\(\\frac{\\delta L}{\\delta \\theta} = 0\\) and solve for \\(\\theta\\): \\[\\frac{\\delta L}{\\delta \\theta} = (1 - \\theta)^{n-s-1}\\theta^{s-1}\\{s - n\\theta \\} = 0.\\] There are, technically, three possible solutions to this: when \\(\\theta^{s-1} = 0 \\rightarrow \\theta = 0\\), when \\(s - n\\theta = 0 \\rightarrow\\theta = \\frac{s}{n}\\), or when \\((1 - \\theta)^{n-s-1} \\rightarrow\\theta = 1\\). Now, remember \\(\\theta\\) is a probability and we are after a maximum likelihood estimate based on the data, so based on the above our best guess for \\(\\theta\\) is \\[\\hat{\\theta} = \\frac{s}{n}.\\] Using this for the lobster data we get \\(\\hat{\\theta} = \\frac{79}{159}.\\) Surprise surprise!! Using R ## define the likelihood as a function of the parameter(s) ## Luckily the Binomial likelihood is already defined in R ## as dbinom() likelihood &lt;- function(theta) dbinom(x = 79, size = 159, prob = theta) ## Use the optimise function to optimise!! ## the second argument specifies the plausible ## interval for the parameter ## Note for a number of parameters &gt; 1 ## the optim() function is used optimise(likelihood, c(0,1), maximum = TRUE) ## $maximum ## [1] 0.4968541 ## ## $objective ## [1] 0.06317819 Maximum likelihood estimation for a CRD Recall the following CRD equation \\[Y_{ik} = \\mu_k + \\epsilon_{ik}\\] where \\(Y_{ik}\\) is the response for the \\(k^{th}\\) experimental unit (\\(k = 1, ..., r_i\\), where \\(r_i\\) is the number of experimental replications in the \\(i^{th}\\) level of the treatment factor) subjected to the \\(i^{th}\\) level of the treatment factor (\\(i = 1, ..., t\\),). Here \\(\\mu_i\\) are the different (cell) means for each level of the treatment factor. Under the assumptions of a the CRD (i.e., \\(\\epsilon_{ik} \\sim N(0, \\sigma^2)\\)) then (for equal number of replicates) the estimates of the cell means (\\(\\mu_k\\)) are found by minimising the error of the sum of squares \\[SS_{\\epsilon} = \\Sigma_{i=1}^t \\Sigma_{k=1}^{r_i}(y_{ik}-\\mu_i)^2.\\] Taking the partial derivatives of \\(SS_{\\epsilon}\\) with respect to each cell mean, setting to zero, and solving each equation with give us our estimates: \\[\\frac{\\delta SS_{\\epsilon}}{\\delta \\mu_i} = -2 \\Sigma_{i=1}^t \\Sigma_{k=1}^{r_i}(y_{ik}-\\mu_i) = 0.\\] This works out as \\(\\hat{\\mu_i} = \\overline{y_i.}\\) So in our mask example \\(\\hat{\\mu}_{\\text{Type 1 }} = 5.7 ,\\; \\hat{\\mu}_{\\text{Type 2 }} = 7.125 \\; , \\&amp;\\; \\hat{\\mu}_{\\text{Type 3 }} = 8.8 .\\) Compare these estimates to those we obtained via least squares estimation in the previously. Maximising the log-likelihood In many situations differentiating the likelihood is tricky. Even the simple binomial example above was a bit finicky. So. we often deal with the log-likelihood (the log of the likelihood). Why? The logarithmic function \\(L \\mapsto \\text{log}(L)\\) is increasing, so the functions \\(L(\\theta)\\) and \\(\\text{log}(L(\\theta))\\) will have the same maximum, \\(\\hat{\\theta}\\). When there are observations \\(x_1, \\ldots, x_n\\), the likelihood \\(L\\) is a product as \\(\\text{log}(a b) = \\text{log}(a) + \\text{log}(b)\\), the log-likelihood converts the product into a sum. It is often easier to differentiate a sum than a product, so the log-likelihood is easier to maximize while still giving the same MLE. If we need to use a computer to calculate and maximize the likelihood, there will often be numerical problems with computing the likelihood product, whereas the log-likelihood sum can be accurately calculated. Let's consider the binomial example above. We had \\(L(\\theta ; s) = {n \\choose s} \\theta^s (1 - \\theta)^{n-s}.\\) Therefore, \\[\\begin{array}{cl} \\text{log}(L(\\theta ; s)) &amp;= \\text{log}({n \\choose s}) + \\text{log}(\\theta^s) + \\text{log}((1 - \\theta)^{n-s})\\\\ &amp;= \\text{log}({n \\choose s}) + s\\text{log}(\\theta) + (n-s)\\text{log}(1 - \\theta). \\end{array}\\] Differentiating this: \\[\\begin{array}{cl} \\frac{\\delta \\text{log}(L(\\theta ; s))}{\\delta \\theta} &amp;= 0 + \\frac{s}{\\theta} \\times 1 + \\frac{n-s}{1-\\theta}\\times (-1)\\\\ &amp;= \\frac{s}{\\theta} - \\frac{n-s}{1-\\theta} \\\\ \\end{array}\\] Setting this to zero we get \\(\\frac{s}{\\theta} = \\frac{n-s}{1-\\theta} \\rightarrow s(1-\\theta) = \\theta(n-s) \\rightarrow s - s\\theta = \\theta n - s\\theta \\rightarrow s + (s\\theta - s\\theta) = \\theta n \\rightarrow \\theta = \\frac{s}{n}.\\) Therefore, as above \\[\\hat{\\theta} = \\frac{s}{n}.\\] Maximum likelihood estimation for a Poisson distribution The Poisson process counts the number of events occurring in a fixed time or space, when events occur independently and at a constant average rate, \\(\\lambda\\). For \\(X \\sim \\text{Poisson}(\\lambda)\\), \\[f_X(x) = P(X=x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}\\] for \\(x = 0,1,2,\\dots\\) Maximising the Likelihood Suppose that \\(x_1, \\ldots, x_n\\) are iid observations from a Poisson distribution with unknown parameter \\(\\lambda\\): \\[ L(\\lambda\\,; x_1, \\ldots, x_n) = K e^{-n\\lambda} \\, \\lambda^{n \\overline{x}} \\,,\\] where \\(\\overline{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\\), and \\(K=\\prod_{i=1}^n \\frac1{x_i\\,!}\\) is a constant that doesn't depend on \\(\\lambda\\). We differentiate \\(L(\\lambda\\,; x_1, \\ldots, x_n)\\) and set to 0 to find the MLE: \\[\\begin{array}{rl} 0 &amp;= \\frac{\\delta}{\\delta\\lambda} L(\\lambda\\,; x_1, \\ldots, x_n) \\\\ &amp;= K \\left( -n e^{-n\\lambda} \\, \\lambda^{n \\overline{x}} + n\\overline{x} e^{-n\\lambda} \\, \\lambda^{n \\overline{x} - 1} \\right)\\\\ &amp;= K e^{-n\\lambda} \\lambda^{n \\overline{x} - 1} \\left(- n\\lambda + n\\overline{x} \\right) \\end{array}\\] \\(\\rightarrow \\lambda=\\infty, \\lambda=0, \\text{or } \\lambda = \\overline{x}.\\) If we know that \\(L(\\lambda\\,; x_1, \\ldots, x_n)\\) reaches a unique maximum in \\(0 &lt; \\lambda &lt; \\infty\\) then the MLE is \\(\\overline{x}\\). So the maximum likelihood estimator is \\[ \\hat{\\lambda} = \\overline{x} = \\frac{x_1 + \\ldots + x_n}{n} \\;.\\] Maximising the log-likelihood function As above, \\[L(\\lambda\\,; x_1, \\ldots, x_n) = \\prod_{i=1}^n \\frac{\\lambda^{x_i}}{x_i\\,!} e^{-\\lambda}.\\] Therefore, \\[\\begin{array}{rl}\\text{log}(L(\\lambda\\,; x_1, \\ldots, x_n)) &amp;= \\sum_{i=1}^n \\text{log}(\\frac{\\lambda^{x_i}}{x_i\\,!} e^{-\\lambda})\\\\ &amp;= \\sum_{i=1}^n \\text{log}(\\frac{1}{x_i\\,!}) + \\text{log}(\\lambda^{x_i}) + \\text{log}(e^{-\\lambda}) \\\\ &amp;= \\sum_{i=1}^n \\text{log}(\\frac{1}{x_i\\,!}) + x_i \\text{log}(\\lambda) + (-\\lambda) \\\\ &amp;= K&#39; + \\text{log}(\\lambda) \\sum_{i=1}^n x_i - n \\lambda \\quad \\mbox{ where $K&#39;$ is a constant} \\\\ &amp;= K&#39; + \\text{log}(\\lambda) n\\overline{x} - n \\lambda. \\end{array}\\] Differentiate and set to 0 for the MLE: \\[\\begin{array}{rcl} 0 &amp;=&amp; \\frac{\\delta}{\\delta\\lambda} \\text{log} (L(\\lambda\\,; x_1, \\ldots, x_n) ) \\\\ &amp;=&amp; \\frac{\\delta}{\\delta\\lambda} \\left (K&#39; + \\text{log}(\\lambda) n\\overline{x} - n \\lambda \\right )\\\\ &amp;=&amp; \\frac{n\\overline{x}}{\\lambda} - n \\\\ \\end{array}\\] assuming a unique maximum in \\(0 &lt; \\lambda &lt; \\infty\\) the MLE is \\(\\hat{\\lambda} = \\overline{x}\\) as before. Maximum likelihood estimation for a continuous random variable Let \\(X\\) be a continuous random variable with probability density function \\[f_X(x) = \\left\\{ \\begin{array}{cl} \\frac{2(s-x)}{s^2} &amp; \\mbox{for } 0 &lt; x &lt; s\\,, \\\\ 0 &amp; \\mbox{otherwise}\\,. \\end{array} \\right.\\] Here, \\(s\\) is a parameter to be estimated, where \\(s\\) is the maximum value of \\(X\\) and \\(s&gt;0\\). Assuming a single observation \\(X=x\\) the likelihood function is \\[ L(s\\,;x) = \\frac{2(s-x)}{s^2}\\] for \\(\\;x &lt; s &lt; \\infty.\\) Differentiating this \\[\\begin{array}{rl} \\frac{dL}{ds} &amp;= 2 \\left(-2 s^{-3}(s-x) + s^{-2}\\right ) \\\\ &amp;= 2s^{-3} (-2(s-x) + s) \\\\ &amp;= \\frac{2}{s^3} (2x-s). \\end{array}\\] At the MLE, \\[ \\frac{\\delta L}{\\delta s} = 0 \\implies s=\\infty \\quad\\mbox{ or }\\quad s = 2x.\\] Realistically \\(s=\\infty\\) is not the maximum (see graph below) so \\(s = 2x.\\) Therefore maximum likelihood estimator is \\[\\hat{s} = 2X.\\] Using R to get the MLE Let's say we observe \\(X = 3\\), then to find the MLE using R we use ## define the likelihood as a function of the parameter(s) likelihood &lt;- function(s) (2*(s - 3))/s^2 ## Use the optimise function to optimise ## the second argument specifies the plausible ## interval for the parameter ## Note for a number of parameters &gt; 1 ## the optim() function is used optimise(likelihood, c(1,8), maximum = TRUE) ## $maximum ## [1] 6.000018 ## ## $objective ## [1] 0.1666667 How does this to compare to the exact estimator, \\(\\hat{s} = 2X\\), we found using calculus above? Consider, too, the plot below showing \\(L(s; X = 3)\\). "],["introduction-to-bayesian-statistics.html", "Introduction to Bayesian statistics", " Introduction to Bayesian statistics ‚ÄúCritical thinking is an active and ongoing process. It librarys that we all think like Bayesians, updating our knowledge as new information comes in.‚Äù --- Daniel J. Levitin, A Field Guide to Lies: Critical Thinking in the Information Age Conditional probability The probability of the event \\(A\\) occurring given that the event \\(B\\) has already occurred is \\[P(A‚à£B) = \\frac{P(A \\:\\text{and}\\: B)}{P(B)}\\]. This is called a conditional probability. Note that \\(P(A‚à£B)\\) is not the same as \\(P(B‚à£A)\\) An example Rapid antigen (lateral flow) tests are rapid antigen tests used to detect SARS-COV-2 infection (COVID-19). They are very easy to use and a lot less uncomfortable than having a swab for a PCR taken! In summary, the lateral flow test can show a positive (\\(+\\)) or a negative (\\(-\\)) result. The person taking the test either has (infected) or does not have COVID-19 (not infected). It is reported that the average sensitivity of the Innova lateral flow tests is \\(\\sim 0.787\\). Breaking this down means that given you have SARS-CoV-2 (Covid19) the chance of a positive lateral flow test is 0.787. It was also reported that the specificity of this test was 0.997. That is, the chance of a negative test given that you do not have COVID-19 is 0.997. This can be summarised as \\(P( + | \\text{infected}) = 0.787\\) and \\(P( -| \\text{not infected}) = 0.997\\) What you would probably like to know is given that the test is positive, what is the probability that you have COVID-19? \\(P( \\text{infected}| +) = ?\\) Let's assume that people in the population with COVID-19 is 10% (not far off the estimated % with Omicron in London a few weeks ago); that is, \\(P(\\text{infected}) = 0.1\\). But, what about \\(P( \\text{infected}| +) ?\\) Recall, \\[P(A‚à£B) = \\frac{P(A \\:\\text{and}\\: B)}{P(B)}.\\] So, \\[P( \\text{infected}| +) = \\frac{P(\\text{infected} \\:\\text{and}\\: +)}{P(+)}.\\] We have, \\(P(\\text{infected} \\:\\text{and}\\: +) = P(\\text{infected})\\times P( + | \\text{infected}) = 0.1 \\times 0.787 = 0.0787.\\) So, \\(P(+) = P(\\text{infected} \\:\\text{and}\\: +) + P(\\text{clear} \\:\\text{and}\\: +) = 0.0787 + ( 0.9 \\times (1 - 0.997)) = 0.0787 + ( 0.9 \\times 0.003) = 0.0787 + 0.0027 = 0.0814.\\) Therefore, \\(P( \\text{infected}| +) = \\frac{ 0.0787}{0.0814} = 0.9668305.\\) Rearranging, \\[P( \\text{infected}| +) = \\frac{P( + | \\text{infected})P(\\text{infected})}{P(+)}.\\] Bayes' rule Bayes' theorem, named after British mathematician Reverend Thomas Bayes, is a mathematical formula for determining conditional probability. His work and theorems were presented in An Essay towards solving a Problem in the Doctrine of Chances, this was read to the Royal Society in 1763 after his death. Theorem, Bayes' rule The conditional probability of the event \\(A\\) conditional on the event \\(B\\) is given by \\[P(B|A) = \\frac{P(A|B)P(B)}{P(A)}\\] Let's think of this in terms of our data and hypothesis: \\[P(\\text{hypothesis}‚à£\\text{data}) = \\frac{P(\\text{data} | \\text{hypothesis} )P(\\text{hypothesis})}{P(\\text{data})}\\] Recall from the previous sections that our hypotheses relate to estimating parameter values (e.g., intercepts, differences in means, slopes etc ). The formula above (Bayes' theorem) calculates the probability of the parameter(s), say \\(\\theta\\), values given the data. But what is the difference here to maximum likelihood estimation (i.e., MLE, the frequentist approach)? Taking an MLE approach assumes that the parameters are fixed (i.e., they have one true value); the parameters are unknown and are to be estimated. Using this approach we typically estimate a point estimate of the parameter of interest. Taking a Bayesian approach assumes that the parameters are not fixed. Instead, parameters are assumed to come from some fixed unknown distribution (i;e., a range of plausible values). This approach librarys that we have some prior beliefs about the data (even if these beliefs are uninformative). This information is introduces a priori to the modelling framework. \\[P(B|A) = \\frac{P(A|B)P(B)}{P(A)}\\] Let \\(A = \\theta\\) and \\(B = \\text{data}\\), then the above translates to \\[P(\\theta‚à£\\text{data}) = \\frac{P(\\text{data} | \\theta )P(\\theta)}{P(\\text{data})}\\] where \\(P(\\theta‚à£\\text{data})\\) represents what you know after having seen the data. This is called he posterior distribution and is the basis for inference, a distribution, possibly multivariate if more than one parameter (\\(\\theta\\)). \\(\\frac{P(\\text{data} | \\theta )\\) is the likelihood, think back to the previous section. \\(P(\\text{data})\\) is called the prior distribution and represents what you know before seeing the data. The source of much discussion about the Bayesian approach. Now. \\[P(\\text{data}) = \\int P(\\text{data}|\\theta)P(\\theta)d\\theta\\] is typically a high-dimensional integral, difficult if not impossible to calculate. A simple example: lobsters Again we consider data from the published article Influence of predator identity on the strength of predator avoidance responses in lobsters.. Recall that the authors were interested in how a juvenile lobster's size was related to its vulnerability to predation. In total, 159 juvenile lobsters were collected from their natural habitat in the Gulf of Maine, USA, and the length of each lobster's carapace (upper shell) was measured to the nearest 3 mm. The lobsters were then tethered to the ocean floor for 24 hours. Any missing lobsters were assumed to have been consumed by a predator, while the surviving lobsters were released. We define large juvenile's as those with carapace \\(\\geq\\) 40 mm, and otherwise we class them as small. library(tidyverse) data &lt;- read_csv(&quot;https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/lobster.csv&quot;) lobsters &lt;- data %&gt;% mutate(size = ifelse(size &gt;= 40, &quot;large&quot;, &quot;small&quot;)) %&gt;% mutate(survived = ifelse(survived == 0, &quot;consumed&quot;, &quot;alive&quot;)) %&gt;% group_by(size, survived) %&gt;% tally() %&gt;% pivot_wider(names_from = c(survived), values_from = n) size alive consumed large 56 23 small 23 57 So, as before, 23 of the small juvenile lobsters survived predation from a total of 80. We are interested in the probability of survival, \\(\\theta\\), for the general population of small lobsters. The obvious estimate is simply to take the ratio, \\(\\frac{23}{80} = 0.2875\\) . But, what are the implied stats behind our common sense estimate? Let \\(S\\) be the number alive after the 24 hours, then we can assume a Binomial distribution: \\[P(S = s) = {n \\choose s} \\theta^s (1 - \\theta)^{n-s}\\] A frequentist approach would be to maximise the likelihood with respect to \\(\\theta\\), see the previous section. This would result in the maximum likelihood estimate (MLE) of \\(\\hat{\\theta} = \\frac{23}{80} = 0.2875\\) Using a Bayesian approach we first need to start off with a prior distribution. This should reflect our prior beliefs about the parameter(s) of interest. We know \\(\\theta\\) is a probability, so it is a continuous random variable and that lies between zero and one. A suitable prior distribution might be the Beta defined on the interval [0, 1]. Therefore, we assume a priori \\(\\theta \\sim \\text{Beta(a, b)}\\) so that \\(P(\\theta) = \\theta^{a‚àí1}(1 ‚àí \\theta)^{b‚àí1}\\). See the plot below for the range of shapes a Beta distribution takes with different parameter values. Recall from above that \\[P(\\theta‚à£\\text{data}) = \\frac{P(\\text{data} | \\theta )P(\\theta)}{P(\\text{data})},\\] which can be written as \\[P(\\theta‚à£\\text{data}) \\propto P(\\text{data} | \\theta )P(\\theta).\\] The \\(\\propto\\) means proportional to; basically this means we can ignore any terms not containing the parameters. Therefore, \\[\\begin{array}{rl} P(\\theta‚à£s) &amp; \\propto {n \\choose s} \\theta^s (1 - \\theta)^{n-s}\\theta^{a‚àí1}(1 ‚àí \\theta)^{b-1} \\\\ &amp; \\propto \\theta^{(a+s)‚àí1}(1 ‚àí \\theta)^{(b+n‚àís)‚àí1} \\end{array}.\\] So the posterior distribution for survival is \\(\\theta | s \\sim \\text{Beta}(a + s, b + n - s).\\) We're going to choose an uninformative prior (i.e., \\(\\text{Beta}(1, 1)\\) above). So \\(\\theta_{\\text{small}} \\sim \\text{Beta}(1 + 23, 1 + 80 - 23) = \\text{Beta}(24, 58).\\) We want the expected value of this, which already has an explicit form: \\[\\mathbb{E}[\\text{Beta}(a, b)] = \\frac{a}{a + b} = \\frac{24}{82}.\\] How does this compare to our MLE estimate from above? Typically, Bayesian and frequentist estimates will always agree if there is sufficient data, so long as the likelihood is not explicitly ruled out by the prior. --- Olivier Gimenez, Bayesian statistics with R Prior sensitivity When choosing a prior distribution you should focus on what that prior means in the context of the research problem. Prior choice will influence the posterior distribution. Uninformative priors can be chosen if we wish to rely only on the likelihood (i.e., let the data speak for itself), which is itself subjective based on how/where data were collected. However, uninformative priors are in general unrealistic as equal weight is given to all values! Prior choice and sensitivity is beyond the scope of this course; however, I would strongly encourage you to read the linked materials at the start of this module. What would happen to our posterior distribution, above, if we were to choose a different prior? "],["module-5.html", "Module 5", " Module 5 Learning objectives Describe the following components of a generalised linear model the response variable the response distribution the link function Fit Poisson and logistic regression models using R Interpret estimated effects with reference to confidence intervals from Poisson and logistic regression models Determine a sensible response distribution to use for a generalised linear model fitted to a particular data set Include and interpret interaction effects in a generalised linear model using R "],["introduction-to-generalised-linear-models-glms.html", "Introduction to generalised linear models (GLMs)", " Introduction to generalised linear models (GLMs) &quot;Ecology has increasingly become a data- and model-centric discipline...&quot; --- Trends in ecology and conservation over eight decades Recall, the simple linear regression model from module 2: \\[Y_i = \\alpha + \\beta_1x_i + \\epsilon_i\\] where \\[\\epsilon_i \\sim \\text{Normal}(0,\\sigma^2).\\] Here for observation \\(i\\) \\(Y_i\\) is the value of the response \\(x_i\\) is the value of the explanatory variable \\(\\epsilon_i\\) is the error term: the difference between \\(Y_i\\) and its expected value \\(\\alpha\\) is the intercept term (a parameter to be estimated), and \\(\\beta_1\\) is the slope: coefficient of the explanatory variable (a parameter to be estimated) We also saw a different specification of this model in module 3: There is an alternative, equivalent way of specifying the linear regression model which attributes the randomness directly to the response variable rather than the error \\(\\epsilon_i\\): \\[Y_i \\sim \\text{Normal}(\\alpha + \\beta_1 x_i, \\sigma^2).\\] That is, we assume the \\(i^{th}\\) observation's response, \\(Y_i\\), comes from a normal distribution with mean \\(\\mu_i = \\alpha + \\beta_1 x_i\\) and variance \\(\\sigma^2\\). In this case we assume that the \\(i^{th}\\) observation's response, \\(Y_i\\), comes from a normal distribution, the mean of \\(Y_i\\) is a linear combination of the explanatory terms, the variance of \\(Y_i\\), \\(\\sigma^2\\), is the same for all observations, and that each observation's response is independent of all others. But, what if we want to be a little more flexible and move away from some of these assumptions? What if we want to rid ourselves from a model with normal errors? The answer, Generalised Linear Models (GLMs). "],["poisson-regression.html", "Poisson regression", " Poisson regression We saw the Poisson distribution in module 4. There, we talked about it in terms of rates. Additionally, it is commonly used as a distribution for counts. In summary, the Poisson distribution is a discrete distribution (of positive values only) and has \\(\\text{Var}(Y_i) = \\mu_i\\) (i.e., we expect the variance to increase with the mean). If we were to assume (as previously) that \\(\\mu_i = \\alpha + \\beta_1 x_i\\) then we would be allowing \\(\\mu &lt; 0\\), which is not supported by the Poisson distribution. So, we use a link function to map between \\(\\mu_i\\) and the real number line: \\[\\text{log}(\\mu_i) = \\alpha + \\beta_1 x_i.\\] So, \\(\\mu_i \\geq 0\\) and \\(-\\infty &lt; \\text{log}(\\mu_i) &lt; \\infty\\); however negative the linear predictor \\(\\alpha + \\beta_1 x_i\\) gets \\(\\mu_i\\) will always be positive. Equivalently, \\[ \\mu_i = \\text{exp}(\\alpha + \\beta_1 x_i)\\] and \\[Y_i \\sim \\text{Poisson}(\\mu_i)\\] Interpreting coefficients Recall, for a linear regression model \\(\\mu = \\alpha + \\beta_1x\\), when \\(x=0\\) \\(y = \\alpha\\) and for every one-unit increase in \\(x\\), \\(y\\) increases by amount \\(\\beta_1\\). For a Poisson regression model we have \\(\\text{log}(\\mu) = \\alpha + \\beta_1 x.\\) We can interpret this in the same way! That is, when \\(x\\) is zero, the log of the expected value of the response equals \\(\\alpha\\) and for every one-unit increase in \\(x\\), the log of the expected value of the response increases by amount \\(\\beta_1\\). But interpreting the effect of \\(x\\) on the log of the expected value is not straightforward. Now, we have \\[\\mu = \\text{e}^{ \\alpha + \\beta_1 x} = \\text{e}^{ \\alpha}(\\text{e}^{\\beta_1})^{ x}.\\] This implies that when \\(x = 0\\) \\(\\mu = \\text{e}^{ \\alpha}(\\text{e}^{\\beta_1})^{0} = \\text{e}^{ \\alpha}\\times 1 = \\text{e}^{ \\alpha},\\) when \\(x = 1\\) \\(\\mu = \\text{e}^{ \\alpha}(\\text{e}^{\\beta_1})^{1} = \\text{e}^{ \\alpha}\\text{e}^{\\beta_1} = \\text{e}^{ \\alpha + \\beta_1},\\) when \\(x = 2\\) \\(\\mu = \\text{e}^{ \\alpha}(\\text{e}^{\\beta_1})^{2} = \\text{e}^{ \\alpha}\\text{e}^{\\beta_1} \\text{e}^{\\beta_1} = \\text{e}^{ \\alpha + \\beta_1 + \\beta_1},\\) when \\(x = 3\\) \\(\\mu = \\text{e}^{ \\alpha}(\\text{e}^{\\beta_1})^{3} = \\text{e}^{ \\alpha}\\text{e}^{\\beta_1} \\text{e}^{\\beta_1}\\text{e}^{\\beta_1} = \\text{e}^{ \\alpha + \\beta_1 + \\beta_1 + \\beta_1},\\) and so on ... Therefore, for every n-unit increase in x, the expected value of the response is multiplied by \\(\\text{e}^{n\\beta_1}.\\) Goodness-of-fit Typically, we use Poisson regression to ensure the expected value is \\(&gt;0\\), account for non-constant variance, and assume a discrete distribution for a discrete response. But, how do we assess if our choice was appropriate? Use the deviance! For a fitted Poisson regression the deviance, \\(D\\), is \\[D = 2 \\sum^{n}_{i=1} \\{ Y_{i} \\log(Y_{i}/\\mu_{i}) - (Y_{i}-\\mu_{i}) \\}\\] where if \\(Y_i=0\\), the \\(Y_{i} \\log(Y_{i}/\\mu_{i})\\) term is taken to be zero, and \\(\\mu_{i} = \\exp(\\hat{\\beta}_{0} + \\hat{\\beta}_{1}X_{1} + ... + \\hat{\\beta}_{p} X_{p})\\) is the predicted mean for observation \\(i\\) based on the estimated model parameters. The deviance is a measure of how well the model fits the data. That is, if the model fits well, the observed values \\(Y_{i}\\) will be close to their predicted means \\(\\mu_{i}\\), causing both of the terms in \\(D\\) to be small, and so the deviance to be small. The flip side of this is that a large deviance indicates a bad fitting model. Formally, we can test the null hypothesis that the model is correct by calculating a p-value using \\[ p = \\Pr(\\chi^2_{n - k} &gt; D).\\] Conditions of the chi-squared approximation The distribution of the deviance under the null hypothesis is approximately chi-squared if the response of each observation is well approximated by a normal distribution. This holds for Poisson random variables with \\(\\mu_i &gt; 5\\). However, if our chi-squared approximation assumptions are not met we should find another way. See the example below for a simulation based approach. An example: bird abundance A recent publication Partitioning beta diversity to untangle mechanisms underlying the assembly of bird communities in Mediterranean olive groves investigates bird abundance data for a number of olive farms. Each farm is catalogued according to the level of landscape complexity (high; intermediate; low) and the type of management of the ground cover (extensive or intensive). library(tidyverse) data &lt;- read_delim(&quot;https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/bird_abundance.csv&quot;) birds &lt;- data %&gt;% dplyr::select(c(&quot;OliveFarm&quot;,&quot;Management&quot;,&quot;Turdus_merula&quot;,&quot;Phylloscopus_collybita&quot;)) %&gt;% pivot_longer(., c(-OliveFarm, -Management), &quot;Species&quot;, values_to = &quot;Count&quot;) birds ## # A tibble: 80 √ó 4 ## OliveFarm Management Species Count ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 MORALEDAEXP intensive Turdus_merula 21 ## 2 MORALEDAEXP intensive Phylloscopus_collybita 14 ## 3 MORALEDACON extensive Turdus_merula 29 ## 4 MORALEDACON extensive Phylloscopus_collybita 21 ## 5 OJUELOSEXP intensive Turdus_merula 5 ## 6 OJUELOSEXP intensive Phylloscopus_collybita 31 ## 7 OJUELOSCON extensive Turdus_merula 12 ## 8 OJUELOSCON extensive Phylloscopus_collybita 52 ## 9 LUNAEXP extensive Turdus_merula 103 ## 10 LUNAEXP extensive Phylloscopus_collybita 10 ## # ‚Ä¶ with 70 more rows Fitting a poisson model we specify family = &quot;poisson&quot; in a call to glm(). Note that the default link function for family = &quot;poisson&quot; is the log link; we could also use the equivalent syntax poisson(link = log) to specify this model. Or, we could change the link function to something else (e.g., poisson(link = identity)) that makes sense. glm_bird &lt;- glm(Count ~ Species, data = birds, family = &quot;poisson&quot;) summary(glm_bird) ## ## Call: ## glm(formula = Count ~ Species, family = &quot;poisson&quot;, data = birds) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -9.994 -3.851 -1.348 2.868 10.828 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.06105 0.03422 89.45 &lt;2e-16 *** ## SpeciesTurdus_merula 0.94537 0.04032 23.45 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 2234.6 on 79 degrees of freedom ## Residual deviance: 1621.9 on 78 degrees of freedom ## AIC: 2030.1 ## ## Number of Fisher Scoring iterations: 5 The fitted model is therefore equatiomatic::extract_eq(glm_bird, use_coefs = TRUE, coef_digits = 3) \\[ \\log ({ \\widehat{E( \\operatorname{Count} )} }) = 3.061 + 0.945(\\operatorname{Species}_{\\operatorname{Turdus\\_merula}}) \\] Interpreting the coefficients Interpreting the coefficients above we estimate that the log of the expected number of Phylloscopus collybita is 3.06 and the log of the expected number of Turdus merula is 4.01. But what about the expected number? exp(coef(glm_bird)) ## (Intercept) SpeciesTurdus_merula ## 21.35000 2.57377 Therefore, we estimate that the expected average number of Phylloscopus collybita is 21.35 and the expected average number of Turdus merula is 54.95. Using confidence intervals: confint &lt;- exp(confint(glm_bird)[1,]) confint ## 2.5 % 97.5 % ## 19.94989 22.81408 Therefore, we estimate that the expected average number of Phylloscopus collybita is between 19.95 and 22.81. For a multiplicative interpretation of the effect use \\(\\text{exp}(\\beta_1)\\) exp(coef(glm_bird)[2]) ## SpeciesTurdus_merula ## 2.57377 Therefore, we estimate that the expected number of Turdus merula is 2.57 \\(\\times\\) greater than Phylloscopus collybita. For a percentage-change interpretation use \\(100 \\times \\left (\\text{exp}(\\beta_1)‚àí1 \\right )\\): 100*(exp(coef(glm_bird)[2]) - 1) ## SpeciesTurdus_merula ## 157.377 Therefore, we estimate that expected number of Turdus merula is 157.38% greater than Phylloscopus collybita. Using 95% confidence intervals: confint &lt;- 100*(exp(confint(glm_bird)[2, ]) - 1) confint ## 2.5 % 97.5 % ## 137.9259 178.6742 So, we estimate that expected number of Turdus merula is between 137.93% and 178.67% greater than Phylloscopus collybita. Deviance Chi-squared approach Luckily no manual calculations are libraryd, our glm() call returns the values we need directly: ## extract the residual deviance D &lt;- glm_bird$deviance D ## [1] 1621.948 ## extract the residual degrees of freedom (n-k) df &lt;- glm_bird$df.residual df ## [1] 78 Therefore, to test the relevant null hypothesis (that the model is correct) we use 1 - pchisq(D, df) ## [1] 0 We have strong evidence to reject the null hypothesis; suggesting a lack of fit! Simulation based approach Are our chi-squared approximation assumptions met? "],["logistic-regression.html", "Logistic regression", " Logistic regression We saw the Binomial distribution in module 4. We define a random variable, \\(Y_i\\), to have a binomial distribution if it is the number of successes from a number of independent trials, \\(n\\), each with the same probability of success, \\(p\\). It is a discrete distribution, which notes that the number of successes associated with the \\(i^{th}\\) observation must be an integer between \\(0\\) and \\(n_i\\). In addition, it builds in the non-constant variance of \\(Y_i\\) and \\(\\frac{Y_i}{n_i}\\): \\(\\text{Var}(Y_i)=n_ip_i(1‚àíp_i)\\) and \\(\\text{Var}(\\frac{Y_i}{n_i}) = \\frac{p_i(1‚àíp_i)}{n_i}\\). If we were to assume a linear relationship (as previously) that \\(p_i = \\alpha + \\beta_1 x_i\\) then we would be allowing \\(p &lt; 0\\) and \\(p&gt;1\\), which is not supported by the binomial distribution. So, we use a link function to map between \\(p\\) and the real number line: \\[\\text{logit}(p_i) = \\text{log}\\left (\\frac{p_i}{1 - p_i}\\right ) = \\alpha + \\beta_1x_i.\\] This leads to \\[p_i = \\frac{\\exp(\\alpha + \\beta_1x_i)}{1 + \\exp(\\alpha + \\beta_1x_i)}.\\] and \\[Y_i \\sim \\text{Binomial}(n_i, p_i)\\] Interpreting coefficients Recap, for probability \\(p\\) the odds are \\(\\frac{p}{1-p}\\) and the log-odds are \\(\\text{log}\\left (\\frac{p}{1-p}\\right )\\): when \\(p=0.5\\) the odds \\(=1\\) and the log-odds \\(= 0\\) when \\(p=1\\) the odds \\(\\infty\\) and the log-odds \\(= \\infty\\) when \\(p=0\\) the odds \\(=0\\) and the log-odds \\(= -\\infty\\) Recall, for a linear regression model \\(\\mu = \\alpha + \\beta_1x\\), when \\(x=0\\) \\(y = \\alpha\\) and for every one-unit increase in \\(x\\), \\(y\\) increases by amount \\(\\beta_1\\). For a logistic regression model we have \\[\\begin{array}{rl} \\text{logit}(p) = \\alpha + \\beta_1 x \\\\ \\text{log}\\left (\\frac{p}{1-p}\\right ) = \\alpha + \\beta_1 x \\\\ \\text{log}\\left (\\text{odds}\\right ) = \\alpha + \\beta_1 x \\\\ \\end{array}\\] We can interpret this as when \\(x = 0\\), the log-odds of success are equal to \\(\\alpha\\) and that for every one-unit increase in \\(x\\) the log-odds of success increase by \\(\\beta_1\\). But, interpreting the effect of \\(x\\) on the log-odds of success is not straightforward. Now, we have \\[\\text{odds} = \\text{e}^{ \\alpha + \\beta_1 x} = \\text{e}^{ \\alpha}(\\text{e}^{\\beta_1})^{ x}.\\] This implies that when \\(x = 0\\) \\(\\text{odds} = \\text{e}^{ \\alpha}(\\text{e}^{\\beta_1})^{0} \\text{e}^{ \\alpha}\\times 1 = \\text{e}^{ \\alpha},\\) when \\(x = 1\\) \\(\\text{odds} = \\text{e}^{ \\alpha}(\\text{e}^{\\beta_1})^{1} = \\text{e}^{ \\alpha}\\text{e}^{\\beta_1} = \\text{e}^{ \\alpha + \\beta_1},\\) when \\(x = 2\\) \\(\\text{odds} = \\text{e}^{ \\alpha}(\\text{e}^{\\beta_1})^{2} = \\text{e}^{ \\alpha}\\text{e}^{\\beta_1} \\text{e}^{\\beta_1} = \\text{e}^{ \\alpha + \\beta_1 + \\beta_1},\\) when \\(x = 3\\) \\(\\text{odds} = \\text{e}^{ \\alpha}(\\text{e}^{\\beta_1})^{3} = \\text{e}^{ \\alpha}\\text{e}^{\\beta_1} \\text{e}^{\\beta_1}\\text{e}^{\\beta_1} = \\text{e}^{ \\alpha + \\beta_1 + \\beta_1 + \\beta_1},\\) and so on ... Goodnes-of-fit Typically, we use logistic regression to model if we have a binary, or proportional response (e.g., success vs. failure). But, how do we assess if our choice was appropriate? Using the deviance? Formally, we can test the null hypothesis that the model is correct by calculating a p-value using \\[ p = \\Pr(\\chi^2_{n - k} &gt; D).\\] Conditions of the chi-squared approximation The distribution of the deviance under the null hypothesis is approximately chi-squared if the response of each observation is well approximated by a normal distribution. This holds for binomial random variables if the number of trials, \\(n_i\\), is large enough: + when \\(p_i\\) is close to 0.5, \\(n_i \\geq 5\\) is probably sufficient, + but if \\(p_i\\) is close to 0 or 1, \\(n_i\\) must be much larger. However, if our chi-squared approximation assumptions are not met we should find another way. See the example below for a simulation based approach. An example: lobsters Let us, again, consider data from the published article Influence of predator identity on the strength of predator avoidance responses in lobsters.. The authors were interested in how a juvenile lobster's size was related to its vulnerability to predation. In total, 159 juvenile lobsters were collected from their natural habitat in the Gulf of Maine, USA, and the length of each lobster's carapace (upper shell) was measured to the nearest 3 mm, size. The lobsters were then tethered to the ocean floor for 24 hours. Any missing lobsters were assumed to have been consumed by a predator, while the surviving lobsters were released (i.e., survived = 1 if lobster survived, 0 otherwise). library(tidyverse) data &lt;- read_csv(&quot;https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/lobster.csv&quot;) data ## # A tibble: 159 √ó 2 ## size survived ## &lt;dbl&gt; &lt;dbl&gt; ## 1 42 0 ## 2 36 0 ## 3 51 1 ## 4 33 0 ## 5 33 1 ## 6 45 1 ## 7 54 1 ## 8 48 0 ## 9 39 0 ## 10 48 1 ## # ‚Ä¶ with 149 more rows Ungrouped model Fitting a binomial model we specify family = &quot;binomial&quot; in our glm call. Note that the default link function for family = &quot;binomial&quot; is the logit link; we could also use the equivalent syntax binomial(link = logit)to specify this model. glm_mod_ug &lt;- glm(survived ~ size, family = &quot;binomial&quot;, data = data) summary(glm_mod_ug) ## ## Call: ## glm(formula = survived ~ size, family = &quot;binomial&quot;, data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1019 -0.8454 -0.3771 0.9304 2.0711 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -7.89597 1.38501 -5.701 0.00000001191 *** ## size 0.19586 0.03415 5.735 0.00000000977 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 220.41 on 158 degrees of freedom ## Residual deviance: 172.87 on 157 degrees of freedom ## AIC: 176.87 ## ## Number of Fisher Scoring iterations: 4 The fitted model is therefore \\[ \\log\\left[ \\frac { \\widehat{P( \\operatorname{survived} = \\operatorname{1} )} }{ 1 - \\widehat{P( \\operatorname{survived} = \\operatorname{1} )} } \\right] = -7.896 + 0.196(\\operatorname{size}) \\] Grouped model The data are currently ungrouped, despite many lobsters sharing the same carapace size. Therefore, we rearrange the data set so that it is grouped: grouped &lt;- data %&gt;% group_by(size) %&gt;% summarise(y = sum(survived), n = length(survived), p = mean(survived)) grouped ## # A tibble: 11 √ó 4 ## size y n p ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 27 0 5 0 ## 2 30 1 10 0.1 ## 3 33 3 22 0.136 ## 4 36 7 21 0.333 ## 5 39 12 22 0.545 ## 6 42 17 29 0.586 ## 7 45 13 18 0.722 ## 8 48 12 17 0.706 ## 9 51 7 8 0.875 ## 10 54 6 6 1 ## 11 57 1 1 1 Where, size is as above, y is the number of lobsters of each size that survived, t is the total number of lobsters of each size, and pis the proportion of lobsters of each size that survived. Fitting a binomial model again we specify family = &quot;binomial&quot; in our glm call and specify our response as cbind(y, n - y): glm_mod_gr &lt;- glm(cbind(y, n - y) ~ size, family = &quot;binomial&quot;, data = grouped) summary(glm_mod_gr) ## ## Call: ## glm(formula = cbind(y, n - y) ~ size, family = &quot;binomial&quot;, data = grouped) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.12729 -0.43534 0.04841 0.29938 1.02995 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -7.89597 1.38501 -5.701 0.00000001191 *** ## size 0.19586 0.03415 5.735 0.00000000977 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 52.1054 on 10 degrees of freedom ## Residual deviance: 4.5623 on 9 degrees of freedom ## AIC: 32.24 ## ## Number of Fisher Scoring iterations: 4 The fitted model is again \\[ \\log\\left[ \\frac { \\widehat{P( \\operatorname{survived} = \\operatorname{1} )} }{ 1 - \\widehat{P( \\operatorname{survived} = \\operatorname{1} )} } \\right] = -7.896 + 0.196(\\operatorname{size}) \\] Interpreting the coefficients Interpreting the coefficients above we estimate that the log-odds of a juvenile lobster surviving are -8 (use your common sense to ascertain if interpreting the intercept is sensible). We estimate that for every 1mm increase in carapace length the log-odds of a juvenile lobster surviving increase by 0.196. What about the odds? exp(coef(glm_mod_gr)) ## (Intercept) size ## 0.0003722407 1.2163540760 Therefore, we estimate that the odds of a juvenile lobster surviving are 0.000372 (use your common sense to ascertain if interpreting the intercept is sensible). We estimate that for every 1mm increase in carapace length the odds of a juvenile lobster surviving are multiplied by 1.22. What about for a 5mm increase in carapace length? exp(5*coef(glm_mod_gr)[2]) ## size ## 2.662564 Therefore, we estimate that for every 5mm increase in carapace length the odds of a juvenile lobster surviving are multiplied by 2.66. Using 95% confidence intervals: ci &lt;- exp(5*confint(glm_mod_gr)[2,]) ci ## 2.5 % 97.5 % ## 1.944478 3.811167 Therefore, we estimate that for every 5mm increase in carapace length the odds of a juvenile lobster surviving are multiplied by between 1.94 and 3.81. For a percentage-change interpretation we use \\(100 \\times \\left (\\text{exp}(x\\beta_1)‚àí1 \\right )\\): 100*(exp(5*coef(glm_mod_gr)[2]) - 1) ## size ## 166.2564 confint &lt;- 100*(exp(5*confint(glm_mod_gr)[2, ]) - 1) confint ## 2.5 % 97.5 % ## 94.44778 281.11670 We estimate that for every 5mm increase in carapace length the odds of a juvenile lobster surviving increase by 166.26%. We estimate that for every 5mm increase in carapace length the odds of a juvenile lobster surviving increase between 94.45% and 281.12%. Deviance Chi-squared approach Luckily no manual calculations are libraryd, our glm() call returns the values we need directly: ## extract the residual deviance D &lt;- glm_mod_gr$deviance D ## [1] 4.562321 ## extract the residual degrees of freedom (n-k) df &lt;- glm_mod_gr$df.residual df ## [1] 9 Therefore, to test the relevant null hypothesis (that the model is correct) we use 1 - pchisq(D, df) ## [1] 0.8706732 Here, we have no evidence to against our model being &quot;correct&quot;. Simulation based approach Are our chi-squared approximation assumptions met? "],["a-summary-of-glms.html", "A summary of GLMs", " A summary of GLMs The three distributions we've covered are given below. Linear regression: \\(Y_i \\sim \\text{Normal}(\\mu_i, \\sigma^2)\\) where \\(\\mu_i = \\alpha + \\beta_1 x_i\\) Poisson regression: \\(Y_i \\sim \\text{Poisson}(\\mu_i)\\) where \\(\\text{log}(\\mu_i) = \\alpha + \\beta_1 x_i\\) Logistic regression: \\(Y_i \\sim \\text{Binomial}(n_i, p_i)\\) where \\(\\text{logit}(p_i) = \\alpha + \\beta_1 x_i\\) What would happen if we wanted to add extra explanatory terms (e.g., \\(z_i\\))? Then, Linear regression: \\(Y_i \\sim \\text{Normal}(\\mu_i, \\sigma^2)\\) where \\(\\mu_i = \\alpha + \\beta_1 x_i + \\beta_2 z_i\\) Poisson regression: \\(Y_i \\sim \\text{Poisson}(\\mu_i)\\) where \\(\\text{log}(\\mu_i) = \\alpha + \\beta_1 x_i+ \\beta_2 z_i\\) Logistic regression: \\(Y_i \\sim \\text{Binomial}(n_i, p_i)\\) where \\(\\text{logit}(p_i) = \\alpha + \\beta_1 x_i+ \\beta_2 z_i\\) What about interactions (e.g., \\(x_iz_i\\))? then, Linear regression: \\(Y_i \\sim \\text{Normal}(\\mu_i, \\sigma^2)\\) where \\(\\mu_i = \\alpha + \\beta_1 x_i + \\beta_2 z_i + \\beta_3 x_iz_i\\) Poisson regression: \\(Y_i \\sim \\text{Poisson}(\\mu_i)\\) where \\(\\text{log}(\\mu_i) = \\alpha + \\beta_1 x_i+ \\beta_2 z_i + \\beta_3 x_iz_i\\) Logistic regression: \\(Y_i \\sim \\text{Binomial}(n_i, p_i)\\) where \\(\\text{logit}(p_i) = \\alpha + \\beta_1 x_i+ \\beta_2 z_i + \\beta_3 x_iz_i\\) Building a GLM Assume the observations are independent of one another, then, Choose a distribution for the response. For example, Normal, Poisson, or Binomial. Choose a parameter to relate to explanatory terms. For example, \\(\\mu_i\\), \\(\\mu_i\\), or \\(p_i\\). Choose a link function. For example, identity, log, or logit. Choose explanatory terms Estimate additional parameters. For example, \\(\\sigma^2\\). We are not restricted to the three distributions above. Many others exist: Gamma and inverse-Gaussian, for continuous responses on the interval \\([0,\\infty)\\) Beta, for continuous responses on the interval \\([0,1]\\) Negative binomial, for discrete responses on \\((0,1,2,\\cdots)\\), with \\(\\text{Var}(Y) \\geq E(Y)\\) ... Other modelling approaches (not examinable) R function Use gam() Fit a generalised additive model. The R package mgcv must be loaded nlme() Fit linear and non-linear mixed effects models. The R package nlme must be loaded gls() Fit generalised least squares models. The R package nlme must be loaded "],["generalised-linear-mixed-effects-models-glmmms.html", "Generalised linear mixed-effects models (GLMMMs)", " Generalised linear mixed-effects models (GLMMMs) Recall module 3 when we covered fitting linear models with random effects (lmer). The fixed effects and random effects were specified via the model formula. Now we've covered GLMs we can include random effects here too and fit GLMMs! Recall Fixed effects: terms (parameters) in a statistical model which are fixed, or non-random, quantities (e.g., treatment group's mean response). For the same treatment, we expect this quantity to be the same from experiment to experiment. Random effects: terms (parameters) in a statistical model which are considered as random quantities or variables (e.g., block id). Specifically, terms whose levels are a representative sample from a population, and where the variance of the population is of interest should be allocated as random. Setting a block as a random effect allows us to infer variation between blocks as well as the variation between experimental units within blocks. Fitting a GLMM The authors of this paper transplanted gut microbiota from human donors with Autism Spectrum Disorder (ASD) or typically developing (TD) controls into germ-free mice. Faecal samples were collected from three TD and five ASD donors and were used to colonise GF male and female mice from strain C57BL/6LJ. Individuals colonized by the same donor were allowed to breed. Adult offspring mice were behavior tested; tests included marble burying (MB), open-field testing (OFT), and ultrasonic vocalization (USV). library(tidyverse) data &lt;- read_csv(&quot;https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/autism.csv&quot;) data ## # A tibble: 206 √ó 3 ## Donor Treatment MB_buried ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 C1 NT Female 3 ## 2 C1 NT Female 8 ## 3 C1 NT Female 1 ## 4 C1 NT Female 3 ## 5 C1 NT Male 2 ## 6 C1 NT Male 6 ## 7 C1 NT Male 4 ## 8 C1 NT Male 2 ## 9 C1 NT Female 9 ## 10 A24-new ASD Male 7 ## # ‚Ä¶ with 196 more rows We should separate out the treatment values: mice &lt;- data %&gt;% separate(., col = Treatment, into = c(&quot;Diagnosis&quot;, &quot;Sex&quot;)) mice ## # A tibble: 206 √ó 4 ## Donor Diagnosis Sex MB_buried ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 C1 NT Female 3 ## 2 C1 NT Female 8 ## 3 C1 NT Female 1 ## 4 C1 NT Female 3 ## 5 C1 NT Male 2 ## 6 C1 NT Male 6 ## 7 C1 NT Male 4 ## 8 C1 NT Male 2 ## 9 C1 NT Female 9 ## 10 A24-new ASD Male 7 ## # ‚Ä¶ with 196 more rows Recall, using lmer to fit a LMM lmer &lt;- lmerTest::lmer(MB_buried ~ Sex * Diagnosis + (1|Donor), data = mice) car::Anova(lmer, type = 2) ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: MB_buried ## Chisq Df Pr(&gt;Chisq) ## Sex 3.6329 1 0.056648 . ## Diagnosis 8.6944 1 0.003192 ** ## Sex:Diagnosis 2.9745 1 0.084588 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 But is the constant error variance appropriate? Below we plot the model residuals. Using glmer glmer &lt;- lme4::glmer(MB_buried ~ Sex * Diagnosis + (1|Donor), data = mice, family = poisson(link = &quot;log&quot;)) car::Anova(glmer, type = 2) ## Analysis of Deviance Table (Type II Wald chisquare tests) ## ## Response: MB_buried ## Chisq Df Pr(&gt;Chisq) ## Sex 9.9452 1 0.0016127 ** ## Diagnosis 10.5836 1 0.0011410 ** ## Sex:Diagnosis 12.5904 1 0.0003877 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],["module-6.html", "Module 6", " Module 6 Learning objectives Explain the aims and motivation behind cluster analysis and its relevance in biology Write R code to carry out hierarchical and k-means cluster analysis Interpret R output from hierarchical and k-means cluster analysis Interpret and communicate, to both a statistical and non-statistical audience, clustering techniques, specifically, Divisive methods, nonparametric algorithms such as k-means Agglomerative methods, clustering cases and/or variables into a hierarchy of sets (i.e., hierarchical clustering) Explain the aims and motivation behind Principal Component Analysis (PCA) and its relevance in biology Write R code to carry out PCA Interpret and communicate, to both a statistical and non-statistical audience, dimension reduction techniques Other resources Eigenfaces ClusterDucks K-means cluster analysis Little book for Multivariate Analysis 'explor' is an R package to allow interactive exploration of multivariate analysis results The Mathematics Behind Principal Component Analysis (6 min read) "],["clustering.html", "Clustering", " Clustering &quot;Clusters may be described as continuous regions of (a) space containing a relatively high density of points, separated from other such regions by regions containing a relatively low density of points.&quot; --- Everitt, 1980 &quot;Cluster analysis has the apparently simple aim of finding clusters in a data cloud of sampling units in the absence of any a priori information about which point belongs in which cluster. This apparently unambitious aim is unfortunately fraught with problems.&quot; --- Brian McArdle, STATS302 In brief, cluster analysis involves using measures of (dis)similarity and distances to help us define clusters. We use this to uncover hidden or latent clustering by partitioning the data into tighter sets. There are two main methods for doing this: 1) divisive methods use nonparametric algorithms (such as k-means) to split data into a small number of clusters, and 2) agglomerative methods that cluster cases and/or variables into a hierarchy of sets (e.g., hierarchical clustering). We can use to resampling-based bootstrap methods validate clusters. Divisive (partitioning) methods. For a single run of a partitioning method, the number of clusters ( \\(k\\) ) is typically fixed beforehand. Typically, there are only two steps to a partitioning method: an initial allocation (usually rather arbitrary) into \\(k\\) preliminary clusters, and then reallocation of each point either to the closest centroid, or so as to optimise some property of the clusters. This is repeated until there is no further improvement. The initial allocation is usually started by choosing \\(k\\) sampling units to use as seeds to set the clusters. There are a number of ways to choose these seeds. These seeds are used as the initial centres of the clusters, points are allocated to the nearest cluster centre, and in most programs the cluster centroid is adjusted as they are added. K-means K-means clustering involves defining clusters so that the overall variation within a cluster (known as total within-cluster variation) is minimized. How do we define this variation? Typically, using Euclidean distances; the total within-cluster variation, is in this case, is defined as the sum of squared distances Euclidean distances between observations and the corresponding cluster centroid. In summary, the k-means procedure is The number of clusters (k) are specified k objects from the dataset are selected at random and set as the initial cluster centers or means Each observation is assigned to their closest centroid (based on the Euclidean distance between the object and the centroid) For each of the k clusters the cluster centroid is then updated based on calculating the new mean values of all the data points in the cluster Repeat the two previous steps until 1) the cluster assignments stop changing or 2) the maximum number of iterations is reached Identifying the appropriate \\(k\\) is important because too many or too few clusters impedes viewing overall trends. Too many clusters can lead to over-fitting (which limits generalizations) while insufficient clusters limits insights into commonality of groups. There are assorted methodologies to identify the appropriate \\(k\\). Tests range from blunt visual inspections to robust algorithms. The optimal number of clusters is ultimately a subjective decision. K-means: an example using the palmerpenguins data library(palmerpenguins) ## getting rid of NAs penguins_nafree &lt;- penguins %&gt;% drop_na() ## introducing a new package GGally, please install ## using install.packages(&quot;GGally&quot;) library(GGally) penguins_nafree %&gt;% dplyr::select(species, where(is.numeric)) %&gt;% ggpairs(columns = c(&quot;flipper_length_mm&quot;, &quot;body_mass_g&quot;, &quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;)) We see that a lot of these variables (e.g., flipper_length_mm, body_mass_g, and bill_length_mm) are relatively strongly (positively) related to one another. Could they actually be telling us the same information? Combined we could think of these three variables all telling us a little about bigness of penguin. Is there a way we could reduce these three variables, into say 1, to represent the bigness of a penguin. We may not need all the information (variation) captured by these variables, but could get away with fewer new uncorrelated variables that represent basically the same information (e.g., penguin bigness), thereby, reducing the dimensionality of the data (more on this later). ## create a data frame of what we&#39;re interested in df &lt;- penguins_nafree %&gt;% dplyr::select(where(is.numeric), -year) We use the kmeans() function. The first argument of kmeans() should be the dataset you wish to cluster. Below we use data frame df, the penguin data discussed above. But how many clusters do we choose? Let's try 1 to 5... (i.e., using the centers argument). Setting nstart = 25 means that R will try 25 different random starting assignments and then select the best results corresponding to the one with the lowest within cluster variation. ## set the seed so we all start off in the same place set.seed(4321) ## one cluster k1 &lt;- kmeans(df, centers = 1, nstart = 25) ## two clusters k2 &lt;- kmeans(df, centers = 2, nstart = 25) ## three clusters k3 &lt;- kmeans(df, centers = 3, nstart = 25) ## four clusters k4 &lt;- kmeans(df, centers = 4, nstart = 25) ## five clusters k5 &lt;- kmeans(df, centers = 5, nstart = 25) The kmeans() function returns a list of components: cluster, integers indicating the cluster to which each observation is allocated centers, a matrix of cluster centers/means totss, the total sum of squares withinss, within-cluster sum of squares, one component per cluster tot.withinss, total within-cluster sum of squares betweenss, between-cluster sum of squares size, number of observations in each cluster Choosing the number of clusters We have an idea there may be 3 clusters, perhaps, but how do we know this is the best fit? Remember it's a subjective choice and we'll be looking at a few pointers Visual inspection method library(factoextra) ## a new package for kmeans viz, please install p1 &lt;- fviz_cluster(k1, data = df) p2 &lt;- fviz_cluster(k2, data = df) p3 &lt;- fviz_cluster(k3, data = df) p4 &lt;- fviz_cluster(k4, data = df) p5 &lt;- fviz_cluster(k5, data = df) ## for arranging plots library(patchwork) (p1| p2| p3)/ (p4 | p5) Alternatively, you can use standard pairwise scatter plots to illustrate the clusters compared to the original variables. df %&gt;% mutate(cluster = k3$cluster, species = penguins_nafree$species) %&gt;% ggplot(aes(flipper_length_mm, bill_depth_mm, color = factor(cluster), label = species)) + geom_text() Elbow method Optimal clusters are at the point in which the knee &quot;bends&quot; or in mathematical terms when the marginal total within sum of squares (tot.withinss) for an additional cluster begins to decrease at a linear rate This is easier to see via a plot: fviz_nbclust(df, kmeans, method = &quot;wss&quot;) + labs(subtitle = &quot;Elbow method&quot;) There is a pretty obvious inflection (elbow) at 2 clusters, but maybe at 3 too. We can rule out an optimal number of clusters above 3 as there is then only a minimal marginal reduction in total within sum of squares. However, the model is ambiguous on whether 2 or 3 clusters is optimal... Silhouette method # Silhouette method fviz_nbclust(df, kmeans, method = &quot;silhouette&quot;)+ labs(subtitle = &quot;Silhouette method&quot;) Gap method # Gap statistic # recommended value: nboot = 500 for your analysis (it will take a while) set.seed(123) ## remove this fviz_nbclust(df, kmeans, nstart = 25, method = &quot;gap_stat&quot;, nboot = 50)+ labs(subtitle = &quot;Gap statistic method&quot;) Basically it's up to you to collate all the suggestions and make and informed decision ## Trying all the cluster indecies AHHHHH library(NbClust) cluster_30_indexes &lt;- NbClust(data = df, distance = &quot;euclidean&quot;, min.nc = 2, max.nc = 9, method = &quot;complete&quot;, index =&quot;all&quot;) ## *** : The Hubert index is a graphical method of determining the number of clusters. ## In the plot of Hubert index, we seek a significant knee that corresponds to a ## significant increase of the value of the measure i.e the significant peak in Hubert ## index second differences plot. ## ## *** : The D index is a graphical method of determining the number of clusters. ## In the plot of D index, we seek a significant knee (the significant peak in Dindex ## second differences plot) that corresponds to a significant increase of the value of ## the measure. ## ## ******************************************************************* ## * Among all indices: ## * 5 proposed 2 as the best number of clusters ## * 6 proposed 3 as the best number of clusters ## * 1 proposed 4 as the best number of clusters ## * 4 proposed 5 as the best number of clusters ## * 1 proposed 8 as the best number of clusters ## * 3 proposed 9 as the best number of clusters ## ## ***** Conclusion ***** ## ## * According to the majority rule, the best number of clusters is 3 ## ## ## ******************************************************************* fviz_nbclust(cluster_30_indexes) + theme_minimal() + labs(title = &quot;Frequency of Optimal Clusters using 30 indexes in NbClust Package&quot;) ## Among all indices: ## =================== ## * 2 proposed 0 as the best number of clusters ## * 1 proposed 1 as the best number of clusters ## * 5 proposed 2 as the best number of clusters ## * 6 proposed 3 as the best number of clusters ## * 1 proposed 4 as the best number of clusters ## * 4 proposed 5 as the best number of clusters ## * 1 proposed 8 as the best number of clusters ## * 3 proposed 9 as the best number of clusters ## * 3 proposed NA&#39;s as the best number of clusters ## ## Conclusion ## ========================= ## * According to the majority rule, the best number of clusters is 3 . Not obvious, basically still undecided between 2 and 3, but according to the absolute majority rule the &quot;best&quot; number is 3 Hierarchical agglomerative clustering. Most of the hierarchical methods are agglomerative and they operate in the same way: All sampling units that are zero distance (however this is defined) apart are fused into clusters. The threshold for fusion is then raised from zero until two clusters (they may be individual points) are found that are close enough to fuse. The threshold is raised, fusing the clusters as their distance apart is reached until all the clusters have been fused into one big one. So, the close clusters are fused first, then those further apart, till all have been fused. This process allows the history of the fusions, the hierarchy, to be displayed as a dendrogram. This is an advantage of the agglomerative methods, if the data have a nested structure these techniques lead to a useful way of displaying it. Unlike the k-means most of the agglomerative techniques can use a broad range of similarity or distance measures. This, however, means that considerable care must be taken to choose the appropriate one; different measures often lead to different results. Single linkage (nearest neighbour) clustering. Single Linkage (nearest neighbour/minimal jump): Computes the distance between clusters as the smallest distance between any two points in the two clusters. Single Linkage identifies clusters based on how far apart they are at their closest points. This means that if there are any intermediate points then single linkage will fuse the groups without leaving any trace of their separate identities. This is called chaining and it often leads to uninformative dendrograms. If, however, the clusters are well separated in the data, then single linkage can handle groups of different shapes and sizes easily. In addition, single linkage will give the same clustering after any monotonic transformation of the distance measure (i.e., it is fairly robust to the choice of measure). Complete linkage (farthest neighbour) clustering. Instead of measuring the distance between two clusters as that between their two nearest members complete Linkage (maximum jump) uses that between the two farthest members (i.e., it calculates the maximum distance between two points from each cluster.) The resulting clusters are often compact, spherical and well defined. Complete linkage can, however, be sensitive to tied distances. Although, it too, is robust to a certain amount of measurement error and choice of distance. Group average linkage (UPGMA) Group average linkage (UPGMA) is probably the most popular hierarchical clustering method! You might like to think of it as an attempt to avoid the extremes of the single and complete linkage methods as the distance between two clusters is the average of the distances between the members of the two groups. As a result this method tends to produce compact spherical clusters. Ward's method (incremental sums of squares, minimum variance, agglomerative sums of squares). Ward's method is the hierarchical version of the k-means partitioning method. At each fusion it attempts to minimise the increase in total sum of squared distances within the clusters. This is equivalent to minimising the sum of squared within cluster deviations from the centroids Ward's method, at any one stage, can only fuse those clusters already in existence (i.e., it is not allowed to reallocate points). A bad start to the agglomeration process can place the algorithm on a path from which it can never reach the global optimum for a given number of clusters. Its chief flaw is a tendency to form clusters of equal size, regardless of the true number. Like the complete linkage and group average methods it is also biased towards forming spherical clusters. Despite this, Ward's method performs well in simulations and is often method of choice. Hierarchical clustering: an example In summary Start with a matrix of distances, (or similarities) between pairs of observations (cases) Choice of distance measure key first step Algorithm: Initial \\(n\\) singleton clusters Scan distance matrix for two closest individuals, group them together Compute distance from cluster of size 2 to remaining \\(n-1\\) singleton clusters Method Pros Cons Single linkage number of clusters comb-like trees. Complete linkage compact clusters one obs. can alter groups Average linkage similar size and variance not robust Centroid robust to outliers smaller number of clusters Ward minimising an inertia clusters small if high variability Ants Data were collected on the distribution of ant species at 30 sites across the Auckland region using pitfall traps. Twenty pitfall traps at each site were left open for ten days and the number of individuals captured counted for the four most abundant species: Nylanderia spp, Pheidole rugosula, Tetramorium grassii, and Pachycondyla sp. library(tidyverse) ants &lt;- read_csv(&quot;https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/pitfalls.csv&quot;) ants ## # A tibble: 30 √ó 8 ## Location Habitat Month Site Nyl Phe Tet Pac ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 West Forest 1 WF1 0 0 0 157 ## 2 West Grass 1 WG1 0 2 7 37 ## 3 West Urban 1 WU1 3 7 0 0 ## 4 West Forest 2 WF2 0 0 0 31 ## 5 West Grass 2 WG2 5 0 25 0 ## 6 West Forest 3 WF3 0 0 0 21 ## 7 West Grass 3 WG3 0 3 2 1 ## 8 West Urban 3 WU3 0 1 0 0 ## 9 Central Forest 1 CF1 0 0 0 1 ## 10 Central Grass 1 CG1 0 3 22 2 ## # ‚Ä¶ with 20 more rows Data are species counts, so we will use Bray Curtis measure: pitfall.dist &lt;- vegan::vegdist(ants[,5:8], method = &quot;bray&quot;, binary = FALSE) factoextra::fviz_dist(pitfall.dist) Single-linkage single &lt;- ants[,5:8] %&gt;% vegan::vegdist(., method = &quot;bray&quot;, binary = FALSE) %&gt;% hclust(method = &quot;single&quot;) plot(single, labels = ants$Site) Maximum linkage complete &lt;- ants[,5:8] %&gt;% vegan::vegdist(., method = &quot;bray&quot;, binary = FALSE) %&gt;% hclust(method = &quot;complete&quot;) plot(complete, labels = ants$Site) Average linkage (UPGMA) average &lt;- ants[,5:8] %&gt;% vegan::vegdist(., method = &quot;bray&quot;, binary = FALSE) %&gt;% hclust(method = &quot;average&quot;) plot(average, labels = ants$Site) Ward‚Äôs method ward &lt;- ants[,5:8] %&gt;% vegan::vegdist(., method = &quot;bray&quot;, binary = FALSE) %&gt;% hclust(method = &quot;ward.D&quot;) plot(ward, labels = ants$Site) WHAT ARE DENDROGRAMS GOOD FOR? Suggesting clusters for further study... Using the function cutree() to split into clusters and plot: ants$clust4 &lt;- cutree(ward, k = 4) library(ape) ## install pitfall.phylo &lt;- as.phylo(ward) pitfall.phylo$tip.label &lt;- ants$Site ## Set colours colours &lt;- c(&quot;red&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;black&quot;) plot(pitfall.phylo, cex = 0.6, tip.color = colours[ants$clust4], label.offset = 0.05) "],["principal-component-analysis-pca.html", "Principal Component Analysis (PCA)", " Principal Component Analysis (PCA) Reduction of dimensions is needed when there are far too many features in a dataset, making it hard to distinguish between the important ones that are relevant to the output and the redundant or not-so important ones. Reducing the dimensions of data is called dimensionality reduction. So the aim is to find the best low-dimensional representation of the variation in a multivariate (lots and lots of variables) data set, but how do we do this? PCA is a member of a family of techniques for dimension reduction (ordination). The word ordination was applied to dimension reduction techniques by botanical ecologists whose aim was to identify gradients in species composition in the field. For this reason they wanted to reduce the quadrat √ó species (observations √ó variables) data matrix to a single ordering (hence ordination) of the quadrats which they hoped would reflect the underlying ecological gradient. One way is termed Principal Component Analysis (PCA). PCA is a feature extraction method that reduces the dimensionality of the data (number of variables) by creating new uncorrelated variables while minimizing loss of information on the original variables. It is a technique for the analysis of an unstructured sample of multivariate data. Its aim is to display the relative positions of the observations in the data cloud in fewer dimensions (while losing as little information as possible) and to help give insight into the way the observations vary. It is not a hypothesis testing technique (like t-test or Analysis of Variance); it is an exploratory, hypothesis generating tool that describes patterns of variation, and suggests relationships that should be investigated further. Think of a baguette. The baguette pictured here represents two data dimensions: 1) the length of the bread and 2) the height of the bread (we'll ignore depth of bread for now). Think of the baguette as your data; when we carry out PCA we're rotating our original axes (x- and y-coordinates) to capture as much of the variation in our data as possible. This results in new uncorrelated variables that each explain a % of variation in our data; the procedure is designed so that the first new variable (PC1) explains the most, the second (PC2) the second most and so on. Now rather than a baguette think of data; the baguette above represent the shape of the scatter between the two variables plotted below. The rotating grey axes represent the PCA procedure, essentially searching for the best rotation of the original axes to represent the variation in the data as best it can. Mathematically the Euclidean distance (e.g., the distance between points \\(p\\) and \\(q\\) in Euclidean space, \\(\\sqrt{(p-q)^2}\\)) between the points and the rotating axes is being minimized (i.e., the shortest possible across all points), see the blue lines. Once this distance is minimized across all points we &quot;settle&quot; on our new axes (the black tiled axes). Luckily we can do this all in R! One problem with PCA is that the components are not scale invariant. That means if we change the units in which our variables are expressed, we change the components; and not in any simple way either. So, every scaling or adjustment of the variables in preparation for the analysis could (and usually does ) produce a separate component structure. It is therefore important to choose a standardisation or transformation carefully: PCA will give different results depending on whether we analyse the covariance matrix, where the data have merely been centred (corrected for the column, variable, mean), or the correlation matrix, where the data have been standardised to z -scores (centred and converted into standard deviation units). This is particularly important, as many computer programs to do PCA automatically analyse the correlation matrix . If you do not want that standardisation; you may have to explicitly ask for the covariance matrix. As you would expect, the results from the two analyses will usually be very different, see below. Examples in R The palmerpenguins data ## getting rid of NAs penguins_nafree &lt;- penguins %&gt;% drop_na() When carrying out PCA we're only interested in numeric variables, so let's just plot those. We can use the piping operator %&gt;% to do this with out creating a new data frame library(GGally) penguins_nafree %&gt;% dplyr::select(species, where(is.numeric)) %&gt;% ggpairs(aes(color = species), columns = c(&quot;flipper_length_mm&quot;, &quot;body_mass_g&quot;, &quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;)) Using prcomp() There are three basic types of information we obtain from Principal Component Analysis: PC scores: the coordinates of our samples on the new PC axis: the new uncorrelated variables (stored in pca$x) Eigenvalues: (see above) represent the variance explained by each PC; we can use these to calculate the proportion of variance in the original data that each axis explains Variable loadings (eigenvectors): these reflect the weight that each variable has on a particular PC and can be thought of as the correlation between the PC and the original variable Before we carry out PCA we should scale out data. WHY? pca &lt;- penguins_nafree %&gt;% dplyr::select(where(is.numeric), -year) %&gt;% ## year makes no sense here so we remove it and keep the other numeric variables scale() %&gt;% ## scale the variables prcomp() ## print out a summary summary(pca) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 1.6569 0.8821 0.60716 0.32846 ## Proportion of Variance 0.6863 0.1945 0.09216 0.02697 ## Cumulative Proportion 0.6863 0.8809 0.97303 1.00000 This output tells us that we obtain 4 principal components, which are called PC1 PC2, PC3, and PC4 (this is as expected because we used the 4 original numeric variables!). Each of these PCs explains a percentage of the total variation (Proportion of Variance) in the dataset: PC1 explains \\(\\sim\\) 68% of the total variance, which means that just over half of the information in the dataset (5 variables) can be encapsulated by just that one Principal Component. PC2 explains \\(\\sim\\) 19% of the variance. PC3 explains \\(\\sim\\) 9% of the variance. PC4 explains \\(\\sim\\) 2% of the variance. From the Cumulative Proportion row we see that by knowing the position of a sample in relation to just PC1 and PC2 we can get a pretty accurate view on where it stands in relation to other samples, as just PC1 and PC2 explain 88% of the variance. The loadings (relationship) between the initial variables and the principal components are stored in pca$rotation: pca$rotation ## PC1 PC2 PC3 PC4 ## bill_length_mm 0.4537532 -0.60019490 -0.6424951 0.1451695 ## bill_depth_mm -0.3990472 -0.79616951 0.4258004 -0.1599044 ## flipper_length_mm 0.5768250 -0.00578817 0.2360952 -0.7819837 ## body_mass_g 0.5496747 -0.07646366 0.5917374 0.5846861 Here we can see that bill_length_mm has a strong positive relationship with PC1, whereas bill_depth_mm has a strong negative relationship. Both fliper_length_mm and body_mass_g also have a strong positive relationship with PC1. Plotting this we get The new variables (PCs) are stored in pca$x, lets plot some of them alongside the loadings using a biplot. For PC1 vs PC2: library(factoextra) ## install this package first fviz_pca_biplot(pca, geom = &quot;point&quot;) + geom_point (alpha = 0.2) Now for PC2 vs PC3 fviz_pca_biplot(pca, axes = c(2,3),geom = &quot;point&quot;) + geom_point (alpha = 0.2) But how many PCs (new variables) do we keep? The whole point of this exercise is to reduce the number of variables we need to explain the variation in our data. So how many of these new variables (PCs) do we keep? To assess this we can use the information printed above alongside a screeplot: fviz_screeplot(pca) Principal components from the original variables Recall that the principal components are a linear combination of the (statndardised) variables. So for PC1 loadings1 &lt;- pca$rotation[,1] loadings1 ## bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## 0.4537532 -0.3990472 0.5768250 0.5496747 Therefore, the first Principle Component will be \\(0.454\\times Z1 -0.399 \\times Z2 + 0.5768 \\times Z3 + 0.5497 \\times Z3\\) where \\(Z1\\), \\(Z2\\), \\(Z3\\). and \\(Z4\\) are the scaled numerical variables form the penguins dataset (i.e., bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g). To compute this we use R: scaled_vars &lt;- penguins_nafree %&gt;% dplyr::select(where(is.numeric), -year) %&gt;% scale() %&gt;% as_tibble() ## By &quot;Hand&quot; by_hand &lt;- loadings1[1]*scaled_vars$&quot;bill_length_mm&quot; + loadings1[2]*scaled_vars$&quot;bill_depth_mm&quot; + loadings1[3]*scaled_vars$&quot;flipper_length_mm&quot; + loadings1[4]*scaled_vars$&quot;body_mass_g&quot; ## From PCA pc1 &lt;- pca$x[,1] plot(by_hand,pc1) Reality check: reducing white noise... set.seed(1234) ## just for reproduciblity noise &lt;- as_tibble(replicate(10,rnorm(200, mean = 50, sd = 10))) noise ## # A tibble: 200 √ó 10 ## V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 37.9 54.9 37.7 53.1 39.8 37.9 36.0 29.9 47.5 44.7 ## 2 52.8 57.0 50.4 56.1 36.1 53.0 73.8 34.4 37.8 45.0 ## 3 60.8 51.9 45.8 33.1 49.5 34.6 58.7 69.6 49.3 38.5 ## 4 26.5 57.0 41.0 57.8 68.1 56.4 34.6 48.2 63.6 51.3 ## 5 54.3 53.1 54.2 50.1 49.0 57.0 61.3 63.6 45.4 54.7 ## 6 55.1 57.6 51.5 48.2 57.8 30.9 60.4 66.5 50.7 48.1 ## 7 44.3 68.4 64.6 61.1 39.0 59.4 55.9 51.2 57.8 33.8 ## 8 44.5 61.1 38.8 64.8 47.8 47.8 54.0 50.0 34.1 46.0 ## 9 44.4 50.3 44.8 38.5 55.7 43.3 51.4 52.0 36.3 54.1 ## 10 41.1 38.9 49.3 60.1 46.5 54.5 56.2 56.8 40.7 42.8 ## # ‚Ä¶ with 190 more rows corrplot::corrplot(cor(noise), method = &quot;ellipse&quot;,type = &quot;upper&quot;) pca &lt;- noise %&gt;% scale() %&gt;% prcomp() summary(pca) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 1.1890 1.1331 1.0936 1.0281 1.0075 0.96654 0.94742 ## Proportion of Variance 0.1414 0.1284 0.1196 0.1057 0.1015 0.09342 0.08976 ## Cumulative Proportion 0.1414 0.2698 0.3894 0.4951 0.5966 0.68998 0.77974 ## PC8 PC9 PC10 ## Standard deviation 0.90017 0.85665 0.81141 ## Proportion of Variance 0.08103 0.07339 0.06584 ## Cumulative Proportion 0.86078 0.93416 1.00000 fviz_screeplot(pca, choice = &quot;eigenvalue&quot;) + geom_hline(yintercept = 1) What does this show? "],["multidimensional-scaling-mds.html", "Multidimensional Scaling (MDS)", " Multidimensional Scaling (MDS) Multidimensional scaling (MDS) is an extended family of techniques that try to reproduce the relative positions of a set of points in a reduced space given, not the points themselves, but only a matrix with interpoint distances ( dissimilarities ). These distances might be measured with error, or even be non-Euclidean. Metric Scaling Metric scaling tries to produce a set of coordinates (a configuration of points) in a reduced number of dimensions whose matrix of interpoint Euclidean distances approximates the original dissimilarity matrix as closely as possible. Principal coordinates (PCO) is one metric scaling technique (it is sometimes called classical or Torgerson scaling ). Examples in R Consider data which are not represented as points in a feature space: Where we are only provided with (dis)similarity matrices between objects (e.g., chemical compounds, images, trees, or other complex objects) Where there are no obvious coordinates in (continuous) n-dimensional space . Distances (in km) between North Island cities library(tidyverse) ni &lt;- read_csv(&quot;https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/north_islands_distances.csv&quot;) ni &lt;- ni %&gt;% column_to_rownames(var = &quot;...1&quot;) library(pheatmap) pheatmap(ni, cluster_rows = TRUE, treeheight_row = 2, treeheight_col = 2, fontsize_row = 12, fontsize_col = 12, cellwidth = 26, cellheight = 26) mds &lt;- cmdscale(ni, eig = TRUE) mds ## $points ## [,1] [,2] ## Auckland 259.23245 67.43013 ## Gisborne -107.54173 -285.70950 ## Hamilton 129.07943 42.71295 ## Hastings -173.12950 -25.15974 ## Napier -150.83765 -34.70680 ## Rotorua 37.39858 -18.39760 ## Tauranga 118.78535 -85.88683 ## Whanganui -192.73988 181.50600 ## Wellington -385.83172 167.76477 ## Whakatane 49.93256 -140.17112 ## Whangarei 415.65212 130.61774 ## ## $eig ## [1] 5.249373e+05 1.953521e+05 4.217767e+04 1.872276e+04 1.222717e+03 ## [6] 2.910383e-11 -1.399691e+02 -4.733140e+02 -1.103819e+04 -1.883151e+04 ## [11] -2.462990e+04 ## ## $x ## NULL ## ## $ac ## [1] 0 ## ## $GOF ## [1] 0.8600209 0.9206005 Eckmans colour perception (1954) Data may from objects for which we have similarities but no underlying (geometric) space. Here the goal is to understand the underlying dimensionality of colour perception. Similarities for 14 colours, with wavelengths from 434 to 674nm based on rating by 31 subjects Each pair of colours was rated on a 5-point scale: 0 = no similarity up to 4 = identical. After averaging over 31 raters the similarities were divided by 4 such that they are within the unit interval. library(tidyverse) ekman &lt;- read_csv(&quot;https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/ekman.csv&quot;) ekman.mds &lt;- cmdscale(ekman, eig = TRUE) ekman.mds round(ekman.mds$eig,2) autoplot(ekman.mds) Distances (in km) between 21 cities in Europe library(ggfortify) ## Plotting Multidimensional Scaling (for interest) ## stats::cmdscale performs Classical MDS data(&quot;eurodist&quot;) ## road distances (in km) between 21 cities in Europe. autoplot(eurodist) ## Plotting Classical (Metric) Multidimensional Scaling autoplot(cmdscale(eurodist, eig = TRUE)) autoplot(cmdscale(eurodist, eig = TRUE), label = TRUE, shape = FALSE, label.size = 3) ## Plotting Non-metric Multidimensional Scaling ## MASS::isoMDS and MASS::sammon perform Non-metric MDS library(MASS) autoplot(sammon(eurodist)) autoplot(sammon(eurodist), shape = FALSE, label = TRUE,label.size = 3) ## Have a go at interpreting these plots based on the geography of the cities :-) Correspondence Analysis (CA) CA is a special case of metric MDS where the distance measure is the chi-square distance. It is conceptually similar to principal component analysis but where the data are categorical, counts, rather than continuous. CA is traditionally applied to contingency tables where rows and columns are treated equivalently; it decomposes the chi-square statistic associated with this table into orthogonal factors. Correspondence analysis is usually the best way to follow up on a significant chi-square test. HairEyeColor ## , , Sex = Male ## ## Eye ## Hair Brown Blue Hazel Green ## Black 32 11 10 3 ## Brown 53 50 25 15 ## Red 10 10 7 7 ## Blond 3 30 5 8 ## ## , , Sex = Female ## ## Eye ## Hair Brown Blue Hazel Green ## Black 36 9 5 2 ## Brown 66 34 29 14 ## Red 16 7 7 7 ## Blond 4 64 5 8 HC.df &lt;- as.data.frame.matrix(HairEyeColor[ , , 2]) HC.df ## Brown Blue Hazel Green ## Black 36 9 5 2 ## Brown 66 34 29 14 ## Red 16 7 7 7 ## Blond 4 64 5 8 chisq.test(HC.df) ## ## Pearson&#39;s Chi-squared test ## ## data: HC.df ## X-squared = 106.66, df = 9, p-value &lt; 2.2e-16 library(ade4) coaHC &lt;- dudi.coa(HC.df, scannf = FALSE, nf = 2) The first axis shows a contrast between black haired and blonde haired students, mirrored by the brown eye, blue eye contrast. In CA the two categories, rows and columns play symmetric roles and we interpret the proximity of Blue eyes and Blond hair as showing strong co-occurence of these categories. Biplot barycentric scaling Row points at the centre of gravity of the column levels with their respective weights Blue eyes at centre of gravity of the (Black, Brown, Red, Blond) with weights proportional to (9,34,7,64), the hair counts for blue eyes. The Blond row point is very heavily weighted so Blond hair and Blue eyes close together "],["non-metric-multidimensional-scaling.html", "Non-metric Multidimensional Scaling", " Non-metric Multidimensional Scaling Multidimensional scaling aims to minimize the difference between the squared distances \\(D^2\\) from the distance matrix \\(D\\), and the squared distances between the points with their new coordinates. Unfortunately, this objective tends to be sensitive to outliers: one single data point with large distances to everyone else can dominate, and thus skew, the whole analysis. Under certain circumstances trying to preserve the actual dissimilarities might be too restrictive or even pointless. For example if there is large error in the dissimilarity estimates, if the dissimilarities or the data they were based on were ranks (ordinal), then the magnitude of the distances are too crude to be worth preserving. A method that preserved only the rank order of the dissimilarities would be more appropriate. The algorithm to do this is virtually the same as the one given above for metric scaling. The sole difference is that the linear regression that fitted the estimated distances for the solution to the dissimilarities is now replaced with an order preserving regression. Robust ordination, or non-metric multidimensional scaling (NMDS), attempts to embed the points in a new space such that the order of the reconstructed distances in the new map is the same as the ordering of the original distance matrix. NMDS looks for a transformation f() of the given dissimilarities, distances d. The quality of the approximation can be measured by the standardized residual sum of squares (STRESS) function: \\(\\text{Stress}^2 = \\frac{\\Sigma(f(d) - \\tilde{d})^2}{\\Sigma d^2}\\) where \\(f(d)\\approx \\tilde{d}\\). NMDS is not sequential: we have to specify the underlying dimensionality k at the outset (like kmeans) optimization is run to maximize the reconstruction of the distances in k dimensions. there is no notion of percentage of variation explained by individual axes as provided in PCA. as for kmeans Make a screeplot for \\(k = 1,2,3,...\\) and looking at how well the STRESS drops. because each calculation of a NMDS result librarys a new optimization that is both random and dependent on the value of k, we repeat the process M times Examples in R Use the function metaMDS from the vegan package; metaMDS performs NMDS, and tries to find a stable solution using several random starts. In addition, it standardizes the scaling in the result, so that the configurations are easier to interpret. Illustration with k = 2 library(vegan) nMDS.2 &lt;- replicate(100, metaMDS(ekman, k = 2, autotransform = FALSE)) stressplot(metaMDS(ekman, k = 2, autotransform = FALSE), pch = 20, cex = 2) ## Run 0 stress 0.2705898 ## Run 1 stress 0.2631587 ## ... New best solution ## ... Procrustes: rmse 0.2122761 max resid 0.4405217 ## Run 2 stress 0.282509 ## Run 3 stress 0.2860458 ## Run 4 stress 0.292585 ## Run 5 stress 0.2724171 ## Run 6 stress 0.2808561 ## Run 7 stress 0.2658076 ## Run 8 stress 0.2624427 ## ... New best solution ## ... Procrustes: rmse 0.1488645 max resid 0.3136837 ## Run 9 stress 0.2638454 ## Run 10 stress 0.2782765 ## Run 11 stress 0.2700731 ## Run 12 stress 0.2733299 ## Run 13 stress 0.2745804 ## Run 14 stress 0.2626338 ## ... Procrustes: rmse 0.152867 max resid 0.3162977 ## Run 15 stress 0.2821937 ## Run 16 stress 0.2731302 ## Run 17 stress 0.3273443 ## Run 18 stress 0.2730656 ## Run 19 stress 0.2781293 ## Run 20 stress 0.2761824 ## *** No convergence -- monoMDS stopping criteria: ## 19: stress ratio &gt; sratmax ## 1: scale factor of the gradient &lt; sfgrmin "],["linear-discriminant-analysis-lda.html", "Linear Discriminant Analysis (LDA)", " Linear Discriminant Analysis (LDA) LDA is a supervised learning technique: The main goal is to predict some feature of interest using sing one or more variables (the predictors) Example in R library(tidyverse) diabetes &lt;- read_csv(&quot;https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/diabetes.csv&quot;) The data diabetes$group &lt;- factor(diabetes$group) diabetes ## # A tibble: 144 √ó 7 ## id relwt glufast glutest steady insulin group ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1 0.81 80 356 124 55 3 ## 2 3 0.94 105 319 143 105 3 ## 3 5 1 90 323 240 143 3 ## 4 7 0.91 100 350 221 119 3 ## 5 9 0.99 97 379 142 98 3 ## 6 11 0.9 91 353 221 53 3 ## 7 13 0.96 78 290 136 142 3 ## 8 15 0.74 86 312 208 68 3 ## 9 17 1.1 90 364 152 76 3 ## 10 19 0.83 85 296 116 60 3 ## # ‚Ä¶ with 134 more rows Some variables can predict group of a patient ggplot(reshape2::melt(diabetes, id.vars = c(&quot;id&quot;, &quot;group&quot;)), aes(x = value, col = group)) + geom_density() + facet_wrap( ~variable, ncol = 1, scales = &quot;free&quot;) + theme(legend.position = &quot;bottom&quot;) Possible classification rules? ggplot(diabetes, mapping = aes(x = insulin, y = glutest)) + theme_bw() + geom_point(aes(colour = group), size = 3) + labs( x = &quot;insulin&quot; , y = &quot;glutest&quot;) + theme(axis.title = element_text( size = 16), axis.text = element_text(size = 12)) Carrying out LDA Some similarity to regression library(MASS) diabetes_lda &lt;- lda(group ~ insulin + glutest, data = diabetes) diabetes_lda ## Call: ## lda(group ~ insulin + glutest, data = diabetes) ## ## Prior probabilities of groups: ## 1 2 3 ## 0.2222222 0.2500000 0.5277778 ## ## Group means: ## insulin glutest ## 1 320.9375 1027.3750 ## 2 208.9722 493.9444 ## 3 114.0000 349.9737 ## ## Coefficients of linear discriminants: ## LD1 LD2 ## insulin -0.004463900 -0.01591192 ## glutest -0.005784238 0.00480830 ## ## Proportion of trace: ## LD1 LD2 ## 0.9677 0.0323 Components of diabetes_lda diabetes_lda$prior gives the prior probabilities of belonging to each group. By default these reflect the proportions of membership in the data: prop.table(table(diabetes$group)) ## ## 1 2 3 ## 0.2222222 0.2500000 0.5277778 --&gt; randomly chosen subject has probability 0.52 of coming from group 3 diabetes_lda$mean gives the means of each predictor in each group: Proportion of Trace gives the percentage separation achieved by each discriminant function diabetes_lda$scaling contains the linear discriminant functions (i.e., the linear combination of variables giving best separation between groups): diabetes_lda$scaling ## LD1 LD2 ## insulin -0.004463900 -0.01591192 ## glutest -0.005784238 0.00480830 i.e., LD1: \\(-0.00446 \\times \\text{insulin} - 0.00578 \\times \\text{glutest}\\) LD2: \\(-0.01591 \\times \\text{insulin} + 0.00481 \\times \\text{glutest}\\) How well does LDA do on training data? ghat &lt;- predict(diabetes_lda)$class table(prediced = ghat, observed = diabetes$group) ## observed ## prediced 1 2 3 ## 1 25 0 0 ## 2 6 24 6 ## 3 1 12 70 The missclassification rate is therefore mean(ghat != diabetes$group) ## [1] 0.1736111 Prediction diabetes.pred &lt;- predict(diabetes_lda) str(diabetes.pred) ## List of 3 ## $ class : Factor w/ 3 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 3 3 3 3 3 3 3 3 3 3 ... ## $ posterior: num [1:144, 1:3] 0.00000104 0.00000106 0.00000247 0.00000326 0.00000477 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:144] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. ..$ : chr [1:3] &quot;1&quot; &quot;2&quot; &quot;3&quot; ## $ x : num [1:144, 1:2] 1.62 1.61 1.42 1.37 1.29 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:144] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. ..$ : chr [1:2] &quot;LD1&quot; &quot;LD2&quot; $class: predicted group for each observation $posterior: probability of falling into each group $x: matrix with 2 columns one for each LD score Output Every possible point is classified to one of three groups The divisions between groups are linear (Linear Discriminant Analysis) The three ellipses represent the class centres and the covariance matrix of the LDA model. Note there is only one covariance matrix, which is the same for all three classes. This results in the sizes and orientations of the ellipses being the same for the three classes (only their centres differ) the ellipses represent contours of equal class membership probability. A key assumption of LDA is that the correlations between variables are the same in each group (i.e., common covariance matrix). Recall that, by default, the prior probabilities are the initial proportions. What if we set equal prior probabilities? The confusion matrix/missclassification rate: equal.ghat &lt;- predict(diabetes_lda, prior = rep(1,3)/3)$class table(predicted = equal.ghat,observed = diabetes$group) ## observed ## predicted 1 2 3 ## 1 25 0 0 ## 2 7 28 9 ## 3 0 8 67 ## missclassification rate mean(equal.ghat != diabetes$group) ## [1] 0.1666667 There are now 8 cases classified as Group 3 with prior weights classified as Group 2 with equal weights \\(\\rightarrow\\) bias towards group with larger initial size. "]]
