[["index.html", "bluebook BIOSCI738 Preface 0.1 Course Overview 0.2 Key Topics", " bluebook BIOSCI738 Charlotte Jones-Todd Semester 1, 2021, University of Auckland Preface Artwork by @allison_horst 0.1 Course Overview This is a postgraduate course in statistical methods geared towards the needs of students of biology, ecology, and environmental science. Whether heading to research or industry, it is imperative that biology students have the statistical and computational skills to apply and interpret fundamental statistical concepts and analyses to assess and critique their experiments and other data. This course is suited to students with an interest in (bio)statistics who would like to equip themselves with the tools and know-how to be able to correctly prepare experiments, analyse data, interpret their results and draw valid conclusions. The statistical concepts and methods taught in this course will provide students with the tools to make and evaluate scientific discoveries as well as propose and justify decisions based on data. Some basic statistical knowledg is assumed. This course will use the programming language R; R is a free software environment for statistical computing and graphics. Students are strongly encouraged to use R through the freely available IDE (integrated development environment) RStudio. Students will have access to R and RStudio in university computing laboratories, but are also encouraged to download and install R and RStudio on their own devices. 0.2 Key Topics Taught material will be delivered, each week, via 2 hour lectures. Each week there will also be a 1 hour practical workshop focused on the material covered in the lecture. A list of topics and concepts covert in this course is given below. Exploratory Data Analysis and Communication Data wrangling Data visualisation Experimental Design and Statistical Inference Introduction to design and analysis of experiments Comparison procedures: pairwise comparisons of means, one-way ANOVA Multiple comparison procedures (controlling errors in hypothesis testing) Multiple regression with continuous and categorical explanatory variables Mixed models; incorporating fixed and random effects Resampling procedures: randomisation, permutation, and bootstrapping Multivariate Analysis Cluster analysis Unsupervised learning: principal components analysis, dimension reduction Ordination: multidimensional scaling, correspondence analysis Supervised learning: discriminant analysis Networks and graphs "],["r-rstudio-and-git.html", "1 R, RStudio, and git 1.1 Learning objectives 1.2 Intro to R &amp; RStudio 1.3 Getting started 1.4 Reproducible research 1.5 Version control with git and GitHub 1.6 Exploratory Data Analysis 1.7 Other resources", " 1 R, RStudio, and git 1.1 Learning objectives Define the difference between R and RStudio Explain what an R function is; describe what an argument to an R function is Explain what an R package is; distinguish between the functions install.packages() and library() Use the appropriate R function to read in a data file Explain the importance of reproducibility in terms of scientific research Use the functionality offered by git and GitHub through RStudio 1.2 Intro to R &amp; RStudio R is the pheromone to RStudio's PDA R is the pheromone to RStudio's PDA. R is a language, specifically, a programming language; it's the way you can speak to your computer to ask it to carry out certain computations. RStudio is an integrated development environment (IDE). This means it is basically an interface, albeit a fancy one, that makes it easier to communicate with your computer in the language R. The main benefit is the additional features it has that enable you to more efficiently speak R. Note R and RStudio are two different pieces of software; for this course you are expected to download both. As you'd expect, the PDA depends on the pheromones (i.e., RStudio depends on R) so you have to download R to use RStudio! 1.2.1 Why? R It's free It's open source A general-purpose of programming language Written by statisticians (here in Auckland!) It's available for all operating systems (Windows, Linux, and Mac) There is a huge online support network It's extremely flexible; if you can code it you can do it! 15,000+ packages available! ... RStudio &quot;If R were an airplane, RStudio would be the airport...&quot; --- Julie Lowndes, Introduction to RStudio Awesomeness Speaks nicely to R Tab completion Debugging capabilities There is a huge online support network Offers many other features and tools to make your workflow with R easier It facilitates reproducibility ... 1.2.2 Installing R and RStudio As mentioned above RStudio depends on R so there is an order you should follow when you download these software. Download and install R by following these instructions. Make sure you choose the correct operating system. Download and install RStudio by going here choosing RStudio Desktop Open Source License Free and following instructions. Check all is working Open up RStudio from your computer menu, the icon will look something like this (DO NOT use this icon , this is a link to R and will only open a very basic interface) Wait a little and you should see RStudio open up to something similar to the screenshot below Pay close attention to the notes in the screenshot and familiarise yourself with the terms. Finally, in the Console next to the prompt type 1:10 and press enter on your keyboard. Your computer should say something back you (in the Console)! What do you think you were asking it to do? Does the output make sense?1 1.3 Getting started As in step 3. above open up RStudio from your computer menu, the icon will look something like this . Using the diagram above identify the different panes: Console where you directly type command in and communicate with your computer (via the language R). Environment pane Files pane Some terminology Running code: the act of telling R to perform an act by giving it commands in the console. Objects: where values are saved in (see later for creating an object. Script: a text file containing a set of commands and comments. Comments: notes written within a Script to better document/explain what's happening 1.3.1 R errors üò± data &lt;- read.csv(&quot;data_file_not_in_my_working_directory.csv&quot;) ## Warning in file(file, &quot;rt&quot;): cannot open file ## &#39;data_file_not_in_my_working_directory.csv&#39;: No such file or directory ## Error in file(file, &quot;rt&quot;): cannot open the connection library(some_library_I_have_not_installed) ## Error in library(some_library_I_have_not_installed): there is no package called &#39;some_library_I_have_not_installed&#39; some_function_I_spelled_worng(x = x) ## Error in some_function_I_spelled_worng(x = x): could not find function &quot;some_function_I_spelled_worng&quot; an_object_I_have_not_created ## Error in eval(expr, envir, enclos): object &#39;an_object_I_have_not_created&#39; not found What do you think the issues are here üòâ 1.3.2 R Scripts (a .r file) Go File &gt; New File &gt; R Script to open up a new Script If you had only three panes showing before, a new (fourth) pane should open up in the top left of RStudio. This file will have a .r extension and is where you can write, edit, and save the R commands you write. It's a dedicated text editor for your R code (very useful if you want to save your code to run at a later date). The main difference between typing your code into a Script vs Console is that you edit it and save it for later! Remember though the Console is the pane where you communicate with your computer so all code you write will have to be Run here. There are two ways of running a line of code you've written in your Script Ensure your cursor is on the line of code you want to run, hold down Ctrl and press Enter. Ensure your cursor is on the line of code you want to run, then use your mouse to click the Run button (it has a green arrow next to it) on the top right of the Script pane. Type 1:10 in your Script and practise running this line of code using both methods above. Not that if you've Run the code successfully then your computer will speak back to you each time via the Console 1.3.3 Writing Comments Comments are notes to yourself (future or present) or to someone else and are, typically, written interspersed in your code. Now, the comments you write will typically be in a language your computer doesn't understand (e.g., English). So that you can write yourself notes in your Script you need to tell your computer using the R language to ignore them. To do this precede any note you write with #, see below. The # is R for ignore anything after this character. ## IGNORE ME ## I&#39;m a comment ## I repeat I&#39;m a comment ## I am not a cat ## OK let&#39;s run some code 2 + 2 ## [1] 4 ## Hmm maybe I should check this ## @kareem_carr ;-) Now remember when you want to leave your R session you'll need to Save your Script to use it again. To do this go File &gt; Save As and name your file what you wish (remember too to choose a relevant folder on your computer, or as recommended use the .Rproj set-up as above). 1.3.4 Change the RStudio appearance up to your taste Go to Tools &gt; Global Options &gt; Apperance 1.4 Reproducible research Keep all similar files for the same analysis in the same place NEVER change raw data 1.4.1 Good practice Always start with a clean workspace Why? So your ex (code) can't come and mess up your life! Go to Tools &gt; Global Options Project-oriented workflow. Recommended: .Rproj Organised Set up each Each assignment/university course as a project Self-contained a project is a folder that contains all relevant files All paths can then be relative to that project Reproducible the project should just work on a different computer Got to Project (top right) &gt; New Project &gt; Create Project Project set-up ‚ö†Ô∏èWarning‚ö†Ô∏è Jenny Bryan will set your computer on fire üî• if you start your script like this rm(list = ls()) This does NOT create a fresh R process it makes your script vulnerable it will come back to bite you 1.5 Version control with git and GitHub All workshops will use these tools git the software &quot;Track Changes features from Microsoft Word on steroids&quot; --- Jenny Bryan a version control system manages the evolution of a set of files (tidily) GitHub an online hosting service &quot;Think of it as DropBox but much, much better&quot; --- Jenny Bryan home for your Git-based projects on the internet 1.5.1 Setup TL;DR Register an account with GitHub https://github.com Make sure you've got the latest version of R R.version.string ## [1] &quot;R version 4.0.5 (2021-03-31)&quot; Upgrade RStudio to the new preview version (optional) Install git: follow these instructions Get started 1.5.2 Cloning a repository from GitHub using RStudio On GitHub, navigate to the Code tab of the repository On the right side of the screen, click Clone or download Click the Copy to clipboard icon to the right of the repository URL (e.g., https://github.com/STATS-UOA/workshops-biosci738.git) Open RStudio in your local environment Click File, New Project, Version Control, Git Paste the repository URL and enter TAB to move to the Project directory name field. I've chosen to store this folder on my Desktop, obviously put it wherever you wish :-) Click Create Project. Your Files pane should now look similar to this 1.6 Exploratory Data Analysis or EDA we will be using tidyverse. 'tidyverse' is a collection of R packages that all share underlying design philosophy, grammar, and data structures. They are specifically designed to make data wrangling, manipulation, visualisation, and analysis simpler. 1.6.1 Starting out with tidyverse Artwork by [@allison_horst](https://github.com/allisonhorst/) Starting out with tidyverse To install all the packages that belong to the tidyverse run ## request (download) the tidyverse packages from the centralised library install.packages(&quot;tidyverse&quot;) To tell your computer to access the tidyverse functionality in your session run (Note you'll have to do this each time you start up an R session): ## Get the tidyverse packages from our local library library(tidyverse) 1.6.2 Reading in data from a .csv file First off download the paua.csv file from CANVAS To read the data into RStudio In the Environment pane click Import Dataset &gt; ** From Text (readr)** &gt; Browse &gt; Choose your file, remembering which folder you downloaded it to. this is where .Rproj is useful &gt; Another pane should pop up, check the data looks as you might expect &gt; Import Or paua &lt;- read_csv(&quot;paua.csv&quot;) 1.6.3 Explore your data Let's have a look at your data in the Console paua ## # A tibble: 60 x 3 ## Species Length Age ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Haliotis iris 1.8 1.50 ## 2 Haliotis australis 5.4 11.9 ## 3 Haliotis australis 4.8 5.42 ## 4 Haliotis iris 5.75 4.50 ## 5 Haliotis iris 5.65 5.50 ## 6 Haliotis iris 2.8 2.50 ## 7 Haliotis australis 5.9 6.49 ## 8 Haliotis iris 3.75 5.00 ## 9 Haliotis australis 7.2 8.56 ## 10 Haliotis iris 4.25 5.50 ## # ‚Ä¶ with 50 more rows 1.6.4 Explore your data Using the glimpse() command for an alternative view glimpse(paua) ## Rows: 60 ## Columns: 3 ## $ Species &lt;chr&gt; &quot;Haliotis iris&quot;, &quot;Haliotis australis&quot;, &quot;Haliotis australis&quot;, &quot;‚Ä¶ ## $ Length &lt;dbl&gt; 1.80, 5.40, 4.80, 5.75, 5.65, 2.80, 5.90, 3.75, 7.20, 4.25, 6.‚Ä¶ ## $ Age &lt;dbl&gt; 1.497884, 11.877010, 5.416991, 4.497799, 5.500789, 2.500972, 6‚Ä¶ 1.6.5 The pipe operator %&gt;% A nifty tidyverse tool is called the pipe operator %&gt;%. The pipe operator allows us to combine multiple operations in R into a single sequential chain of actions. Say you would like to perform a hypothetical sequence of operations on a hypothetical data frame x using hypothetical functions f(), g(), and h(): Take x then Use x as an input to a function f() then Use the output of this as an input to a function g() then Use the output of this as an input to a function h() So to calculate the mean Age of each Species in the paua dataset we would use paua %&gt;% group_by(Species) %&gt;% summarize(mean_age = mean(Age)) ## # A tibble: 2 x 2 ## Species mean_age ## * &lt;chr&gt; &lt;dbl&gt; ## 1 Haliotis australis 7.55 ## 2 Haliotis iris 4.40 You would read the sequence above as: Take the paua data.frame then Use this and apply the group_by() function to group by Species Use this output and apply the summarize() function to calculate the mean Age of each group (Species), calling the resulting number mean_age Or to describe my daily routine... I %&gt;% wake_up(time = &quot;later than I should&quot;) %&gt;% give(who = &quot;Watson&quot; , what = &quot;medication&quot;) %&gt;% make(who= &quot;myself&quot;, what = &quot;coffee&quot;) %&gt;% drink() %&gt;% try(remember_what_I_have_on(date = &quot;today&quot;)) Have a go at writing your own! 1.7 Other resources R for Data Science RStudio Education An Introduction to R R for Biologists The Popularity of Data Science Software Happy Git and GitHub for the useR Artwork by @allison_horst You should have seen the numbers 1 to 10 printed out as a sequence.‚Ü© "],["mƒÅori-data-sovereignty-principles.html", "2 MƒÅori Data Sovereignty principles 2.1 Learning objectives 2.2 MƒÅori Data Sovereignty principles 2.3 MƒÅori Data Sovereignty principles 2.4 Resources", " 2 MƒÅori Data Sovereignty principles ''Data sovereignty is the idea that data are subject to the laws and governance structures within the nation it is collected'' &quot;MƒÅori Data Sovereignty has emerged as a critical policy issue as Aotearoa New Zealand develops world-leading linked administrative data resources.&quot; --- Andrew Sporle, Maui Hudson, Kiri West. Chapter 5, Indigenous Data Sovereignty and Policy Te Tiriti o Waitangi/Treaty of Waitangi obliges the Government to actively protect taonga, consult with MƒÅori in respect of taonga, give effect to the principle of partnership and recognize MƒÅori rangatiratanga over taonga. Factors that relate to how communities might recognize the taonga nature of any dataset include provenance of the data: does the dataset come from a significant MƒÅori source? opportunity for the data: could the dataset support MƒÅori aspirations for their people or their whenua (land)? utility of the data: does the dataset have multiple uses? 2.1 Learning objectives Define data sovereignty and explain this in relation to a researcher's obligation when collecting, displaying, and analysing data Define and discuss MƒÅori Data Sovereignty principles 2.2 MƒÅori Data Sovereignty principles &quot;For Indigenous peoples, historical encounters with statistics have been fraught, and none more so than when involving official data produced as part of colonial attempts at statecraft.&quot; --- Lovett, R., Lee, V., Kukutai, T., Cormack, D., Rainie, S.C. and Walker, J., 2019. Good data practices for Indigenous data sovereignty and governance. Good data, pp.26-36. &quot;MƒÅori Data Sovereignty has emerged as a critical policy issue as Aotearoa New Zealand develops world-leading linked administrative data resources.&quot; --- Andrew Sporle, Maui Hudson, Kiri West. Chapter 5, Indigenous Data Sovereignty and Policy 2.3 MƒÅori Data Sovereignty principles ‚ÄúMƒÅori data refers to data produced by MƒÅori or that is about MƒÅori and the environments we have relationships with.&quot; --- Te Mana Raraunga Charter Data is a ‚Äúpotential taonga, something precious that needs to be maintained, in relation to its utility‚Äù --- Dr W. Edwards, TMR website MƒÅori Data Sovereignty principles to inform the recognition of MƒÅori rights and interests in data, and the ethical use of data to enhance MƒÅori well-being: Rangatiratanga, authority MƒÅori have an inherent right to exercise control over MƒÅori data and MƒÅori data ecosystems. This right includes, but is not limited to, the creation, collection, access, analysis, interpretation, management, security, dissemination, use and reuse of MƒÅori data. Decisions about the physical and virtual storage of MƒÅori data shall enhance control for current and future generations. Whenever possible, MƒÅori data shall be stored in Aotearoa New Zealand. MƒÅori have the right to data that is relevant and empowers sustainable self-determination and effective self-governance Whakapapa, relationships All data has a whakapapa (genealogy). Accurate metadata should, at minimum, provide information about the provenance of the data, the purpose(s) for its collection, the context of its collection, and the parties involved. The ability to disaggregate MƒÅori data increases its relevance for MƒÅori communities and iwi. MƒÅori data shall be collected and coded using categories that prioritise MƒÅori needs and aspirations. Current decision-making over data can have long-term consequences, good and bad, for future generations of MƒÅori. A key goal of MƒÅori data governance should be to protect against future harm. Whanaungatanga, obligations Individuals‚Äô rights (including privacy rights), risks and benefits in relation to data need to be balanced with those of the groups of which they are a part. In some contexts, collective MƒÅori rights will prevail over those of individuals. Individuals and organisations responsible for the creation, collection, analysis, management, access, security or dissemination of MƒÅori data are accountable to the communities, groups and individuals from whom the data derive Kotahitanga, collective benefit Data ecosystems shall be designed and function in ways that enable MƒÅori to derive individual and collective benefit. Build capacity. MƒÅori Data Sovereignty requires the development of a MƒÅori workforce to enable the creation, collection, management, security, governance and application of data. Connections between MƒÅori and other Indigenous peoples shall be supported to enable the sharing of strategies, resources and ideas in relation to data, and the attainment of common goals. Manaakitanga, reciprocity The collection, use and interpretation of data shall uphold the dignity of MƒÅori communities, groups and individuals. Data analysis that stigmatises or blames MƒÅori can result in collective and individual harm and should be actively avoided. Free, prior and informed consent shall underpin the collection and use of all data from or about MƒÅori. Less defined types of consent shall be balanced by stronger governance arrangements. Kaitiakitanga, guardianship MƒÅori data shall be stored and transferred in such a way that it enables and reinforces the capacity of MƒÅori to exercise kaitiakitanga over MƒÅori data. Ethics. Tikanga, kawa (protocols) and mƒÅtauranga (knowledge) shall underpin the protection, access and use of MƒÅori data. MƒÅori shall decide which MƒÅori data shall be controlled (tapu) or open (noa) access. 2.4 Resources Why data sovereignty matters Indigenous Data Sovereignty and Policy Principles of MƒÅori Data Sovereignty Good data practices for Indigenous data sovereignty and governance. "],["data-wrangling-and-vizualisation.html", "3 Data wrangling and vizualisation 3.1 Learning objectives 3.2 Common dataframe manipulations in the tidyverse 3.3 Data Viz 3.4 Ten Simple Rules for Better Figures 3.5 ggplot2 3.6 Resources", " 3 Data wrangling and vizualisation 3.1 Learning objectives Carry out, and interpret the outputs of, basic exploratory data analysis using in-built R functions Discuss the ethics of data vizualisation Create and communicate informative data visualisations using R Discuss and critique data visualisations 3.2 Common dataframe manipulations in the tidyverse 3.2.1 Using dplyr and tidyr tidy data &quot;Tidy datasets are all alike, but every messy dataset is messy in its own way.&quot; --- Hadley Wickham There are three interrelated rules which make a dataset tidy: Each variable must have its own column Each observation must have its own row Each value must have its own cell [illustrations from the Openscapes blog Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst Why ensure that your data is tidy? Consistency: using a consistent format aids learning and reproducibility Simplicity: it's a format that is well understood by R &quot;Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets.&quot; --- Hadley Wickham, Tidy data 3.2.2 Introuducing the Palmer penguins library(palmerpenguins) ## contains some nice penguin data penguins ## # A tibble: 344 x 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen NA NA NA NA ## 5 Adelie Torgersen 36.7 19.3 193 3450 ## 6 Adelie Torgersen 39.3 20.6 190 3650 ## 7 Adelie Torgersen 38.9 17.8 181 3625 ## 8 Adelie Torgersen 39.2 19.6 195 4675 ## 9 Adelie Torgersen 34.1 18.1 193 3475 ## 10 Adelie Torgersen 42 20.2 190 4250 ## # ‚Ä¶ with 334 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; So, what does this show us? A tibble: 344 x 8: A tibble is a specific kind of data frame in R. The penguin dataset has 344 rows (i.e., 344 different observations). Here, each observation corresponds to a penguin. 8 columns corresponding to 3 variables describing each observation. species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, sex, and year are the different variables of this dataset. We then have a preview of the first 10 rows of observations corresponding to the first 10 penguins. ``... with 334 more rows indicates there are 334 more rows to see, but these have not been printed (likely as it would clog our screen) To learn more about the penguins read the paper that talks all about the data collection. 3.2.3 Common dataframe manipulations in the tidyverse, using dplyr and tidyr Even from these first few rows of data we can see that there are some NA values. Let's count the number of NAs. Remember the %&gt;% operator? Here we're going to be introduced to a few new things the apply() function, the is.na() function, and how R deals with logical values! library(tidyverse) penguins %&gt;% apply(.,2,is.na) %&gt;% apply(.,2,sum) ## species island bill_length_mm bill_depth_mm ## 0 0 2 2 ## flipper_length_mm body_mass_g sex year ## 2 2 11 0 There's lot going on in that code! Let's break it down Take penguins then Use penguins as an input to the apply() function (this is specified as the first argument using the .) Now the apply() function takes 3 arguments: the data object you want it to apply something to (in our case penguins) the margin you want to apply that something to; 1 stands for rows and 2 stands for columns, and the function you want it to apply (in our case is.na()). So the second line of code is asking R to apply the is.na() function over the columns of penguins is.na() asks for each value it's fed is it an NA value; it returns a TRUE if so and a FALSE otherwise The output from the first apply() is then fed to the second apply() (using the .). The sum() function then add them up! R treats a TRUE as a 1 and a FALSE as a 0. So how many NAs do you think there are! Doesn't help much. To Now we know there are NA values throughout the data let's remove then and create a new NA free version called penguins_nafree. There is a really handy tidyverse (dplyr) function for this! penguins_nafree &lt;- penguins %&gt;% drop_na() penguins_nafree ## # A tibble: 333 x 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen 36.7 19.3 193 3450 ## 5 Adelie Torgersen 39.3 20.6 190 3650 ## 6 Adelie Torgersen 38.9 17.8 181 3625 ## 7 Adelie Torgersen 39.2 19.6 195 4675 ## 8 Adelie Torgersen 41.1 17.6 182 3200 ## 9 Adelie Torgersen 38.6 21.2 191 3800 ## 10 Adelie Torgersen 34.6 21.1 198 4400 ## # ‚Ä¶ with 323 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; Below are some other useful manipulation functions; have a look at the outputs and run them yourselves and see if you can work out what they're doing. filter(penguins_nafree, island == &quot;Torgersen&quot; ) ## # A tibble: 47 x 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen 36.7 19.3 193 3450 ## 5 Adelie Torgersen 39.3 20.6 190 3650 ## 6 Adelie Torgersen 38.9 17.8 181 3625 ## 7 Adelie Torgersen 39.2 19.6 195 4675 ## 8 Adelie Torgersen 41.1 17.6 182 3200 ## 9 Adelie Torgersen 38.6 21.2 191 3800 ## 10 Adelie Torgersen 34.6 21.1 198 4400 ## # ‚Ä¶ with 37 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; summarise(penguins_nafree, avgerage_bill_length = mean(bill_length_mm)) ## # A tibble: 1 x 1 ## avgerage_bill_length ## &lt;dbl&gt; ## 1 44.0 group_by(penguins_nafree, species) ## # A tibble: 333 x 8 ## # Groups: species [3] ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen 36.7 19.3 193 3450 ## 5 Adelie Torgersen 39.3 20.6 190 3650 ## 6 Adelie Torgersen 38.9 17.8 181 3625 ## 7 Adelie Torgersen 39.2 19.6 195 4675 ## 8 Adelie Torgersen 41.1 17.6 182 3200 ## 9 Adelie Torgersen 38.6 21.2 191 3800 ## 10 Adelie Torgersen 34.6 21.1 198 4400 ## # ‚Ä¶ with 323 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; Often we want to summarise variables by different groups (factors). Below we Take the penguins_nafree data then Use this and apply the group_by() function to group by species Use this output and apply the summarize() function to calculate the mean (using (mean()) bill length (bill_length_mm) of each group (species), calling the resulting number avgerage_bill_length penguins_nafree %&gt;% group_by(species) %&gt;% summarise(avgerage_bill_length = mean(bill_length_mm)) ## # A tibble: 3 x 2 ## species avgerage_bill_length ## * &lt;fct&gt; &lt;dbl&gt; ## 1 Adelie 38.8 ## 2 Chinstrap 48.8 ## 3 Gentoo 47.6 We can also group by multiple factors, for example, penguins_nafree %&gt;% group_by(island,species) %&gt;% summarise(avgerage_bill_length = mean(bill_length_mm)) ## `summarise()` has grouped output by &#39;island&#39;. You can override using the `.groups` argument. ## # A tibble: 5 x 3 ## # Groups: island [3] ## island species avgerage_bill_length ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 Biscoe Adelie 39.0 ## 2 Biscoe Gentoo 47.6 ## 3 Dream Adelie 38.5 ## 4 Dream Chinstrap 48.8 ## 5 Torgersen Adelie 39.0 3.3 Data Viz &quot;...have obligations in that we have a great deal of power over how people ultimately make use of data, both in the patterns they see and the conclusions they draw.&quot; --- Michael Correll, Ethical Dimensions of Visualization Research &quot;Clutter and confusion are not attributes of data - they are shortcomings of design.&quot; --- Edward Tufte 3.3.1 Exploratory and explanatory plots Exploratory plots (for you) data exploration doesn't have to look pretty just needs to get to the point explore and discover new data facets formulate new questions For example, Explanatory plots (for others), most common kind of graph used in scientific publications clear purpose designed for the audience make it easy to read (this covers a lot of things) do not distort guide the reader to a particular conclusion answer a specific question support a decision For example, Plots by Cedric Scherer and mentioned on this blog 3.4 Ten Simple Rules for Better Figures &quot;Scientific visualization is classically defined as the process of graphically displaying scientific data. However, this process is far from direct or automatic. There are so many different ways to represent the same data: scatter plots, linear plots, bar plots, and pie charts, to name just a few. Furthermore, the same data, using the same type of plot, may be perceived very differently depending on who is looking at the figure. A more accurate definition for scientific visualization would be a graphical interface between people and data.&quot; --- Nicolas P. Rougier, Michael Droettboom, Philip E. Bourne, Ten Simple Rules for Better Figures Know Your Audience Identify Your Message Adapt the Figure to the Support Medium Captions Are Not Optional Do Not Trust the Defaults Use Color Effectively Do Not Mislead the Reader There are formulas to measure how misleading a graph is! Avoid Chartjunk Message Trumps Beauty &quot;message and readability of the figure is the most important aspect while beauty is only an option&quot; --- Nicolas P. Rougier, Michael Droettboom, Philip E. Bourne, Ten Simple Rules for Better Figures Get the Right Tool I'm an advocate for R üòâ 3.5 ggplot2 ggplot2 is an R package for producing statistical, or data, graphics; it has an underlying grammar based on the Grammar of Graphics Every ggplot2 plot has three key components: data, A set of aesthetic mappings between variables in the data and visual properties, and At least one layer which describes how to render each observation. Layers are usually created with a geom function. 3.5.1 ggplot2 layers 3.5.2 Examples Scatter plot using geom_point() ggplot(penguins,aes(x = body_mass_g, y = flipper_length_mm)) + ## data &amp; aesthetics geom_point() + ## geom geom_smooth(method = &#39;lm&#39;, se = FALSE) ## statistics (linear regression line) Boxplot using geom_boxplot() ggplot(penguins,aes(x = species, y = flipper_length_mm)) + ## data &amp; aesthetics geom_boxplot() + ## geom ggtitle(&quot;Flipper length (mm) by species&quot;) + ylab(&quot;Flipper length (mm)&quot;) + xlab(&quot;Species&quot;) + theme_dark() ## theme Scatter plot specifying color using geom_point() ggplot(penguins,aes(x = body_mass_g, y = flipper_length_mm, color = species)) + ## data and aesthetics geom_point() + ## geom geom_smooth(method = &#39;lm&#39;, se = FALSE) ## statistic (linear regression line without intervals) 3.5.3 The Good, the Bad, and the Ugly... box &lt;- ggplot(penguins,aes(x = species, y = flipper_length_mm)) + ## data &amp; aesthetics geom_boxplot() + ## geom ggtitle(&quot;Flipper length (mm) by species&quot;) + ylab(&quot;Flipper length (mm)&quot;) + xlab(&quot;Species&quot;) + theme_dark() ## theme box jitter &lt;- ggplot(penguins,aes(x = species, y = flipper_length_mm)) + ## data &amp; aesthetics geom_jitter() + ## geom ggtitle(&quot;Flipper length (mm) by species&quot;) + ylab(&quot;Flipper length (mm)&quot;) + xlab(&quot;Species&quot;) + theme_dark() ## theme jitter violin &lt;- ggplot(penguins,aes(x = species, y = flipper_length_mm)) + ## data &amp; aesthetics geom_violin() + ## geom ggtitle(&quot;Flipper length (mm) by species&quot;) + ylab(&quot;Flipper length (mm)&quot;) + xlab(&quot;Species&quot;) + theme_dark() ## theme violin ggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm)) + geom_point() + geom_smooth(method = &quot;lm&quot;, col = &quot;blue&quot;, se = FALSE) ## `geom_smooth()` using formula &#39;y ~ x&#39; ggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm, col = species)) + geom_point(size = 2, alpha = 0.5) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + facet_grid(~ sex) + theme_bw() + labs(title = &quot;Flipper Length and Body Mass, by Sex &amp; Species&quot;, subtitle = paste0(nrow(penguins), &quot; of the Palmer Penguins&quot;), x = &quot;Body Mass (g)&quot;, y = &quot;Flipper Length (mm)&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; penguins_nafree &lt;- penguins %&gt;% drop_na() ggplot(penguins_nafree, aes(x = body_mass_g, y = flipper_length_mm, col = species)) + geom_point(size = 2, alpha = 0.5) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + facet_grid(~ sex) + theme_bw() + labs(title = &quot;Flipper Length and Body Mass, by Sex &amp; Species&quot;, subtitle = paste0(nrow(penguins_nafree), &quot; of the Palmer Penguins&quot;), x = &quot;Body Mass (g)&quot;, y = &quot;Flipper Length (mm)&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; 3.6 Resources ggplot2 cheatsheet Ten Simple Rules for Better Figures Elegant Graphics for Data Analysis Using ggplot2 to communicate your results Interesting blogs on graphs in the media tidyverse Tidy Data Palmer penguins "],["introduction-to-the-design-and-analysis-of-experiments.html", "4 Introduction to the design and analysis of experiments 4.1 Learning Objectives 4.2 Key phrases 4.3 Three key principles: 4.4 One-Way Analysis of Variance (ANOVA) 4.5 üò± p-values üò± 4.6 Terminology and issues 4.7 Resources", " 4 Introduction to the design and analysis of experiments 4.1 Learning Objectives Identify the following experimental unit observational units List and describe the three main principals of experimental design Randomization Replication Blocking Calculate Sums of Squares (between and within groups) given the observations Define and state the appropriate degrees of freedom in a one-way ANOVA scenario Calculate the F-statistics given the appropriate Sums of Squares and degrees of freedom Interpret and discuss a given p-value in the context of a stated hypothesis test Explain between group and within group variation Communicate statistical concepts and experimental outcomes clearly using language appropriate for both a scientific and non-scientific audience 4.2 Key phrases Experimental unit Smallest portion of experimental material which is independently perturbed Treatment The experimental condition independently applied to an experimental unit Observational unit The smallest unit on which a response is measured. If one measurement is made on each rat: Observational unit = Experimental unit. If Multiple measurements are made on each rat: Each experimental unit has &gt;1 observational unit (pseudo- or technical replication). 4.3 Three key principles: 4.3.1 Replication Biological replication: each treatment is independently applied to each of several humans, animals or plants To generalize results to population Technical replication: two or more samples from the same biological source which are independently processed Advantageous if processing steps introduce a lot of variation Increases the precision with which comparisons of relative abundances between treatments are made Pseudo-replication: one sample from the same biological source, divided into two or more aliquots which are independently measured Advantageous for noisy measuring instruments Increases the precision with which comparisons of relative abundances between treatments are made 4.3.2 Randomization Protects against bias Plan the experiment in such a way that the variations caused by extraneous factors can all be combined under the general heading of &quot;chance&quot;. Ensures that each treatment has the same probability of getting good (or bad) units and thus avoids systematic bias random allocation can cancel out population bias; it ensures that any other possible causes for the experimental results are split equally between groups typically statistical analysis assumes that observations are independent. This is almost never strictly true in practice but randomisation means that our estimates will behave as if they were based on independent observations 4.3.3 Blocking Blocking helps control variability by making treatment groups more alike. Experimental units are divided into subsets (called blocks) so that units within the same block are more similar than units from different subsets or blocks. Blocking is a technique for dealing with nuisance factors. A nuisance factor is a factor that has some effect on the response, but is of no interest (e.g., age class). 4.4 One-Way Analysis of Variance (ANOVA) 4.4.1 Between group SS (SSB) The idea: Assess distances between treatment (surgical condition) means relative to our uncertainty about the actual (true) treatment means. add up the differences: -1.192 + -0.703 + 1.895 = 0. This is always the case! So adding up the differences: -1.192 + -0.703 + 1.895 = 0. Not a great way to measure distances! Sums of Squares? -1.192^2 + -0.703^2 + 1.895^2 add up the squared differences? but... there are 4 observations in each group (treatment) 4\\(\\times\\)(-1.192)^2 + 4\\(\\times\\)(-0.703)^2 + 4\\(\\times\\)(1.895)^2 This is the Between Groups Sums of Squares or the Between group SS (SSB) So the Between group SS (SSB) = 22.02635 Adding up the differences: -1.192 + -0.703 + 1.895 = 0. This is always the case and that itself gives us information... We only need to know two of the values to work out the third! So we have only 2 bits of unique information; SSB degrees of freedom = 2 4.4.2 Within group SS (SSW) The Within group SS (SSW) arises from the same idea: To assess distances between treatment (surgical condition) means relative to our uncertainty about the actual (true) treatment means. Procedure: Observation - Treatment mean Square the difference Add them up! Within group SS (SSW) unexplained variance 4.4.3 F-statistic Recall the Between group SS (SSB) = 22.02635 So mean SSB = 22.02635 / 2 The within group SS (SSW) = 6.059075 Here we have 3\\(\\times\\) 3 bits of unique information: within groups degrees of freedom is 9. So mean SSW = 6.059/9 Consider the ratio \\({\\frac{{\\text{variation due to treatments}}}{{\\text{unexplained variance}}}} = {\\frac{{\\text{ mean between-group variability}}}{{\\text{mean within-group variability}}}}\\) \\(=\\frac{\\text{mean SSB}}{\\text{mean SSW}}\\) \\(=\\frac{\\text{MSB}}{\\text{MSW}}\\) = \\(=\\frac{\\text{experimental variance}}{\\text{error variance}}\\) 16.3586975 This is the F-statistic! 4.4.4 Degrees of freedom (DF) Essentially statistical currency (i.e., unique bits of information). So in the example above we have 3 treatment groups and if we know the mean of two we know the third (i.e., 2 unique bits of info) so SSB df = 2. Now, for SSW df. We have 12 observations (4 in each group); we know the treatment means so if we have three of those observed values in each group we know the fourth: 12 - 3 = 9 (i.e., number of observations - number of df lost due to knowing the cell means). 4.5 üò± p-values üò± The ASA Statement on p-Values: Context, Process, and Purpose &quot;Good statistical practice, as an essential component of good scientific practice, emphasizes principles of good study design and conduct, a variety of numerical and graphical summaries of data, understanding of the phenomenon under study, interpretation of results in context, complete reporting and proper logical and quantitative understanding of what data summaries mean. No single index should substitute for scientific reasoning.&quot; --- ASA Statement on p-Values What is a p-Value? Informally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value p-values can indicate how incompatible the data are with a specified statistical model p-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold Proper inference requires full reporting and transparency A p-value, or statistical significance, does not measure the size of an effect or the importance of a result By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis 4.6 Terminology and issues Type I error (false positive): declare a difference (i.e., reject \\(H_0\\)) when there is no difference (i.e. \\(H_0\\) is true). Risk of the Type I error is determined by the level of significance (which we set!) (i.e., \\(\\alpha =\\text{ P(Type I error)} = \\text{P(false positive)}\\). Type II error (false negative): difference not declared (i.e., \\(H_0\\) not rejected) when there is a difference (i.e., \\(H_0\\) is false). Let \\(\\beta =\\) P(do not reject \\(H_0\\) when \\(H_0\\) is false); so, \\(1-\\beta\\) = P(reject \\(H_0\\) when \\(H_0\\) is false) = P(a true positive), which is the statistical power of the test. Each time we carry out a hypothesis test the probability we get a false positive result (type I error) is given by \\(\\alpha\\) (the level of significance we choose). When we have multiple comparisons to make we should then control the Type I error rate across the entire family of tests under consideration, i.e., control the Family-Wise Error Rate (FWER); this ensures that the risk of making at least one Type I error among the family of comparisons in the experiment is \\(\\alpha\\). State of Nature Don't reject \\(H_0\\) reject \\(H_0\\) \\(H_0\\) is true ‚úÖ Type I error \\(H_0\\) is false Type II error ‚úÖ 4.7 Resources Looking forward Traditional name Model formula R code Simple regression \\(Y \\sim X_{continuous}\\) lm(Y ~ X) One-way ANOVA \\(Y \\sim X_{categorical}\\) lm(Y ~ X) Two-way ANOVA \\(Y \\sim X1_{categorical} + X2_{categorical}\\) lm(Y ~ X1 + X2) ANCOVA \\(Y \\sim X1_{continuous} + X2_{categorical}\\) lm(Y ~ X1 + X2) Multiple regression \\(Y \\sim X1_{continuous} + X2_{continuous}\\) lm(Y ~ X1 + X2) Factorial ANOVA \\(Y \\sim X1_{categorical} * X2_{categorical}\\) lm(Y ~ X1 * X2) or lm(Y ~ X1 + X2 + X1:X2) Glass, David J. Experimental Design for Biologists. Second ed. 2014. Print. Welham, S. J. Statistical Methods in Biology : Design and Analysis of Experiments and Regression. 2015. Print. Fisher, Ronald Aylmer. The Design of Experiments. 8th ed. Edinburgh: Oliver &amp; Boyd, 1966. Print. O &amp; B Paperbacks. "],["a-completely-randomomised-design.html", "5 A completely randomomised design 5.1 Learning Objectives 5.2 Analysis of a Completely Randomised Design in R: aov() and lm()", " 5 A completely randomomised design 5.1 Learning Objectives Describe a Completely Randomised (experimental) Design Carry out linear regression in R with one categorical explanatory variable (one-way ANOVA) and draw the appropriate inference Communicate statistical concepts and experimental outcomes clearly using language appropriate for both a scientific and non-scientific audience 5.2 Analysis of a Completely Randomised Design in R: aov() and lm() library(tidyverse) rats &lt;- read_csv(&quot;crd_rats_data.csv&quot;) rats %&gt;% group_by(Surgery) %&gt;% summarise(avg = mean(logAUC)) ## # A tibble: 3 x 2 ## Surgery avg ## * &lt;fct&gt; &lt;dbl&gt; ## 1 C 8.46 ## 2 P 8.95 ## 3 S 11.5 5.2.1 aov() rats_aov &lt;- aov(logAUC ~ Surgery, data = rats) summary(rats_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Surgery 2 22.026 11.013 16.36 0.00101 ** ## Residuals 9 6.059 0.673 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Inference Hypothesis: We test the Null hypothesis, \\(H_0\\), population (Surgery) means are the same on average verses the alternative hypothesis, \\(H_1\\), that at least one differs from the others! Probability of getting an F-statistic at least as extreme as the one we observe (think of the area under the tails of the curve below) p-value Pr(&gt;F)= 0.001 tells us we have sufficient evidence to reject \\(H_0\\) at the 1% level of significance 5.2.2 lm() lm() rats_lm &lt;- lm(logAUC ~ Surgery, data = rats) Inference summary(rats_lm)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.4600 0.4102531 20.6214144 6.930903e-09 ## SurgeryP 0.4900 0.5801856 0.8445574 4.202408e-01 ## SurgeryS 3.0875 0.5801856 5.3215734 4.799872e-04 Which pairs of means are different? Pair-wise comparisons of means Use two-sample t-tests We need to calculate our observed t-value where \\(\\text{t-value} = \\frac{\\text{Sample Difference}_{ij} - \\text{Difference assuming } H_0 \\text{ is true}_{ij}}{\\text{SE of } \\text{Sample Difference}_{ij}}\\) where \\(\\text{Sample Difference}_{ij}\\) = Difference between pair of sample means Compute the p-value for observed t-value (Intercept) = \\(\\text{mean}_C\\) = 8.46 SE of (Intercept) = SE of \\(\\text{mean}_C\\) = SEM = 0.4102531 \\(\\text{Surgery}_P\\) = \\(\\text{mean}_P\\) ‚Äì \\(\\text{mean}_C\\) = 0.49 SE of \\(\\text{Surgery}_P\\) = SE of (\\(\\text{mean}_P\\) - \\(\\text{mean}_C\\) ) = SED = 0.5801856 Hypotheses being tested The t value and Pr (&gt;|t|) are the t - and p-value for testing the null hypotheses: Mean abundance is zero for C population No difference between the population means of P and C No difference between the population means of S and C We're interested in 2 and 3, but not necessarily 1! Two-sample t -tests for pairwise comparisons of means SurgeryP : t value = Estimate √∑ Std.Error = 0.8446; Pr (&gt;|t|) = 0.4202 F-test: anova(rats_lm) ## Analysis of Variance Table ## ## Response: logAUC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Surgery 2 22.0263 11.0132 16.359 0.001006 ** ## Residuals 9 6.0591 0.6732 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The same as aov() in fact aov() is calling lm() in the background. 5.2.2.1 Diagnostic plots Carrying out any linear regression we have some key assumptions Independence There is a linear relationship between the response and the explanatory variables The residuals have constant variance The residuals are normally distributed gglm::gglm(rats_lm) # Plot the four main diagnostic plots Residuals vs Fitted plot You are basically looking for no pattern or structure in your residuals (e.g., a &quot;starry&quot; night). You definitely don't want to see is the scatter increasing around the zero line (dashed line) as the fitted values get bigger (e.g., think of a trumpet, a wedge of cheese, or even a slice of pizza) which would indicate unequal variances (heteroscedacity). Normal quantile-quantile (QQ) plot This plot shows the sorted residuals versus expected order statistics from a standard normal distribution. Samples should be close to a line; points moving away from 45 degree line at the tails suggest the data are from a skewed distribution. Scale-Location plot (\\(\\sqrt{\\text{|standardized residuals vs Fitted|}}\\)) Another way to check the homoskedasticity (constant-variance) assumption. We want the line to be roughly horizontal. If this is the case then the average magnitude of the standardized residuals isn't changing much as a function of the fitted values. We'd also like the spread around the line not to vary much with the fitted values; then the variability of magnitudes doesn't vary much as a function of the fitted values. Residuals vs Leverage plot (standardized residuals vs Leverage) This can help detect outliers in a linear regression model. For linear regression model leverage measures how sensitive a fitted value is to a change in the true response. We're looking at how the spread of standardized residuals changes as the leverage. This can also be used to detect heteroskedasticity and non-linearity: the spread of standardized residuals shouldn't change as a function of leverage. In addition, points with high leverage may be influential: that is, deleting them would change the model a lot. "],["multiple-comparisons.html", "6 Multiple comparisons 6.1 Learning objectives 6.2 Adjustments for multiple testing", " 6 Multiple comparisons 6.1 Learning objectives Discuss and critique methods for controlling errors in hypothesis testing, for example Fisher‚Äôs LSD and the Bonferroni Correction Detail and draw inference form multiple comparison procedures such as Tukey‚Äôs HSD and Dunnett's test Describe the family wise error rate (FWER) and false discover rate (FDR) in the context of multiple comparisons Calculate the marginal means for a balanced and unbalanced design Communicate statistical concepts and experimental outcomes clearly using language appropriate for both a scientific and non-scientific audience 6.2 Adjustments for multiple testing Recall that each time we carry out a hypothesis test the probability we get a false positive result (type I error) is given by \\(\\alpha\\) (the level of significance we choose). When we have multiple comparisons to make we should then control the Type I error rate across the entire family of tests under consideration, i.e., control the Family-Wise Error Rate (FWER); this ensures that the risk of making at least one Type I error among the family of comparisons in the experiment is \\(\\alpha\\). State of Nature Don't reject \\(H_0\\) reject \\(H_0\\) \\(H_0\\) is true ‚úÖ Type I error \\(H_0\\) is false Type II error ‚úÖ or... The familywise error rate (FWER) is the risk of making at least one Type I error among the family of comparisons in the experiment. Now let's consider carrying out \\(m\\) independent t-tests and let for any single test, let Pr(commit a Type 1 error) \\(= \\alpha_c\\) be the per comparison error rate (PCER). So for a single test the probability a correct decision is made is \\(1 - \\alpha_c\\). Therefore for \\(m\\) independent t-tests the probability of committing no Type I errors is \\((1 - \\alpha_c)^m\\) and the probability of committing at least one Type I error is \\(1 -(1 - \\alpha_c)^m = \\alpha_F\\) which is the upper limit of the FWER. 6.2.1 Classification of multiple hypothesis tests Suppose we have a number \\(m\\) of null hypotheses, \\(H_1, H_2, ..., H_m\\). Using the traditional parlence we reject the null hypothesis if the test is declared significant and do not reject the null hypothesis if the test is non-significant. Now, summing each type of outcome over all \\(H_i (i = 1.,..,m)\\) yields the following random variables: Null hypothesis is true (H0) Alternative hypothesis is true (HA) Total Test is declared significant V S R Test is declared non-significant U T m - R Total \\(m_{0}\\) \\(m - m_0\\) m \\(m\\) is the total number hypotheses tested \\(m_{0}\\) is the number of true null hypotheses, an unknown parameter \\(m - m_0\\) is the number of true alternative hypotheses \\(V\\) is the number of false positives (Type I error) (also called false discoveries) \\(S\\) is the number of true positives (also called true discoveries) \\(T\\) is the number of false negatives (Type II error) \\(U\\) is the number of true negatives \\(R=V+S\\) is the number of rejected null hypotheses (also called discoveries, either true or false) 6.2.2 Using the predictmeans package Recall, rats_lm &lt;- lm(logAUC ~ Surgery, data = rats) coef(rats_lm) ## (Intercept) SurgeryP SurgeryS ## 8.4600 0.4900 3.0875 \\[ \\begin{aligned} \\operatorname{logAUC} &amp;= \\alpha + \\beta_{1}(\\operatorname{Surgery}_{\\operatorname{P}}) + \\beta_{2}(\\operatorname{Surgery}_{\\operatorname{S}}) + \\epsilon \\end{aligned} \\] Using the predictmeans package # Load predictmeans (assumes already installed) library(predictmeans) Fisher‚Äôs, Least Significant Difference (LSD) Carry out post-hoc tests only if the ANOVA F-test is significant. If so declare significant \\(100\\alpha\\%\\) any pairwise difference &gt; LSD. This does not control the FWER. tukey &lt;- predictmeans(rats_lm , modelterm = &quot;Surgery&quot;, adj = &quot;tukey&quot;,pairwise = TRUE) Bonferroni correction We reject the \\(H_0\\) for which the p-value, p-val, is p-val \\(&lt; \\alpha_c = \\frac{\\alpha_f}{n_c}\\) where \\(\\alpha_f\\) is the FWER and \\(n_c\\) is the number of pairwise comparisons. Howerer, this makes no assumptions about independence between tests. bonferroni &lt;- predictmeans(rats_lm , modelterm = &quot;Surgery&quot;, adj = &quot;bonferroni&quot;,pairwise = TRUE) 6.2.3 Multiple comparison procedures Tukey‚Äôs Honest Significant Difference (HSD) This compares the mean of every treatment with the mean of every other treatmen and uses a studentized range distribution compated with a t-distribution for Fisher's LSD and the Bonferroni correction. Here Tukey's studentixed range (TSR) \\(=q_{m,df}(1 - \\frac{\\alpha}{2})\\sqrt{2\\times \\frac{\\text{residual MS}}{\\text{# reps}}}\\) TukeyHSD(aov(logAUC~Surgery, data = rats)) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = logAUC ~ Surgery, data = rats) ## ## $Surgery ## diff lwr upr p adj ## P-C 0.4900 -1.1298813 2.109881 0.6863267 ## S-C 3.0875 1.4676187 4.707381 0.0012479 ## S-P 2.5975 0.9776187 4.217381 0.0039400 False Discovert Rate (FDR) The FDR controls the expected (mean) proportion of false discoveries amongst the \\(R\\) (out of \\(m\\)) hypotheses declared significant. Consider testing \\(m\\) null hypotheses with corresponding p-values \\(P_1, P_2,...,P_m\\); we then order then so that \\(P_{(1)} &lt; P_{(2)} &lt;...&lt;P_{(m)}\\) (where \\(P_{(i)}\\) is the \\(i^{th}\\) largest \\(i=1,...,m\\)). The \\(i^{th}\\) ordered p-value is calculated as \\(\\frac{i}{m}q^*\\) and the \\(i^{th}\\) null hypotesis is rejected if \\(P_i \\leq \\frac{i}{m}q^*\\) "],["factorial-experiments.html", "7 Factorial experiments 7.1 Learning objectives 7.2 Factorial design (as a CRD) 7.3 TL;DR, Model formula syntax in R", " 7 Factorial experiments 7.1 Learning objectives Describe and discuss factorial experiments with both equal and unequal replication Carry out linear regression in R with two categorical explanatory variables and an interaction (two-way ANOVA with interaction) and draw the appropriate inference Calculate the marginal means for a balanced and unbalanced design 7.2 Factorial design (as a CRD) Example Scientific Objective Global metabolic profiling and comparison of relative abundances of proteins in the inner and outer left ventricle wall of diabetic and healthy male Wistar rats. 7.2.1 Equal replications (balanced design) Analysis using lm() factorial &lt;- read_csv(&quot;factorial_expt.csv&quot;) Fitting models with interaction terms glimpse(factorial) ## Rows: 12 ## Columns: 5 ## $ Disease &lt;chr&gt; &quot;Healthy&quot;, &quot;Healthy&quot;, &quot;Healthy&quot;, &quot;Healthy&quot;, &quot;Healthy&quot;, &quot;Health‚Ä¶ ## $ Organ &lt;chr&gt; &quot;innerLV&quot;, &quot;outerLV&quot;, &quot;innerLV&quot;, &quot;outerLV&quot;, &quot;innerLV&quot;, &quot;outerL‚Ä¶ ## $ Animal &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 ## $ Sample &lt;dbl&gt; 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2 ## $ logAUC &lt;dbl&gt; 9.40, 8.83, 10.33, 10.49, 9.74, 10.98, 7.92, 9.37, 8.69, 11.31‚Ä¶ ## change to factors (saves errors with predictmeans) factorial$Disease &lt;- as.factor(factorial$Disease) factorial$Organ &lt;- as.factor(factorial$Organ) ## shorthand version fac_lm &lt;- lm(logAUC ~ Disease*Organ, data = factorial) ## longhand version fac_lm_lh &lt;- lm(logAUC ~ Disease + Organ +Disease:Organ, data = factorial) ## both are the SAME cbind(&quot;short hand&quot; = coef(fac_lm),&quot;long hand&quot; = coef(fac_lm_lh)) ## short hand long hand ## (Intercept) 7.873333 7.873333 ## DiseaseHealthy 1.950000 1.950000 ## OrganouterLV 2.116667 2.116667 ## DiseaseHealthy:OrganouterLV -1.840000 -1.840000 So the full model is \\[ \\begin{aligned} \\operatorname{logAUC} &amp;= \\alpha + \\beta_{1}(\\operatorname{Disease}_{\\operatorname{Healthy}}) + \\beta_{2}(\\operatorname{Organ}_{\\operatorname{outerLV}}) + \\beta_{3}(\\operatorname{Disease}_{\\operatorname{Healthy}} \\times \\operatorname{Organ}_{\\operatorname{outerLV}})\\ + \\\\ &amp;\\quad \\epsilon \\end{aligned} \\] And the gobal null hypotheses being tested are: \\(H_0: \\hat{\\mu}_{\\text{Diabetic}} = \\hat{\\mu}_{\\text{Healthy}}\\) \\(H_0: \\hat{\\mu}_{\\text{innerLV}} = \\hat{\\mu}_{\\text{outerLV}}\\) \\(H_0: \\hat{\\mu}_{\\text{Diabetic,innerLV}} = \\hat{\\mu}_{\\text{Diabetic,outerLV}} = \\hat{\\mu}_{\\text{Healthy,innerLV}} = \\hat{\\mu}_{\\text{Healthy,outerLV}}\\) anova(fac_lm) ## Analysis of Variance Table ## ## Response: logAUC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Disease 1 3.1827 3.1827 3.6304 0.09320 . ## Organ 1 4.2960 4.2960 4.9003 0.05775 . ## Disease:Organ 1 2.5392 2.5392 2.8963 0.12719 ## Residuals 8 7.0135 0.8767 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Plotting the fitted model Note with a balanced design ordering of term doesn't matter. For example, fac_lm &lt;- lm(logAUC ~ Disease*Organ, data = factorial) anova(fac_lm) ## Analysis of Variance Table ## ## Response: logAUC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Disease 1 3.1827 3.1827 3.6304 0.09320 . ## Organ 1 4.2960 4.2960 4.9003 0.05775 . ## Disease:Organ 1 2.5392 2.5392 2.8963 0.12719 ## Residuals 8 7.0135 0.8767 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 fac_lm_2 &lt;- lm(logAUC ~ Organ*Disease, data = factorial) anova(fac_lm_2) ## Analysis of Variance Table ## ## Response: logAUC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Organ 1 4.2960 4.2960 4.9003 0.05775 . ## Disease 1 3.1827 3.1827 3.6304 0.09320 . ## Organ:Disease 1 2.5392 2.5392 2.8963 0.12719 ## Residuals 8 7.0135 0.8767 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Inference using predictmeans interaction &lt;- predictmeans(fac_lm, modelterm = &quot;Disease:Organ&quot;, pairwise = TRUE) interaction$`Predicted Means` ## Organ innerLV outerLV ## Disease ## Diabetic 7.8733 9.9900 ## Healthy 9.8233 10.1000 interaction$`Standard Error of Means` ## All means have the same Stder ## 0.54058 interaction$`Pairwise LSDs` ## Diabetic:innerLV Diabetic:outerLV Healthy:innerLV ## Diabetic:innerLV 0.00000 -2.11667 -1.95000 ## Diabetic:outerLV 1.76294 0.00000 0.16667 ## Healthy:innerLV 1.76294 1.76294 0.00000 ## Healthy:outerLV 1.76294 1.76294 1.76294 ## Healthy:outerLV ## Diabetic:innerLV -2.22667 ## Diabetic:outerLV -0.11000 ## Healthy:innerLV -0.27667 ## Healthy:outerLV 0.00000 ## attr(,&quot;Significant level&quot;) ## [1] 0.05 ## attr(,&quot;Degree of freedom&quot;) ## [1] 8 ## attr(,&quot;Note&quot;) ## [1] &quot;LSDs matrix has mean differences (row-col) above the diagonal, LSDs (adjusted by &#39;none&#39; method) below the diagonal&quot; ## plot print(interaction$predictmeansPlot) 7.2.2 Unqual replications (unbalanced design) As per lecture slides let's set logAUC obvservations 1,2,3, 10 to NA unbalanced &lt;- factorial unbalanced$logAUC[c(1:3,10)] &lt;- NA unbalanced ## # A tibble: 12 x 5 ## Disease Organ Animal Sample logAUC ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Healthy innerLV 1 1 NA ## 2 Healthy outerLV 2 2 NA ## 3 Healthy innerLV 3 1 NA ## 4 Healthy outerLV 4 2 10.5 ## 5 Healthy innerLV 5 1 9.74 ## 6 Healthy outerLV 6 2 11.0 ## 7 Diabetic innerLV 7 1 7.92 ## 8 Diabetic outerLV 8 2 9.37 ## 9 Diabetic innerLV 9 1 8.69 ## 10 Diabetic outerLV 10 2 NA ## 11 Diabetic innerLV 11 1 7.01 ## 12 Diabetic outerLV 12 2 9.29 unbalanced_nafree &lt;- unbalanced %&gt;% drop_na() unbalanced_nafree ## # A tibble: 8 x 5 ## Disease Organ Animal Sample logAUC ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Healthy outerLV 4 2 10.5 ## 2 Healthy innerLV 5 1 9.74 ## 3 Healthy outerLV 6 2 11.0 ## 4 Diabetic innerLV 7 1 7.92 ## 5 Diabetic outerLV 8 2 9.37 ## 6 Diabetic innerLV 9 1 8.69 ## 7 Diabetic innerLV 11 1 7.01 ## 8 Diabetic outerLV 12 2 9.29 unbalanced_nafree %&gt;% group_by(Disease, Organ) %&gt;% tally() ## # A tibble: 4 x 3 ## # Groups: Disease [2] ## Disease Organ n ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 Diabetic innerLV 3 ## 2 Diabetic outerLV 2 ## 3 Healthy innerLV 1 ## 4 Healthy outerLV 2 Analysis using lm() Note: order matters. For example, fac_lm &lt;- lm(logAUC ~ Disease*Organ, data = unbalanced_nafree) anova(fac_lm) ## Analysis of Variance Table ## ## Response: logAUC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Disease 1 7.1102 7.1102 18.4955 0.01264 * ## Organ 1 3.1149 3.1149 8.1027 0.04656 * ## Disease:Organ 1 0.0913 0.0913 0.2376 0.65145 ## Residuals 4 1.5377 0.3844 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 fac_lm_2 &lt;- lm(logAUC ~ Organ*Disease, data = unbalanced_nafree) anova(fac_lm_2) ## Analysis of Variance Table ## ## Response: logAUC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Organ 1 5.7291 5.7291 14.9029 0.01814 * ## Disease 1 4.4960 4.4960 11.6953 0.02678 * ## Organ:Disease 1 0.0913 0.0913 0.2376 0.65145 ## Residuals 4 1.5377 0.3844 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.3 TL;DR, Model formula syntax in R In R to specify the model you want to fit you typically create a model formula object; this is usually then passed as the first argument to the model fitting function (e.g., lm()). Some notes on syntax: Consider the model formula example y ~ x + z + x:z. There is a lot going on here: The variable to the left of ~ specifies the response, everything to the right specify the explanatory variables + indicated to include the variable to the left of it and to the right of it (it does not mean they should be summed) : denotes the interaction of the variables to its left and right Additional, some other symbols have special meanings in model formula: * means to include all main effects and interactions, so a*b is the same as a + b + a:b ^ is used to include main effects and interactions up to a specified level. For example, (a + b + c)^2 is equivalent to a + b + c + a:b + a:c + b:c (note (a + b + c)^3 would also add a:b:c) - excludes terms that might otherwise be included. For example, -1 excludes the intercept otherwise included by default, and a*b - b would produce a + a:b Mathematical functions can also be directly used in the model formula to transform a variable directly (e.g., y ~ exp(x) + log(z) + x:z). One thing that may seem counter intuitive is in creating polynomial expressions (e.g., x2). Here the expression y ~ x^2 does not relate to squaring the explanatory variable x (this is to do with the syntax ^ you see above. To include x2 as a term in our model we have to use the I() (the &quot;as-is&quot; operator). For example, y ~ I(x^2)). "],["blocking-incorporating-into-design-and-analysis-of.html", "8 Blocking: incorporating into design and analysis of 8.1 Learning objectives 8.2 Blocking 8.3 A Randomised Controlled Block Design (RCBD) 8.4 Fixed or Random???", " 8 Blocking: incorporating into design and analysis of 8.1 Learning objectives Describe a Randomized Complete Block Design (RCBD) Describe a Split-plot Design (RCBD) Carry out analysis of a RCBD in R using aov(), lm(), and lmer() and discuss and compare the three Define a fixed and random effect in the context of experimental design 8.2 Blocking Recall Blocking helps control variability by making treatment groups more alike. Experimental units are divided into subsets (called blocks) so that units within the same block are more similar than units from different subsets or blocks. Blocking is a technique for dealing with nuisance factors. A nuisance factor is a factor that has some effect on the response, but is of no interest (e.g., age class). Key idea Partition known sources of variation which are unimportant to key scientific question(s) to improve precision of comparisons between treatment means. 8.3 A Randomised Controlled Block Design (RCBD) response = systematic component + error component 8.4 Fixed or Random??? Fixed effects Terms (parameters) in a statistical model which are fixed, or non-random, quantities. For example, Treatment group‚Äôs mean response: for the same Treatment, we expect this quantity to be the same from experiment to experiment. Terms with specific levels chosen for the experiment, and the primary aim is unbiased estimation of effects, should be allocated as fixed Random effects Terms (parameters) in a statistical model which are considered as random quantities or variables. Terms associated with the structure of the design should be allocated as random. Terms whose levels are a representative sample from a population, and where the variance of the population is of interest, should be allocated as random 8.4.1 Ignoring an effect You'll find the rcbd.csv file on CANVAS. library(tidyverse) rcbd &lt;- read_csv(&quot;rcbd.csv&quot;) ## Note: Run should be a factor rcbd$Run &lt;- as.factor(rcbd$Run) glimpse(rcbd) ## Rows: 12 ## Columns: 5 ## $ Run &lt;fct&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4 ## $ Surgery &lt;chr&gt; &quot;C&quot;, &quot;P&quot;, &quot;S&quot;, &quot;C&quot;, &quot;P&quot;, &quot;S&quot;, &quot;C&quot;, &quot;P&quot;, &quot;S&quot;, &quot;C&quot;, &quot;P&quot;, &quot;S&quot; ## $ Rat &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 ## $ logAUC8 &lt;dbl&gt; 9.24, 8.81, 10.75, 3.89, 8.62, 10.24, 8.42, 9.93, 11.68, 8.77,‚Ä¶ ## $ logAUC4 &lt;dbl&gt; 7.55, 9.34, 12.43, 6.59, 7.94, 8.56, 7.32, 8.37, 7.27, 8.24, 1‚Ä¶ One-way vs two-way... anova(lm(logAUC4 ~ Surgery, data = rcbd)) ## Analysis of Variance Table ## ## Response: logAUC4 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Surgery 2 14.662 7.3308 2.5034 0.1366 ## Residuals 9 26.355 2.9284 lm2 &lt;- lm(logAUC4 ~ Run + Surgery, data = rcbd) anova(lm2) ## Analysis of Variance Table ## ## Response: logAUC4 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Run 3 18.1317 6.0439 4.4097 0.05812 . ## Surgery 2 14.6617 7.3308 5.3487 0.04640 * ## Residuals 6 8.2235 1.3706 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Always check assumptions gglm::gglm(lm2) 8.4.2 Analysis using lmer() from lme4 library(lme4) lmer_mod &lt;- lmer(logAUC8 ~ Surgery + (1|Run), data = rcbd) summary(lmer_mod) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: logAUC8 ~ Surgery + (1 | Run) ## Data: rcbd ## ## REML criterion at convergence: 37.2 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.8525 -0.2273 0.1772 0.4036 1.3309 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Run (Intercept) 1.479 1.216 ## Residual 1.447 1.203 ## Number of obs: 12, groups: Run, 4 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 7.5800 0.8552 8.863 ## SurgeryP 1.9750 0.8506 2.322 ## SurgeryS 3.8500 0.8506 4.526 ## ## Correlation of Fixed Effects: ## (Intr) SrgryP ## SurgeryP -0.497 ## SurgeryS -0.497 0.500 Now what about the Random effects We have two variance components Between Groups (Runs) \\(\\hat{\\sigma^2}_{\\text{Run}}\\) = 1.479 Within Runs (between observations) \\(\\hat{\\sigma_2}\\) = 1.447 Note that aov() presents the same information, but in a different way: summary(aov(logAUC8 ~ Surgery + Error(Run), data = rcbd)) ## ## Error: Run ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals 3 17.65 5.883 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Surgery 2 29.652 14.826 10.25 0.0116 * ## Residuals 6 8.682 1.447 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Within Runs (Residuals) \\(\\hat{\\sigma}_2\\) = 1.447 (same as lmer) Between Run variance = \\(\\hat{\\sigma}^2\\) + \\(3\\:\\hat{\\sigma}^2_{\\text{Run}}\\) so \\(\\hat{\\sigma}^2_{\\text{Run}} = \\frac{5.883 - \\hat{\\sigma}^2 }{3} = \\frac{5.883 - 1.447}{3} = 1.479\\) 8.4.2.1 SEMs &amp; SEDs library(predictmeans) pred_means &lt;- predictmeans(lmer_mod, modelterm = &quot;Surgery&quot;, pairwise = TRUE, plot = FALSE) pred_means ## $`Predicted Means` ## Surgery ## C P S ## 7.580 9.555 11.430 ## ## $`Standard Error of Means` ## All means have the same Stder ## 0.85524 ## ## $`Standard Error of Differences` ## Max.SED Min.SED Aveg.SED ## 0.8505913 0.8505913 0.8505913 ## ## $LSD ## Max.LSD Min.LSD Aveg.LSD ## 2.08132 2.08132 2.08132 ## attr(,&quot;Significant level&quot;) ## [1] 0.05 ## attr(,&quot;Degree of freedom&quot;) ## [1] 6 ## ## $`Pairwise LSDs` ## C P S ## C 0.00000 -1.97500 -3.850 ## P 2.08132 0.00000 -1.875 ## S 2.08132 2.08132 0.000 ## attr(,&quot;Significant level&quot;) ## [1] 0.05 ## attr(,&quot;Degree of freedom&quot;) ## [1] 6 ## attr(,&quot;Note&quot;) ## [1] &quot;LSDs matrix has mean differences (row-col) above the diagonal, LSDs (adjusted by &#39;none&#39; method) below the diagonal&quot; ## ## $`Pairwise p-value` ## C P S ## C 0.0000 -2.3219 -4.5263 ## P 0.0593 0.0000 -2.2043 ## S 0.0040 0.0697 0.0000 ## attr(,&quot;Degree of freedom&quot;) ## [1] 6 ## attr(,&quot;Note&quot;) ## [1] &quot;The matrix has t-value above the diagonal, p-value (adjusted by &#39;none&#39; method) below the diagonal&quot; ## attr(,&quot;Letter-based representation of pairwise comparisons at significant level &#39;0.05&#39;&quot;) ## Treatment Mean Group ## 1 S 11.430 A ## 2 P 9.555 AB ## 3 C 7.580 B ## ## $mean_table ## Surgery Predicted means Standard error Df LL of 95% CI UL of 95% CI ## 1 C 7.580 0.8552404 6 5.487302 9.672698 ## 2 P 9.555 0.8552404 6 7.462302 11.647698 ## 3 S 11.430 0.8552404 6 9.337302 13.522698 "],["split-plot-and-repeated-measures-designs.html", "9 Split-plot and repeated measures designs 9.1 Learning objectives 9.2 Analysis of a split-plot design 9.3 Analysis of a repeated measures design 9.4 Repeated measures designs as split-plots in time", " 9 Split-plot and repeated measures designs 9.1 Learning objectives Describe and discuss split-plot experimental design Describe and discuss repeated measures experiments Write R code to visualise repeated measures data Carry out appropriate analysis in R and draw the appropriate inference 9.2 Analysis of a split-plot design Recall the diabetic and healthy male Wistar rats... BUT what about Data available on CANVAS split_plot &lt;- read_csv(&quot;split_plot.csv&quot;) ## recall we need to set factors split_plot$Animal &lt;- factor(split_plot$Animal) split_plot$Sample &lt;- factor(split_plot$Sample) split_plot ## # A tibble: 12 x 5 ## Disease Organ Animal Sample logAUC ## &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 Healthy innerLV 1 1 9.4 ## 2 Healthy outerLV 1 2 8.83 ## 3 Healthy innerLV 2 1 10.3 ## 4 Healthy outerLV 2 2 10.5 ## 5 Healthy innerLV 3 1 9.74 ## 6 Healthy outerLV 3 2 11.0 ## 7 Diabetic innerLV 4 1 7.92 ## 8 Diabetic outerLV 4 2 9.37 ## 9 Diabetic innerLV 5 1 8.69 ## 10 Diabetic outerLV 5 2 11.3 ## 11 Diabetic innerLV 6 1 7.01 ## 12 Diabetic outerLV 6 2 9.29 9.2.1 Using aov() sp_aov &lt;- aov(logAUC ~ Disease*Organ + Error(Animal/Sample), data = split_plot) summary(sp_aov) ## ## Error: Animal ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Disease 1 3.183 3.183 2.187 0.213 ## Residuals 4 5.822 1.456 ## ## Error: Animal:Sample ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Organ 1 4.296 4.296 14.423 0.0191 * ## Disease:Organ 1 2.539 2.539 8.525 0.0433 * ## Residuals 4 1.191 0.298 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 9.2.2 Using lmer() (from lmeTest and lmer4) and predictmeans() library(lmerTest) ## MUST LOAD THIS ## ## Attaching package: &#39;lmerTest&#39; ## The following object is masked from &#39;package:lme4&#39;: ## ## lmer ## The following object is masked from &#39;package:stats&#39;: ## ## step sp_lmer &lt;- lmerTest::lmer(logAUC ~ Disease*Organ + (1|Animal), data = split_plot) ## MUST SPECIFY WHICH PACKAGE anova(sp_lmer,type = 2) ## Type II Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## Disease 0.6513 0.6513 1 4 2.1866 0.21329 ## Organ 4.2960 4.2960 1 4 14.4227 0.01914 * ## Disease:Organ 2.5392 2.5392 1 4 8.5246 0.04326 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 library(lme4) library(predictmeans) sp_lmer4 &lt;- lme4::lmer(logAUC ~ Disease*Organ + (1|Animal), data = split_plot) ## MUST SPECIFY WHICH PACKAGE sp_predmeans &lt;- predictmeans(sp_lmer4,modelterm = &quot;Disease:Organ&quot;, pairwise = TRUE, plot = FALSE) sp_predmeans ## $`Predicted Means` ## Organ innerLV outerLV ## Disease ## Diabetic 7.8733 9.9900 ## Healthy 9.8233 10.1000 ## ## $`Standard Error of Means` ## All means have the same Stder ## 0.54058 ## ## $`Standard Error of Differences` ## Max.SED Min.SED Aveg.SED ## 0.7645006 0.4456206 0.6582073 ## attr(,&quot;For the Same Level of Factor&quot;) ## Disease Organ ## Aveg.SED 0.4456206 0.7645006 ## Min.SED 0.4456206 0.7645006 ## Max.SED 0.4456206 0.7645006 ## ## $LSD ## Max.LSD Min.LSD Aveg.LSD ## 2.12259 1.23724 1.82748 ## attr(,&quot;Significant level&quot;) ## [1] 0.05 ## attr(,&quot;Degree of freedom&quot;) ## [1] 4 ## ## $`Pairwise LSDs` ## Diabetic:innerLV Diabetic:outerLV Healthy:innerLV ## Diabetic:innerLV 0.00000 -2.11667 -1.95000 ## Diabetic:outerLV 1.23724 0.00000 0.16667 ## Healthy:innerLV 2.12259 2.12259 0.00000 ## Healthy:outerLV 2.12259 2.12259 1.23724 ## Healthy:outerLV ## Diabetic:innerLV -2.22667 ## Diabetic:outerLV -0.11000 ## Healthy:innerLV -0.27667 ## Healthy:outerLV 0.00000 ## attr(,&quot;Significant level&quot;) ## [1] 0.05 ## attr(,&quot;Degree of freedom&quot;) ## [1] 4 ## attr(,&quot;Note&quot;) ## [1] &quot;LSDs matrix has mean differences (row-col) above the diagonal, LSDs (adjusted by &#39;none&#39; method) below the diagonal&quot; ## ## $`Pairwise p-value` ## Diabetic:innerLV Diabetic:outerLV Healthy:innerLV ## Diabetic:innerLV 0.0000 -4.7499 -2.5507 ## Diabetic:outerLV 0.0090 0.0000 0.2180 ## Healthy:innerLV 0.0633 0.8381 0.0000 ## Healthy:outerLV 0.0436 0.8925 0.5683 ## Healthy:outerLV ## Diabetic:innerLV -2.9126 ## Diabetic:outerLV -0.1439 ## Healthy:innerLV -0.6209 ## Healthy:outerLV 0.0000 ## attr(,&quot;Degree of freedom&quot;) ## [1] 4 ## attr(,&quot;Note&quot;) ## [1] &quot;The matrix has t-value above the diagonal, p-value (adjusted by &#39;none&#39; method) below the diagonal&quot; ## attr(,&quot;Letter-based representation of pairwise comparisons at significant level &#39;0.05&#39;&quot;) ## Treatment Mean Group ## 1 Healthy:outerLV 10.100000 A ## 2 Diabetic:outerLV 9.990000 A ## 3 Healthy:innerLV 9.823333 AB ## 4 Diabetic:innerLV 7.873333 B ## ## $mean_table ## Disease Organ Predicted means Standard error Df LL of 95% CI UL of 95% CI ## 1 Diabetic innerLV 7.873333 0.5405835 4 6.372433 9.374234 ## 2 Diabetic outerLV 9.990000 0.5405835 4 8.489099 11.490901 ## 3 Healthy innerLV 9.823333 0.5405835 4 8.322433 11.324234 ## 4 Healthy outerLV 10.100000 0.5405835 4 8.599099 11.600901 9.3 Analysis of a repeated measures design We have (balanced) data with the same number of observations on each rat at the same time points. 9.3.1 The data Data available on CANVAS library(tidyverse) liver &lt;- read_csv(&quot;repeated_measures_liver.csv&quot;) ## change time to factor liver$Time &lt;- as.factor(liver$Time) glimpse(liver) ## Rows: 210 ## Columns: 4 ## $ Animal &lt;chr&gt; &quot;Control1&quot;, &quot;Control1&quot;, &quot;Control1&quot;, &quot;Control1&quot;, &quot;Control1&quot;, ‚Ä¶ ## $ Treatment &lt;chr&gt; &quot;Control&quot;, &quot;Control&quot;, &quot;Control&quot;, &quot;Control&quot;, &quot;Control&quot;, &quot;Cont‚Ä¶ ## $ Time &lt;fct&gt; 0, 5, 10, 15, 20, 25, 30, 0, 5, 10, 15, 20, 25, 30, 0, 5, 10‚Ä¶ ## $ Glucose &lt;dbl&gt; 1.599, 1.279, 1.599, 1.579, 1.279, 1.439, 1.179, 0.840, 0.64‚Ä¶ 9.3.2 Visualise plot data ggplot(liver, aes(x = Time, y = Glucose, color = Treatment, group = Animal)) + geom_line(size = 1) + geom_point(shape = 1) + theme_bw() plot group means liver_means &lt;- liver %&gt;% group_by(Time,Treatment) %&gt;% summarise(Glucose = mean(Glucose)) ## `summarise()` has grouped output by &#39;Time&#39;. You can override using the `.groups` argument. ggplot(liver_means, aes(x = Time, y = Glucose, color = Treatment, group = Treatment)) + geom_line(size = 1) + geom_point(shape = 1) + theme_bw() linear? ggplot(liver_means, aes(x = Time, y = Glucose, color = Treatment, group = Treatment)) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_point(shape = 1) + theme_bw() ## `geom_smooth()` using formula &#39;y ~ x&#39; 9.3.3 Using aov() re_aov &lt;- aov(Glucose ~ Treatment*Time + Error(Animal),data = liver) summary(re_aov) ## ## Error: Animal ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Treatment 3 1.90 0.6335 1.407 0.263 ## Residuals 26 11.71 0.4503 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Time 6 0.7973 0.13289 8.732 0.0000000334 *** ## Treatment:Time 18 0.2539 0.01411 0.927 0.547 ## Residuals 156 2.3741 0.01522 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 9.3.4 Using lmer() (from lmerTest and lme4) and predictmeans() library(lmerTest) ## MUST LOAD THIS re_lmer &lt;- lmerTest::lmer(Glucose ~ Treatment*Time + (1|Animal),data = liver) ## MUST SPECIFY WHICH PACKAGE anova(re_lmer,type = 2) ## Type II Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## Treatment 0.06423 0.021409 3 26 1.4068 0.2632 ## Time 0.79731 0.132885 6 156 8.7318 0.00000003345 *** ## Treatment:Time 0.25390 0.014105 18 156 0.9269 0.5474 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 library(lme4) library(predictmeans) re_lmer4 &lt;- lme4::lmer(Glucose ~ Treatment*Time + (1|Animal),data = liver) ## MUST SPECIFY WHICH PACKAGE predictmeans::residplot(re_lmer4) sp_predmeans &lt;- predictmeans(re_lmer4,modelterm = &quot;Time&quot;, pairwise = TRUE) sp_predmeans ## $`Predicted Means` ## Time ## 0 5 10 15 20 25 30 ## 0.6470 0.5885 0.5668 0.5361 0.4977 0.5023 0.4420 ## ## $`Standard Error of Means` ## All means have the same Stder ## 0.0509 ## ## $`Standard Error of Differences` ## Max.SED Min.SED Aveg.SED ## 0.03192336 0.03192336 0.03192336 ## ## $LSD ## Max.LSD Min.LSD Aveg.LSD ## 0.06306 0.06306 0.06306 ## attr(,&quot;Significant level&quot;) ## [1] 0.05 ## attr(,&quot;Degree of freedom&quot;) ## [1] 156 ## ## $`Pairwise LSDs` ## 0 5 10 15 20 25 30 ## 0 0.00000 0.05842 0.08021 0.11086 0.14925 0.14464 0.20498 ## 5 0.06306 0.00000 0.02178 0.05244 0.09083 0.08621 0.14656 ## 10 0.06306 0.06306 0.00000 0.03066 0.06904 0.06443 0.12478 ## 15 0.06306 0.06306 0.06306 0.00000 0.03839 0.03378 0.09412 ## 20 0.06306 0.06306 0.06306 0.06306 0.00000 -0.00461 0.05573 ## 25 0.06306 0.06306 0.06306 0.06306 0.06306 0.00000 0.06034 ## 30 0.06306 0.06306 0.06306 0.06306 0.06306 0.06306 0.00000 ## attr(,&quot;Significant level&quot;) ## [1] 0.05 ## attr(,&quot;Degree of freedom&quot;) ## [1] 156 ## attr(,&quot;Note&quot;) ## [1] &quot;LSDs matrix has mean differences (row-col) above the diagonal, LSDs (adjusted by &#39;none&#39; method) below the diagonal&quot; ## ## $`Pairwise p-value` ## 0 5 10 15 20 25 30 ## 0 0.0000 1.8301 2.5124 3.4727 4.6753 4.5308 6.4211 ## 5 0.0691 0.0000 0.6823 1.6426 2.8451 2.7007 4.5909 ## 10 0.0130 0.4961 0.0000 0.9603 2.1628 2.0184 3.9086 ## 15 0.0007 0.1025 0.3384 0.0000 1.2025 1.0581 2.9483 ## 20 0.0000 0.0050 0.0321 0.2310 0.0000 -0.1445 1.7458 ## 25 0.0000 0.0077 0.0453 0.2917 0.8853 0.0000 1.8903 ## 30 0.0000 0.0000 0.0001 0.0037 0.0828 0.0606 0.0000 ## attr(,&quot;Degree of freedom&quot;) ## [1] 156 ## attr(,&quot;Note&quot;) ## [1] &quot;The matrix has t-value above the diagonal, p-value (adjusted by &#39;none&#39; method) below the diagonal&quot; ## attr(,&quot;Letter-based representation of pairwise comparisons at significant level &#39;0.05&#39;&quot;) ## Treatment Mean Group ## 1 0 0.6469732 A ## 2 5 0.5885491 AB ## 3 10 0.5667679 B ## 4 15 0.5361116 BC ## 5 25 0.5023348 CD ## 6 20 0.4977232 CD ## 7 30 0.4419911 D ## ## $mean_table ## Time Predicted means Standard error Df LL of 95% CI UL of 95% CI ## 1 0 0.6469732 0.05089815 156 0.5464347 0.7475117 ## 2 5 0.5885491 0.05089815 156 0.4880106 0.6890876 ## 3 10 0.5667679 0.05089815 156 0.4662294 0.6673063 ## 4 15 0.5361116 0.05089815 156 0.4355731 0.6366501 ## 5 20 0.4977232 0.05089815 156 0.3971847 0.5982617 ## 6 25 0.5023348 0.05089815 156 0.4017963 0.6028733 ## 7 30 0.4419911 0.05089815 156 0.3414526 0.5425296 9.4 Repeated measures designs as split-plots in time In this set of balanced data we have + Same number of observations on each rat + At the same time points We could analyse as a split-plot design, where + plots = different rats + subplots = different times within a rat Here we would have two error components + Between rats + Between times within rats There are, however, two big differences between Repeated Measures and Split-plot Designs 1. Cannot randomise the levels of time + Measurement 2 hours, comes after that at 1 hour 2. Split plot design block structure + Imposes the same correlation on any two subplots in the the same plot + Implausible for repeated measures designs as observations further apart in time likely to be less correlated than observations close together "],["statistical-inference.html", "10 Statistical Inference 10.1 Learning Objectives 10.2 Regression 10.3 Model, comparison, selection, and checking (again) 10.4 Point predictions and confidence intervals 10.5 TL;DR lm() 10.6 Other resources: optional but recommended 10.7 Beyond Linear Models to Generalised Linear Models (GLMs) (not examinable)", " 10 Statistical Inference 10.1 Learning Objectives Carry out and interpret tests for the existence of relationships between explanatory variables and the response in a linear model Write R code to fit a linear model with a single continuous explanatory variable Write R code to fit a linear model with a continuous explanatory variable and a factor explanatory variable Interpret estimated effects with reference to confidence intervals from linear regression models. Specifically the interpretation of the intercept the effect of a factor the effect of a one-unit increase in a numeric variable the effect of an x-unit increase in a numeric variable Make a point prediction of the response for a new observation Write R code to fit a linear model with interaction terms in the explanatory variables Interpret estimated effects with reference to confidence intervals from linear regression models. Specifically the interpretation of main effects in a model with an interaction the effect of one variable when others are included in the model Explain why you may want to include interaction effects in a linear model Describe the differences between the operators : and * in an R model-fitting formula 10.2 Regression 10.2.1 Some mathematical notation Let's consider a linear regression with a simple explanatory variable: \\[Y_i = \\alpha + \\beta_1x_i + \\epsilon_i\\] where \\[\\epsilon_i \\sim \\text{Normal}(0,\\sigma^2).\\] Here for observation \\(i\\) \\(Y_i\\) is the value of the response \\(x_i\\) is the value of the explanatory variable \\(\\epsilon_i\\) is the error term: the difference between \\(Y_i\\) and its expected value \\(\\alpha\\) is the intercept term (a parameter to be estimated), and \\(\\beta_1\\) is the slope: coefficient of the explanatory variable (a parameter to be estimated) Does this remind you of anything? 10.2.2 Modeling Bill Depth Remember the penguins from Chapter 2? Key assumptions Independence There is a linear relationship between the response and the explanatory variables The residuals have constant variance The residuals are normally distributed library(tidyverse) library(palmerpenguins) penguins_nafree &lt;- penguins %&gt;% drop_na() ggplot(data = penguins_nafree, aes(x = bill_depth_mm)) + geom_histogram() + theme_classic() + xlab(&quot;Bill depth (mm)&quot;) First off let's fit a null (intercept only) model. This in old money would be called a one sample t-test. slm_null &lt;- lm(bill_depth_mm ~ 1, data = penguins_nafree) summary(slm_null)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 17.16486 0.1079134 159.0614 1.965076e-315 Model formula This model, from above, is simply \\[Y_i = \\alpha + \\epsilon_i.\\] Here for observation \\(i\\) \\(Y_i\\) is the value of the response (bill_depth_mm) and \\(\\alpha\\) is a parameter to be estimated (typically called the intercept). Inference The (Intercept) term, 17.1648649, tells us the (estimated) average value of the response (bill_depth_mm), see penguins_nafree %&gt;% summarise(average_bill_depth = mean(bill_depth_mm)) ## # A tibble: 1 x 1 ## average_bill_depth ## &lt;dbl&gt; ## 1 17.2 The SEM (Std. Error) = 0.1079134. The hypothesis being tested is \\(H_0:\\) ((Intercept) ) \\(\\text{mean}_{\\text{`average_bill_depth`}} = 0\\) vs. \\(H_1:\\) ((Intercept)) \\(\\text{mean}_{\\text{`average_bill_depth`}} \\neq 0\\) The t-statistic is given by t value = Estimate / Std. Error = 159.0614207 The p-value is given byPr (&gt;|t|) = 1.965076110^{-315}. So the probability of observing a t-statistic as least as extreme given under the null hypothesis (average bill depth = 0) given our data is 1.965076110^{-315}, pretty strong evidence against the null hypothesis I'd say! 10.2.3 Single continuous variable Does bill_length_mm help explain some of the variation in bill_depth_mm? p1 &lt;- ggplot(data = penguins_nafree, aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + ylab(&quot;Bill depth (mm)&quot;) + xlab(&quot;Bill length (mm)&quot;) + theme_classic() p1 slm &lt;- lm(bill_depth_mm ~ bill_length_mm, data = penguins_nafree) Model formula This model is simply \\[Y_i = \\alpha + \\beta_1x_i + \\epsilon_i\\] where for observation \\(i\\) \\(Y_i\\) is the value of the response (bill_depth_mm) and \\(x_i\\) is the value of the explanatory variable (bill_length_mm); As above \\(\\alpha\\) and \\(\\beta_1\\) are parameters to be estimated. We could also write this model as \\[ \\begin{aligned} \\operatorname{bill\\_depth\\_mm} &amp;= \\alpha + \\beta_{1}(\\operatorname{bill\\_length\\_mm}) + \\epsilon \\end{aligned} \\] Fitted model As before we can get out estimated parameters (here \\(\\alpha\\) and \\(\\beta_1\\)) using summary(slm)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.78664867 0.85417308 24.335406 1.026904e-75 ## bill_length_mm -0.08232675 0.01926835 -4.272642 2.528290e-05 Here, the (Intercept): Estimate (\\(\\alpha\\) above) gives us the estimated average bill depth (mm) given the estimated relationship bill length (mm) and bill length. The bill_length_mm : Estimate (\\(\\beta_1\\) above) is the slope associated with bill length (mm). So, here for every 1mm increase in bill length we estimated a 0.082mm decrease (or a -0.082mm increase) in bill depth. ## calculate predicted values penguins_nafree$pred_vals &lt;- predict(slm) ## plot ggplot(data = penguins_nafree, aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + ylab(&quot;Bill depth (mm)&quot;) + xlab(&quot;Bill length (mm)&quot;) + theme_classic() + geom_line(aes(y = pred_vals)) 10.2.4 Factor and a continous variable Adding species; remember species is a factor variable! p2 &lt;- ggplot(data = penguins_nafree, aes(y = bill_depth_mm, x = bill_length_mm, color = species)) + geom_point() + ylab(&quot;Bill depth (mm)&quot;) + xlab(&quot;Bill length (mm)&quot;) + theme_classic() p2 slm_sp &lt;- lm(bill_depth_mm ~ bill_length_mm + species, data = penguins_nafree) Model formula Now we have two explanatory variables, so our model formula becomes \\[Y_i = \\beta_0 + \\beta_1z_i + \\beta_2x_i + \\epsilon_i\\] \\[\\epsilon_i \\sim \\text{Normal}(0,\\sigma^2)\\] where for observation \\(i\\) \\(Y_i\\) is the value of the response (bill_depth_mm) \\(z_i\\) is one explanatory variable (bill_length_mm say) \\(x_i\\) is another explanatory variable (species say) \\(\\epsilon_i\\) is the error term: the difference between \\(Y_i\\) and its expected value \\(\\alpha\\), \\(\\beta_1\\), and \\(\\beta_2\\) are all parameters to be estimated. Remember though that when we have factor explanatory variables (e.g., species) we have to use dummy variables, see lecture. Here the Adelie group are the baseline (R does this alphabetically, to change this see previous chapter). So model formula is \\[ \\begin{aligned} \\operatorname{bill\\_depth\\_mm} &amp;= \\alpha + \\beta_{1}(\\operatorname{bill\\_length\\_mm}) + \\beta_{2}(\\operatorname{species}_{\\operatorname{Chinstrap}}) + \\beta_{3}(\\operatorname{species}_{\\operatorname{Gentoo}})\\ + \\\\ &amp;\\quad \\epsilon \\end{aligned} \\] Fitted model summary(slm_sp)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.5652616 0.69092642 15.291442 2.977289e-40 ## bill_length_mm 0.2004431 0.01767974 11.337449 2.258955e-25 ## speciesChinstrap -1.9330779 0.22571878 -8.564099 4.259893e-16 ## speciesGentoo -5.1033153 0.19439523 -26.252267 1.043789e-82 Simpson's paradox... look how the slope associated with bill length (coefficient of bill_length_mm) has switched direction from the model above! Why do you think this is? Here, the (Intercept): Estimate gives us the estimated average bill depth (mm) of the Adelie penguins given the other variables in the model. The bill_length_mm : Estimate (\\(\\beta_1\\) above) is the slope associated with bill length (mm). So, here for every 1mm increase in bill length we estimated a 0.2mm increase in bill depth. What about the coefficient of the other species levels? Look at the plot below, these values give the shift (up or down) of the parallel lines from the Adelie level. So given the estimated relationship between bill depth and bill length these coefficients are the estimated change from the baseline. ## calculate predicted values penguins_nafree$pred_vals &lt;- predict(slm_sp) ## plot ggplot(data = penguins_nafree, aes(y = bill_depth_mm, x = bill_length_mm, color = species)) + geom_point() + ylab(&quot;Bill depth (mm)&quot;) + xlab(&quot;Bill length (mm)&quot;) + theme_classic() + geom_line(aes(y = pred_vals)) 10.2.5 Interactions Recall the (additive) model formula from above \\[Y_i = \\beta_0 + \\beta_1z_i + \\beta_2x_i + \\epsilon_i\\] but what about interactions between variables? For example, \\[Y_i = \\beta_0 + \\beta_1z_i + \\beta_2x_i + \\beta_3z_ix_i + \\epsilon_i\\] Note: to include interaction effects in our model by using either the * or : syntax in our model formula. For example, : denotes the interaction of the variables to its left and right, and * means to include all main effects and interactions, so a*b is the same as a + b + a:b. See Model formula syntax for further details. To specify a model with additive and interaction effects we use slm_int &lt;- lm(bill_depth_mm ~ bill_length_mm*species, data = penguins_nafree) Model formula The model formula is then \\[ \\begin{aligned} \\operatorname{bill\\_depth\\_mm} &amp;= \\alpha + \\beta_{1}(\\operatorname{bill\\_length\\_mm}) + \\beta_{2}(\\operatorname{species}_{\\operatorname{Chinstrap}}) + \\beta_{3}(\\operatorname{species}_{\\operatorname{Gentoo}})\\ + \\\\ &amp;\\quad \\beta_{4}(\\operatorname{bill\\_length\\_mm} \\times \\operatorname{species}_{\\operatorname{Chinstrap}}) + \\beta_{5}(\\operatorname{bill\\_length\\_mm} \\times \\operatorname{species}_{\\operatorname{Gentoo}}) + \\epsilon \\end{aligned} \\] Fitted model summary(slm_int)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.48770713 1.15987305 9.9042797 2.135979e-20 ## bill_length_mm 0.17668344 0.02980564 5.9278518 7.793199e-09 ## speciesChinstrap -3.91856701 2.06730876 -1.8954919 5.890889e-02 ## speciesGentoo -6.36675118 1.77989710 -3.5770333 4.000274e-04 ## bill_length_mm:speciesChinstrap 0.04552828 0.04594283 0.9909769 3.224296e-01 ## bill_length_mm:speciesGentoo 0.03092816 0.04111608 0.7522157 4.524625e-01 As before the (Intercept): Estimate gives us the estimated average bill depth (mm) of the Adelie penguins given the other variables in the model. The bill_length_mm : Estimate (\\(\\beta_1\\) above) is the slope associated with bill length (mm). So, here for every 1mm increase in bill length we estimated a 0.177mm increase in bill depth. The main effects of species (i.e., speciesChinstrap: Estimate and speciesGentoo:Estimate ) again give the shift (up or down) of the lines from the Adelie level; however these lines are no longer parallel! The interaction terms (i.e., bill_length_mm:speciesChinstrap and bill_length_mm:speciesGentoo) specify the species specific slopes given the other variables in the model. Look at the plot below. Now we've specified this all singing and dancing interaction model we might ask are the non-parallel lines non-parallel enough to reject the parallel line model? ## calculate predicted values penguins_nafree$pred_vals &lt;- predict(slm_int) ## plot ggplot(data = penguins_nafree, aes(y = bill_depth_mm, x = bill_length_mm, color = species)) + geom_point() + ylab(&quot;Bill depth (mm)&quot;) + xlab(&quot;Bill length (mm)&quot;) + theme_classic() + geom_line(aes(y = pred_vals)) 10.3 Model, comparison, selection, and checking (again) Remember that it is always is imperative that we check the underlying assumptions of our model! If our assumptions are not met then basically the maths falls over and we can't reliably draw inference from the model (e.g., can't trust the parameter estimates etc.). Two of the most important assumption are: equal variances (homogeneity of variance), and normality of residuals. Let's look at the fit of the slm model (single continuous explanatory variable) gglm::gglm(slm) # Plot the four main diagnostic plots Do you think the residuals are Normally distributed (look at the QQ plot)? Think of what this model is, do you think it's the best we can do? 10.3.1 Model comparison and selection Are the non-parallel lines non-parallel enough to reject the parallel line model? Now we can compare nested linear models by hypothesis testing. Luckily the R function anova() automates this. Yes the same idea as we've previously learnt about ANOVA! We essentially perform an F-ratio test between the nested models! By nested we mean that one model is a subset of the other (i.e., where some coefficients have been fixed at zero). For example, \\[Y_i = \\beta_0 + \\beta_1z_i + \\epsilon_i\\] is a nested version of \\[Y_i = \\beta_0 + \\beta_1z_i + \\beta_2x_i + \\epsilon_i\\] where \\(\\beta_2\\) has been fixed to zero. As an example consider testing the single explanatory variable model slm against the same model with species included as a variable slm_sp. To carry out the appropriate hypothesis test in R we can run anova(slm,slm_sp) ## Analysis of Variance Table ## ## Model 1: bill_depth_mm ~ bill_length_mm ## Model 2: bill_depth_mm ~ bill_length_mm + species ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 331 1220.16 ## 2 329 299.62 2 920.55 505.41 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 As you'll see the anova() function takes the two model objects (slm and slm_sp) each as arguments. It returns an ANOVA testing whether the more complex model (slm_sp) is just as good at capturing the variation in the data as the simpler model (slm). The returned p-value should be interpreted as in any other hypothesis test. i.e., the probability of observing a statistic as least as extreme under our null hypothesis (here that each model is as good at capturing the variation in the data). What would we conclude here? I'd say we have pretty strong evidence against the models being equally good! I'd definitely plump for slm_sp over slm, looking back at the plots above does this make sense? Now what about slm_int vs slm_sp? anova(slm_sp,slm_int) ## Analysis of Variance Table ## ## Model 1: bill_depth_mm ~ bill_length_mm + species ## Model 2: bill_depth_mm ~ bill_length_mm * species ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 329 299.62 ## 2 327 298.62 2 0.99284 0.5436 0.5812 So it seems both models are just as good at capturing the variation in our data: we're happy with the parallel lines! Another way we might compare models is by using the Akaike information criterion (AIC) (you'll see more of this later in the course). AIC is an estimator of out-of-sample prediction error and can be used as a metric to choose between competing models. Between nested models we're looking for the smallest AIC (i.e., smallest out-of-sample prediction error). Typically, a difference of 4 or more is considered to indicate an improvement; this should not be taken as writ however, using multiple comparison techniques is advised. R already has an AIC() function that can be used directly on your lm() model object(s). For example, AIC(slm,slm_sp,slm_int) ## df AIC ## slm 3 1383.4462 ## slm_sp 5 919.8347 ## slm_int 7 922.7294 This backs up what our ANOVA suggested model slm_sp as that preferred! As always it's important to do a sanity check! Does this make sense? Have a look at the outputs from these models and see what you think. Just because we've chosen a model (the best of a bad bunch perhaps) this doesn't let us off the hook. We should check our assumptions gglm::gglm(slm_sp) # Plot the four main diagnostic plots Residuals vs Fitted plot: equal spread? Doesn't look too trumpety! Normal quantile-quantile (QQ) plot: skewed? Maybe slightly right skewed (deviation upwards from the right tail) Scale-Location plot: equal spared? I'd say so. Residuals vs Leverage: ? Maybe a couple of points with high leverage. 10.4 Point predictions and confidence intervals After all that what do estimated parameters mean? 10.4.1 Confidence intervals for parameters For the chosen slm_sp model we can get these simply by using cis &lt;- confint(slm_sp) cis ## 2.5 % 97.5 % ## (Intercept) 9.2060707 11.9244526 ## bill_length_mm 0.1656635 0.2352227 ## speciesChinstrap -2.3771120 -1.4890438 ## speciesGentoo -5.4857298 -4.7209009 By default the 95% intervals are returned (see previous lecture) So this tells us that For every 1mm increase in bill length we estimate the expected bill depth to increases between 0.166 and 0.235 mm We estimate that the expected bill depth of a Chinstrap penguin is between 1.5 and 2.4 mm shallower than the Adelie penguin 10.4.2 Point prediction Using the slm_sp model we can make a point prediction for the expected bill depth (mm) for Gentoo penguins with a bill length of 50mm. Recall the model equation ## $$ ## \\begin{aligned} ## \\operatorname{bill\\_depth\\_mm} &amp;= \\alpha\\ + \\\\ ## &amp;\\quad \\beta_{1}(\\operatorname{bill\\_length\\_mm})\\ + \\\\ ## &amp;\\quad \\beta_{2}(\\operatorname{species}_{\\operatorname{Chinstrap}})\\ + \\\\ ## &amp;\\quad \\beta_{3}(\\operatorname{species}_{\\operatorname{Gentoo}})\\ + \\\\ ## &amp;\\quad \\epsilon ## \\end{aligned} ## $$ We can then simply substitute in the values: \\[\\widehat{\\text{bill depth}} = \\hat{\\alpha} + \\hat{\\beta_1}*50 + \\hat{\\beta_3}*1\\] \\[\\downarrow\\] \\[\\widehat{\\text{bill depth}} = 10.56 + 0.20*50 - 5.10*1\\] \\[\\downarrow\\] \\[15.47\\text{mm}\\] Rather than by hand we can do this easily in R ## create new data frame with data we want to predict to ## the names have to match those in our original data frame newdata &lt;- data.frame(species = &quot;Gentoo&quot;,bill_length_mm = 50) ## use predict() function predict(slm_sp, newdata = newdata) ## more accurate than our by hand version! ## 1 ## 15.4841 What does this look like on a plot 10.5 TL;DR lm() Traditional name Model formula R code Simple regression \\(Y \\sim X_{continuous}\\) lm(Y ~ X) One-way ANOVA \\(Y \\sim X_{categorical}\\) lm(Y ~ X) Two-way ANOVA \\(Y \\sim X1_{categorical} + X2_{categorical}\\) lm(Y ~ X1 + X2) ANCOVA \\(Y \\sim X1_{continuous} + X2_{categorical}\\) lm(Y ~ X1 + X2) Multiple regression \\(Y \\sim X1_{continuous} + X2_{continuous}\\) lm(Y ~ X1 + X2) Factorial ANOVA \\(Y \\sim X1_{categorical} * X2_{categorical}\\) lm(Y ~ X1 * X2) or lm(Y ~ X1 + X2 + X1:X2) Artwork by @allison_horst Meet your MLR teaching assistants Interpret coefficients for categorical predictor variables Interpret coefficients for continuous predictor variables Make predictions using the regression model Residuals Check residuals for normality 10.5.1 Model formula syntax In R to specify the model you want to fit you typically create a model formula object; this is usually then passed as the first argument to the model fitting function (e.g., lm()). Some notes on syntax: Consider the model formula example y ~ x + z + x:z. There is a lot going on here: The variable to the left of ~ specifies the response, everything to the right specify the explanatory variables + indicated to include the variable to the left of it and to the right of it (it does not mean they should be summed) : denotes the interaction of the variables to its left and right Additional, some other symbols have special meanings in model formula: * means to include all main effects and interactions, so a*b is the same as a + b + a:b ^ is used to include main effects and interactions up to a specified level. For example, (a + b + c)^2 is equivalent to a + b + c + a:b + a:c + b:c (note (a + b + c)^3 would also add a:b:c) - excludes terms that might otherwise be included. For example, -1 excludes the intercept otherwise included by default, and a*b - b would produce a + a:b Mathematical functions can also be directly used in the model formula to transform a variable directly (e.g., y ~ exp(x) + log(z) + x:z). One thing that may seem counter intuitive is in creating polynomial expressions (e.g., \\(x^2\\)). Here the expression y ~ x^2 does not relate to squaring the explanatory variable \\(x\\) (this is to do with the syntax ^ you see above. To include \\(x^2\\) as a term in our model we have to use the I() (the &quot;as-is&quot; operator). For example, y ~ I(x^2)). 10.6 Other resources: optional but recommended Exploring interactions with continuous predictors in regression models The ASA Statement on p-Values: Context, Process, and Purpose 10.7 Beyond Linear Models to Generalised Linear Models (GLMs) (not examinable) Recall the assumptions of a linear model The \\(i\\)th observation's response, \\(Y_i\\), comes from a normal distribution Its mean, \\(\\mu_i\\), is a linear combination of the explanatory terms Its variance, \\(\\sigma^2\\), is the same for all observations Each observation's response is independent of all others But, what if we want to rid ourselves from a model with normal errors? The answer: Generalised Linear Models. 10.7.1 Counting animals... A normal distribution does not adequately describe the response, the number of animals It is a continuous distribution, but the response is discrete It is symmetric, but the response is unlikely to be so It is unbounded, and assumes it is plausible for the response to be negative I addition, a linear regression model typically assumes constant variance, but int his situation this unlikely to be the case. So why assume a normal distribution? Let's use a Poisson distribution instead. \\[\\begin{equation*} \\mu_i = \\beta_0 + \\beta_1 x_i, \\end{equation*}\\] So \\[\\begin{equation*} Y_i \\sim \\text{Normal}(\\mu_i\\, \\sigma^2), \\end{equation*}\\] becomes \\[\\begin{equation*} Y_i \\sim \\text{Poisson}(\\mu_i), \\end{equation*}\\] The Poisson distribution is commonly used as a general-purpose distribution for counts. A key feature of this distribution is \\(\\text{Var}(Y_i) = \\mu_i\\), so we expect the variance to increase with the mean. 10.7.2 Other modelling approaches (not examinable) R function Use glm() Fit a linear model with a specific error structure specified using the family = argument (Poisson, binomial, gamma) gam() Fit a generalised additive model. The R package mgcv must be loaded lme() and nlme() Fit linear and non-linear mixed effects models. The R package nlme must be loaded lmer() Fit linear and generalised linear and non-linear mixed effects models. The package lme4 must be installed and loaded gls() Fit generalised least squares models. The R package nlme must be loaded "],["resampling-procedures.html", "11 Resampling Procedures 11.1 Learning Objectives 11.2 Resampling 11.3 Significance testing using permutation (randomisation) tests 11.4 The bootstrap 11.5 Differences between permutation test and bootstrap test", " 11 Resampling Procedures 11.1 Learning Objectives List the aims, write out the appropriate null and alternative hypothesis using statistical notation for, and write R code to carry out a permutation (randomization) test bootsrap procedure 11.2 Resampling What is resampling? Any of a variety of methods for doing one of the following Estimating the precision of sample statistics (e.g., bootstrapping) Performing significance tests (e.g., permutation/exact/randomisation tests) Validating models (e.g., bootstrapping, cross validation) 11.2.1 Significance tests A significance test can tell us whether an observed effect (e.g., difference between two means (or medians), or correlation between two variables) could occur by chance in selecting a random sample from each of the two populations. The basic approach to permutation tests is straightforward: Choose a statistic to measure the effect in question (e.g., differences between group means) Calculate that test statistic on the observed data. Note this metric can be anything you wish Construct the sampling distribution that this statistic would have if the effect were not present in the population (i.e., the distribution under the Null hypothesis, \\(H_0\\)): For chosen number of times shuffle the data labels calculate the test statistic for the reshuffled data and retain Find the location of your observed statistic in the sampling distribution. The location of observed statistic in sampling distribution is informative: if in the main body of the distribution then the observed statistic could easily have occurred by chance if in the tail of the distribution then the observed statistic would rarely occur by chance and there is evidence that something other than chance is operating. Calculate the proportion of times your reshuffled statistics equal or exceed the observed. This p-value is the probability that we observe a statistic at least as ‚Äúextreme‚Äù as the one we observed State the strength of evidence against the null on the basis of this probability. 11.3 Significance testing using permutation (randomisation) tests 11.3.1 Permutation Test on Two Independent Samples 11.3.1.1 PƒÅua shell lengths Remember the PƒÅua data from Chapter 1 One question we may want to ask is if on average the shell length differs between Species? Scientific question: Are the shell lengths of shells the same in both species? Null hypothesis: The distribution of shell lengths in Haliotis iris the same as in Haliotis australis Test statistic: Difference of sample means means &lt;- paua %&gt;% group_by(Species) %&gt;% summarise(means = mean(Length)) ggplot(paua,aes(x = Species, y = Length)) + geom_violin() + geom_point(alpha = 0.4) + ylab(&quot;Length (cms)&quot;) + xlab(&quot;&quot;) + theme_classic() + geom_point(data = means, aes(x = Species, y = means, color = Species), size = 2) + geom_hline(data = means, aes(yintercept = means, color = Species), lty = 2, alpha = 0.5) + theme(legend.position = &quot;none&quot;) + geom_text(data = means, aes(x = Species, y = means + 0.3, label = paste0(&quot;Species averege = &quot;,round(means,3)), color = Species)) ggplot(paua,aes(x = Length, fill = Species)) + geom_histogram(position = &quot;identity&quot;, alpha = 0.3) + xlab(&quot;Length (cms)&quot;) + ylab(&quot;&quot;) + theme_classic() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. But because the data are skewed and we've likely got non-constant variances we may be better off adopting a randomization test, rather than a parametric t-test ## observed differences in means diff_in_means &lt;- (paua %&gt;% group_by(Species) %&gt;% summarise(mean = mean(Length)) %&gt;% summarise(diff = diff(mean)))$diff diff_in_means ## [1] -0.9569444 ## Number of times I want to randomise nreps &lt;- 1000 ## initialize empty array to hold results randomisation_difference_mean &lt;- numeric(nreps) set.seed(1234) ## *****Remove this line for actual analyses***** ## This means that each run with produce the same results and ## agree with the printout that I show. for (i in 1:nreps) { ## the observations data &lt;- data.frame(value = paua$Length) ## randomise labels data$random_labels &lt;-sample(paua$Species, replace = FALSE) ## randomised differences in mean randomisation_difference_mean[i] &lt;- (data %&gt;% group_by(random_labels) %&gt;% summarise(mean = mean(value)) %&gt;% summarise(diff = diff(mean)))$diff } ## results results &lt;- data.frame(randomisation_difference_mean = randomisation_difference_mean) ## How many randomised differences in means are as least as extreme as the one we observed ## absolute value as dealing with two tailed n_exceed &lt;- sum(abs(results$randomisation_difference_mean) &gt;= abs(diff_in_means)) n_exceed ## [1] 1 ## proportion n_exceed/nreps ## [1] 0.001 ggplot(results, aes(x = randomisation_difference_mean)) + geom_histogram() + theme_classic() + ylab(&quot;&quot;) + xlab(&quot;Differences between randomised group means&quot;) + geom_vline(xintercept = diff_in_means, col = &quot;cyan4&quot;, size = 1,alpha = 0.6) + annotate(geom = &#39;text&#39;, label = &quot;Observed difference between means&quot; , x = -Inf, y = Inf, hjust = 0, vjust = 1.5, color = &quot;cyan4&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. How would the parametric t-test have served? t.test(Length ~ Species, data = paua) ## ## Welch Two Sample t-test ## ## data: Length by Species ## t = 3.5404, df = 57.955, p-value = 0.0007957 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.4158802 1.4980086 ## sample estimates: ## mean in group Haliotis australis mean in group Haliotis iris ## 5.766667 4.809722 Not too different after all 11.3.1.2 Jackal mandible lengths ## Mandible lengths (mm) for golden jackals (Canis aureus) of each sex from the British Museum jackal &lt;- data.frame(mandible_length_mm = c(120, 107, 110, 116, 114, 111, 113, 117, 114, 112, 110, 111, 107, 108, 110, 105, 107, 106, 111, 111), sex = rep(c(&quot;Male&quot;,&quot;Female&quot;), each = 10)) Scientific question: Are the jaw lengths of jackals the same in both sexes? Null hypothesis: The distribution of jaw lengths in male jackals the same as in in females Test statistic: Difference of sample means Rather than a for loop let's try this another way. ## observed statistic jackal_mean_diff &lt;- (jackal %&gt;% group_by(sex) %&gt;% summarise(mean = mean(mandible_length_mm)) %&gt;% summarise(diff = diff(mean)))$diff ## Generate all possible combinations ## This time we&#39;re doing ALL possble ones ## rather than a rendom 1000 combinations &lt;- combn(20,10) ## Do the permutations permtest_combinations &lt;- apply(combinations, 2, function(x) mean(jackal$mandible_length_mm[x]) - mean(jackal$mandible_length_mm[-x])) ## Full Permutation test p.value length(permtest_combinations[abs(permtest_combinations) &gt;= jackal_mean_diff]) / choose(20,10) ## [1] 0.003334127 ## Now let&#39;s use 10000 random permutations, sample without replacement ## set up matrix random_perm &lt;- apply(matrix(0, nrow = 10000, ncol = 1), 1, function(x) sample(20)) random_mean_diff &lt;- apply(random_perm, 2, function(x){ z &lt;- jackal$mandible_length_mm[x] mean(z[jackal$sex == &quot;Male&quot;]) - mean(z[jackal$sex == &quot;Female&quot;]) }) random_p.value &lt;- length(random_mean_diff [abs(random_mean_diff) &gt;= jackal_mean_diff]) / 10000 ## note the abs() random_p.value ## [1] 0.0029 ## Now what about a t-test (two-sample) t.test(mandible_length_mm ~ sex, data = jackal)$p.value ## [1] 0.003359952 11.3.2 P-values from permutation tests In experimental situations a large p-value (large tail proportion) means that the luck of the randomisation quite often produces group differences as large or even larger than what we've got in our data. A small p-value means that the luck of the randomisation draw hardly ever produces group differences as large as we've got in our data. Statistical significance does not imply practical significance. Statistical significance says nothing about the size of treatment differences. To estimate the sizes of differences you need confidence intervals. NOTE: We can extend the randomization test to make inference about any sample statistic (not just the mean) 11.4 The bootstrap Recall that the sampling distribution shows us what would happen if we took very many samples under the same conditions. The bootstrap is a procedure for finding the (approximate) sampling distribution from just one sample. In brief, The original sample represents the distribution of the population from which it was drawn. Resamples, taken with replacement from the original sample are representative of what we would get from drawing many samples from the population (the distribution of the statistics calculated from each resample is known as the bootstrap distribution of the statistic). The bootstrap distribution of a statistic represents that statistic‚Äôs sampling distribution. 11.4.1 Example: constructing bootstrap confidence intervals Old faithful is a gyser located in Yellowstone National Park, Wyoming. Below is a histogram of the durations of 299 consecutive eruptions. Clearly bimodal! MASS::geyser ## waiting duration ## 1 80 4.0166667 ## 2 71 2.1500000 ## 3 57 4.0000000 ## 4 80 4.0000000 ## 5 75 4.0000000 ## 6 77 2.0000000 ## 7 60 4.3833333 ## 8 86 4.2833333 ## 9 77 2.0333333 ## 10 56 4.8333333 ## 11 81 1.8333333 ## 12 50 5.4500000 ## 13 89 1.6166667 ## 14 54 4.8666667 ## 15 90 4.3833333 ## 16 73 1.7666667 ## 17 60 4.6666667 ## 18 83 2.0000000 ## 19 65 4.7333333 ## 20 82 4.2166667 ## 21 84 1.9000000 ## 22 54 4.9666667 ## 23 85 2.0000000 ## 24 58 4.0000000 ## 25 79 2.0000000 ## 26 57 4.0000000 ## 27 88 2.8333333 ## 28 68 4.5000000 ## 29 76 4.0666667 ## 30 78 3.7166667 ## 31 74 3.5166667 ## 32 85 4.4666667 ## 33 75 2.2166667 ## 34 65 4.8833333 ## 35 76 2.6000000 ## 36 58 4.1500000 ## 37 91 2.2000000 ## 38 50 4.7666667 ## 39 87 1.8333333 ## 40 48 4.6000000 ## 41 93 2.2666667 ## 42 54 4.1333333 ## 43 86 2.0000000 ## 44 53 4.0000000 ## 45 78 2.0000000 ## 46 52 4.0000000 ## 47 83 1.8833333 ## 48 60 4.2666667 ## 49 87 2.0833333 ## 50 49 4.4666667 ## 51 80 2.5000000 ## 52 60 4.0000000 ## 53 92 1.7666667 ## 54 43 4.3333333 ## 55 89 2.1833333 ## 56 60 4.4833333 ## 57 84 3.8833333 ## 58 69 3.3333333 ## 59 74 3.7333333 ## 60 71 4.0000000 ## 61 108 1.9500000 ## 62 50 5.2666667 ## 63 77 2.0000000 ## 64 57 4.0000000 ## 65 80 2.0000000 ## 66 61 4.0000000 ## 67 82 2.0000000 ## 68 48 4.0000000 ## 69 81 3.5333333 ## 70 73 2.1666667 ## 71 62 4.5000000 ## 72 79 2.0166667 ## 73 54 4.1500000 ## 74 80 4.2000000 ## 75 73 4.3333333 ## 76 81 1.9333333 ## 77 62 4.6500000 ## 78 81 3.8166667 ## 79 71 4.0333333 ## 80 79 4.1666667 ## 81 81 4.6666667 ## 82 74 1.8166667 ## 83 59 4.0000000 ## 84 81 3.0000000 ## 85 66 4.0000000 ## 86 87 2.0000000 ## 87 53 4.4500000 ## 88 80 2.0500000 ## 89 50 4.2500000 ## 90 87 1.9166667 ## 91 51 4.6666667 ## 92 82 1.7333333 ## 93 58 4.3833333 ## 94 81 1.7666667 ## 95 49 4.6000000 ## 96 92 1.8666667 ## 97 50 4.4500000 ## 98 88 1.6333333 ## 99 62 5.0333333 ## 100 93 1.8166667 ## 101 56 5.1000000 ## 102 89 1.6333333 ## 103 51 4.2833333 ## 104 79 2.0000000 ## 105 58 4.0000000 ## 106 82 2.0000000 ## 107 52 4.5333333 ## 108 88 2.0000000 ## 109 52 4.0000000 ## 110 78 2.9333333 ## 111 69 4.7333333 ## 112 75 3.9000000 ## 113 77 1.9500000 ## 114 53 4.1166667 ## 115 80 1.8000000 ## 116 55 4.6666667 ## 117 87 1.8333333 ## 118 53 4.7000000 ## 119 85 2.1166667 ## 120 61 4.7833333 ## 121 93 1.8166667 ## 122 54 4.1000000 ## 123 76 4.6500000 ## 124 80 4.0000000 ## 125 81 2.0000000 ## 126 59 4.0000000 ## 127 86 4.0000000 ## 128 78 4.2166667 ## 129 71 4.1333333 ## 130 77 3.9333333 ## 131 76 3.7500000 ## 132 94 4.4166667 ## 133 75 2.4666667 ## 134 50 4.1666667 ## 135 83 3.8000000 ## 136 82 4.3166667 ## 137 72 3.8666667 ## 138 77 4.6833333 ## 139 75 1.7000000 ## 140 65 4.9666667 ## 141 79 4.2666667 ## 142 72 4.5833333 ## 143 78 4.0000000 ## 144 77 4.0000000 ## 145 79 4.0000000 ## 146 75 4.0000000 ## 147 78 1.9833333 ## 148 64 4.6000000 ## 149 80 0.8333333 ## 150 49 4.9166667 ## 151 88 1.7333333 ## 152 54 4.5833333 ## 153 85 1.7000000 ## 154 51 4.7500000 ## 155 96 1.8333333 ## 156 50 4.5000000 ## 157 80 1.8666667 ## 158 78 4.4500000 ## 159 81 4.4500000 ## 160 72 4.0000000 ## 161 75 4.8000000 ## 162 78 4.0000000 ## 163 87 4.0000000 ## 164 69 2.0000000 ## 165 55 4.0000000 ## 166 83 1.9333333 ## 167 49 4.5833333 ## 168 82 2.0000000 ## 169 57 3.7000000 ## 170 84 2.8666667 ## 171 57 4.8333333 ## 172 84 3.4500000 ## 173 73 4.3833333 ## 174 78 1.8000000 ## 175 57 4.4000000 ## 176 79 2.4833333 ## 177 57 4.5166667 ## 178 90 2.1000000 ## 179 62 4.3500000 ## 180 87 4.3666667 ## 181 78 1.7833333 ## 182 52 4.9166667 ## 183 98 1.8166667 ## 184 48 4.0000000 ## 185 78 4.0000000 ## 186 79 4.0000000 ## 187 65 3.8666667 ## 188 84 1.8500000 ## 189 50 4.7000000 ## 190 83 2.0166667 ## 191 60 4.4666667 ## 192 80 1.8666667 ## 193 50 4.1666667 ## 194 88 1.9000000 ## 195 50 4.2500000 ## 196 84 3.2500000 ## 197 74 4.2166667 ## 198 76 1.8833333 ## 199 65 4.9833333 ## 200 89 1.8500000 ## 201 49 4.0000000 ## 202 88 1.9666667 ## 203 51 4.7666667 ## 204 78 4.0000000 ## 205 85 2.0000000 ## 206 65 4.0000000 ## 207 75 4.0000000 ## 208 77 2.3833333 ## 209 69 4.4166667 ## 210 92 4.2166667 ## 211 68 4.3666667 ## 212 87 2.0000000 ## 213 61 4.4500000 ## 214 81 1.7500000 ## 215 55 4.5000000 ## 216 93 1.6166667 ## 217 53 4.7000000 ## 218 84 2.5666667 ## 219 70 3.7000000 ## 220 73 4.2333333 ## 221 93 1.9333333 ## 222 50 4.3500000 ## 223 87 4.0000000 ## 224 77 4.0000000 ## 225 74 4.0000000 ## 226 72 4.2166667 ## 227 82 4.0000000 ## 228 74 4.1333333 ## 229 80 1.8833333 ## 230 49 4.4666667 ## 231 91 1.9500000 ## 232 53 4.2166667 ## 233 86 1.7166667 ## 234 49 4.4500000 ## 235 79 4.2500000 ## 236 89 3.9666667 ## 237 87 4.3833333 ## 238 76 1.9666667 ## 239 59 4.4500000 ## 240 80 4.2666667 ## 241 89 1.9166667 ## 242 45 4.4166667 ## 243 93 3.0000000 ## 244 72 4.0000000 ## 245 71 2.0000000 ## 246 54 4.0000000 ## 247 79 3.2833333 ## 248 74 1.8333333 ## 249 65 4.6166667 ## 250 78 1.8333333 ## 251 57 4.6166667 ## 252 87 4.6000000 ## 253 72 4.2500000 ## 254 84 1.9333333 ## 255 47 4.9833333 ## 256 84 1.9666667 ## 257 57 4.3000000 ## 258 87 4.2000000 ## 259 68 4.5333333 ## 260 86 4.4000000 ## 261 75 4.6166667 ## 262 73 2.0000000 ## 263 53 4.0000000 ## 264 82 4.0000000 ## 265 93 3.9166667 ## 266 77 2.0000000 ## 267 54 4.5000000 ## 268 96 1.8000000 ## 269 48 4.0000000 ## 270 89 2.7500000 ## 271 63 4.7333333 ## 272 84 3.9666667 ## 273 76 1.9500000 ## 274 62 4.9666667 ## 275 83 1.8500000 ## 276 50 4.8000000 ## 277 85 4.0000000 ## 278 78 4.0000000 ## 279 78 4.0000000 ## 280 81 4.0000000 ## 281 78 4.0000000 ## 282 76 4.0000000 ## 283 74 4.0000000 ## 284 81 2.0000000 ## 285 66 4.0000000 ## 286 84 1.9333333 ## 287 48 4.3333333 ## 288 93 1.6666667 ## 289 47 4.7666667 ## 290 87 1.9500000 ## 291 51 4.6833333 ## 292 78 1.9333333 ## 293 54 4.4166667 ## 294 87 2.1333333 ## 295 52 4.0833333 ## 296 85 2.0666667 ## 297 58 4.0000000 ## 298 88 4.0000000 ## 299 79 2.0000000 ggplot(data = MASS::geyser, aes(x = duration)) + geom_histogram() + xlab(&quot;Duration of eruptions (m)&quot;) Step 1: Calculating the observed mean eruption duration time: mean &lt;- MASS::geyser %&gt;% summarise(mean = mean(duration)) mean ## mean ## 1 3.460814 Step 2: Construct bootstrap distribution ## Number of times I want to bootstrap nreps &lt;- 1000 ## initialize empty array to hold results bootstrap_means &lt;- numeric(nreps) set.seed(1234) ## *****Remove this line for actual analyses***** ## This means that each run with produce the same results and ## agree with the printout that I show. for (i in 1:nreps) { ## bootstrap. note with replacement bootstrap_sample &lt;- sample(MASS::geyser$duration, replace = TRUE) ## bootstraped mean resample bootstrap_means[i] &lt;- mean(bootstrap_sample) } ## results results &lt;- data.frame(bootstrap_means = bootstrap_means) ggplot(data = results, aes(x = bootstrap_means)) + geom_histogram() + geom_vline(xintercept = as.numeric(mean)) + ggtitle(&quot;Bootstrap distribution&quot;) Bootstrap estimate of bias is the difference between the mean of the boostrap distribution and the value of the statistic in the original sample: bias &lt;- as.numeric(mean) - mean(results$bootstrap_means) bias ## [1] 0.001200111 Bootstrap standard error of a statistic is the standard deviation of its bootstrap distribution: sd(results$bootstrap_means) ## [1] 0.06740607 ## compare to SEM of original data MASS::geyser %&gt;% summarise(sem = sd(duration)/sqrt(length(duration))) ## sem ## 1 0.06638498 Bootstrap \\(t\\) confidence interval. If, for a sample of size \\(n\\) the boostrap distribution is approximately Normal and the estimate of bias is small then an approximate \\(C\\) confidence for the parameter corresponding to the statistic is: \\[\\text{statistic} \\pm t^* \\text{SE}_\\text{bootstrap}\\] where \\(t*\\) is the critical value of the \\(t_{n-1}\\) distribution with area \\(C\\) between \\(-t^*\\) and \\(t^*\\). For \\(C = 0.95\\): as.numeric(mean) + c(-1,1) * qt(0.975,298)*sd(results$bootstrap_means) ## [1] 3.328162 3.593466 So our 95% confidence interval is 3.3 to 3.6. Bootstrap \\(percentile\\) confidence interval. Use the bootstrap distribution itself to determine the limits of the confidence interval by taking the limits of the sorted, central \\(C\\) bulk of the distribution. For \\(C = 0.95\\): sort(results$bootstrap_means)[c(25,975)] ## [1] 3.328428 3.591081 11.5 Differences between permutation test and bootstrap test The permutation test exploits symmetry under the null hypothesis. A full permutation test p-value is exact, conditional on data values in the combined sample. A bootstrap estimates the probability mechanism that generated the samples under the null hypothesis. A bootstrap does not require any special symmetry or assumption or exchangability. "],["multivariate-data-methods.html", "12 Multivariate Data Methods 12.1 Learning Objectives 12.2 Clustering 12.3 TL;DR k-means clustering 12.4 Other resources: optional but recommended", " 12 Multivariate Data Methods 12.1 Learning Objectives Explain the aims and motivation behind cluster analysis and its relevance in biology Write R code to carry out hierarchical and k-means cluster analysis Interpret R output from hierarchical and k-means cluster analysis Interpret and communicate, to both a statistical and non-statistical audience, clustering techniques, specifically, Divisive methods, nonparametric algorithms such as k-means Agglomerative methods, clustering cases and/or variables into a hierarchy of sets (i.e., hierarchical clustering) 12.2 Clustering So, it's all about variation again! And the idea of minimizing it. Goals See measures of (dis)similarity and distances that help us define clusters. Uncover hidden or latent clustering by partitioning the data into tighter sets. Divisive methods: nonparametric algorithms such as k-means to split data into a small number of clusters. Agglomerative methods: clustering cases and/or variables into a hierarchy of sets - hierarchical clustering. Study how to validate clusters through resampling-based bootstrap methods 12.2.1 Clustering algorithms The distances are used to construct the clusters. Agglomerative methods, that build a hierarchical clustering tree Partitioning methods that separate the data into subsets Both types of methods require a choice to be made: the number k of clusters. Partitioning methods such as k-means this choice has to be made at the outset whereas for hierarchical clustering this can be deferred to the end of the analysis. 12.2.1.1 k-means K-means clustering involves defining clusters so that the overall variation within a cluster (known as total within-cluster variation) is minimized. How do we define this variation? Typically, using Euclidean distances; the total within-cluster variation, is in this case, is defined as the sum of squared distances Euclidean distances between observations and the corresponding cluster centroid. In summary, this is the procedure The number of clusters (k) are specified k objects from the dataset are selected at random and set as the initial cluster centers or means Each observation is assigned to their closest centroid (based on the Euclidean distance between the object and the centroid) For each of the k clusters the cluster centroid is then updated based on calculating the new mean values of all the data points in the cluster Repeat the two previous steps until 1) the cluster assignments stop changing or 2) the maximum number of iterations is reached 12.2.1.2 Hierarchical clustering Hierarchical clustering is a bottom-up approach: + similar observations and subclasses are assembled iteratively Linnaeus made nested clusters of organisms according to specific characteristics. The order of the labels does not matter within sibling pairs. + Horizontal distances are usually meaningless + Vertical distances can encode some information. In summary, this is the procedure Start with a matrix of distances, (or similarities) between pairs of observations (cases) Choice of distance measure key first step Algorithm: Initial n singleton clusters Scan distance matrix for two closest individuals, group them together Compute distance from cluster of size 2 to remaining n-1 singleton clusters Ways to calculate distances between the aggregates Single Linkage (nearest neighbour/minimal jump): Computes the distance between clusters as the smallest distance between any two points in the two clusters Complete Linkage (maximum jump): Calculates the maximum distance betweentwo points from each cluster Average linkage: Computes the mean of distances between all pairs of observations Ward's method: where the goal is to minimize the variance within clusters Method Pros Cons Single linkage number of clusters comblike trees. Complete linkage compact clusters one obs. can alter groups Average linkage similar size and variance not robust Centroid robust to outliers smaller number of clusters Ward minimising an inertia clusters small if high variability 12.2.1.3 Identify optimal number of clusters Identifying the appropriate k is important because too many or too few clusters impedes viewing overall trends. Too many clusters can lead to over-fitting (which limits generalizations) while insufficient clusters limits insights into commonality of groups. There are assorted methodologies to identify the appropriate \\(k\\). Tests range from blunt visual inspections to robust algorithms. The optimal number of clusters is ultimately a subjective decision. 12.2.2 With ants The data pitfalls.csv is available on CANVAS Data were collected on the distribution of ant species at 30 sites across the Auckland region using pitfall traps. Twenty pitfall traps at each site were left open for ten days and the number of individuals captured counted Data used here are standardised \\(\\text{log}(x + 1)\\) transformed for the four most abundant species: Nylanderia spp Pheidole rugosula Tetramorium grassii Pachycondyla sp At each location twenty pitfall traps were placed in each of four habitats (Forest, Grass, Urban, Scrub) and left for ten days. At the end of this sampling all individuals in the pitfall traps were identified and summed at each site (location x habitat). This sampling protocol was repeated for 3 months over summer 2011. ## # A tibble: 30 x 8 ## Location Habitat Month Site Nyl Phe Tet Pac ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 West Forest 1 WF1 0 0 0 157 ## 2 West Grass 1 WG1 0 2 7 37 ## 3 West Urban 1 WU1 3 7 0 0 ## 4 West Forest 2 WF2 0 0 0 31 ## 5 West Grass 2 WG2 5 0 25 0 ## 6 West Forest 3 WF3 0 0 0 21 ## 7 West Grass 3 WG3 0 3 2 1 ## 8 West Urban 3 WU3 0 1 0 0 ## 9 Central Forest 1 CF1 0 0 0 1 ## 10 Central Grass 1 CG1 0 3 22 2 ## # ‚Ä¶ with 20 more rows library(tidyverse) ants &lt;- read_csv(&quot;pitfalls.csv&quot;) Hierarchical clustering Data are species counts, so we will use Bray Curtis measure: pitfall.dist &lt;- vegan::vegdist(ants[,5:8], method = &quot;bray&quot;, binary = FALSE) factoextra::fviz_dist(pitfall.dist) Computing 4 dendrograms Single-linkage single &lt;- ants[,5:8] %&gt;% vegan::vegdist(., method = &quot;bray&quot;, binary = FALSE) %&gt;% hclust(method = &quot;single&quot;) plot(single, labels = ants$Site) Maximum linkage complete &lt;- ants[,5:8] %&gt;% vegan::vegdist(., method = &quot;bray&quot;, binary = FALSE) %&gt;% hclust(method = &quot;complete&quot;) plot(complete, labels = ants$Site) Average linkage (UPGMA) average &lt;- ants[,5:8] %&gt;% vegan::vegdist(., method = &quot;bray&quot;, binary = FALSE) %&gt;% hclust(method = &quot;average&quot;) plot(average, labels = ants$Site) Ward‚Äôs ward &lt;- ants[,5:8] %&gt;% vegan::vegdist(., method = &quot;bray&quot;, binary = FALSE) %&gt;% hclust(method = &quot;ward.D&quot;) plot(ward, labels = ants$Site) WHAT ARE DENDROGRAMS GOOD FOR? Suggesting clusters for further study... Using the function cutree() to split into clusters and plot: ants$clust4 &lt;- cutree(ward, k = 4) library(ape) ## install pitfall.phylo &lt;- as.phylo(ward) pitfall.phylo$tip.label &lt;- ants$Site ## Set colours colours &lt;- c(&quot;red&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;black&quot;) plot(pitfall.phylo, cex = 0.6, tip.color = colours[ants$clust4], label.offset = 0.05) 12.2.3 k-means using the palmerpenguins data library(palmerpenguins) ## getting rid of NAs penguins_nafree &lt;- penguins %&gt;% drop_na() ## introducing a new package GGally, please install ## using install.packages(&quot;GGally&quot;) library(GGally) penguins_nafree %&gt;% select(species, where(is.numeric)) %&gt;% ggpairs(columns = c(&quot;flipper_length_mm&quot;, &quot;body_mass_g&quot;, &quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;)) We see that a lot of these variables (e.g., flipper_length_mm, body_mass_g, and bill_length_mm) are relatively strongly (positively) related to one another. Could they actually be telling us the same information? Combined we could think of these three variables all telling us a little about bigness of penguin. Is there a way we could reduce these three variables, into say 1, to represent the bigness of a penguin. We may not need all the information (variation) captured by these variables, but could get away with fewer new uncorrelated variables that represent basically the same information (e.g., penguin bigness), thereby, reducing the dimensionality of the data (more on this later). ## create a data frame of what we&#39;re interested in df &lt;- penguins_nafree %&gt;% select(where(is.numeric), -year) We use the kmeans() function. The first argument of kmeans() should be the dataset you wish to cluster. Below we use data frame df, the penguin data discussed above. But how many clusters do we choose? Let's try 1 to 5... (i.e., using the centers argument). Setting nstart = 25 means that R will try 25 different random starting assignments and then select the best results corresponding to the one with the lowest within cluster variation. ## set the seed so we all start off in the same place set.seed(4321) ## one cluster k1 &lt;- kmeans(df, centers = 1, nstart = 25) ## two clusters k2 &lt;- kmeans(df, centers = 2, nstart = 25) ## three clusters k3 &lt;- kmeans(df, centers = 3, nstart = 25) ## four clusters k4 &lt;- kmeans(df, centers = 4, nstart = 25) ## five clusters k5 &lt;- kmeans(df, centers = 5, nstart = 25) The kmeans() function returns a list of components: cluster, integers indicating the cluster to which each observation is allocated centers, a matrix of cluster centers/means totss, the total sum of squares withinss, within-cluster sum of squares, one component per cluster tot.withinss, total within-cluster sum of squares betweenss, between-cluster sum of squares size, number of observations in each cluster 12.2.3.1 Choosing the number of clusters We have an idea there may be 3 clusters, perhaps, but how do we know this is the best fit? Remember it's a subjective choice and we'll be looking at a few pointers Visual inspection method library(factoextra) ## a new packahe for kmeasn viz, please install p1 &lt;- fviz_cluster(k1, data = df) p2 &lt;- fviz_cluster(k2, data = df) p3 &lt;- fviz_cluster(k3, data = df) p4 &lt;- fviz_cluster(k4, data = df) p5 &lt;- fviz_cluster(k5, data = df) ## for arranging plots library(patchwork) (p1| p2| p3)/ (p4 | p5) Alternatively, you can use standard pairwise scatter plots to illustrate the clusters compared to the original variables. df %&gt;% mutate(cluster = k3$cluster, species = penguins_nafree$species) %&gt;% ggplot(aes(flipper_length_mm, bill_depth_mm, color = factor(cluster), label = species)) + geom_text() Elbow method Optimal clusters are at the point in which the knee &quot;bends&quot; or in mathematical terms when the marginal total within sum of squares (tot.withinss) for an additional cluster begins to decrease at a linear rate This is easier to see via a plot: fviz_nbclust(df, kmeans, method = &quot;wss&quot;) + labs(subtitle = &quot;Elbow method&quot;) There is a pretty obvious inflection (elbow) at 2 clusters, but maybe at 3 too. We can rule out an optimal number of clusters above 3 as there is then only a minimal marginal reduction in total within sum of squares. However, the model is ambiguous on whether 2 or 3 clusters is optimal... Silhouette method # Silhouette method fviz_nbclust(df, kmeans, method = &quot;silhouette&quot;)+ labs(subtitle = &quot;Silhouette method&quot;) Gap method # Gap statistic # recommended value: nboot = 500 for your analysis (it will take a while) set.seed(123) ## remove this fviz_nbclust(df, kmeans, nstart = 25, method = &quot;gap_stat&quot;, nboot = 50)+ labs(subtitle = &quot;Gap statistic method&quot;) Basically it's up to you to collate all the suggestions and make and informed decision ## Trying all the cluster indecies AHHHHH library(NbClust) cluster_30_indexes &lt;- NbClust(data = df, distance = &quot;euclidean&quot;, min.nc = 2, max.nc = 9, method = &quot;complete&quot;, index =&quot;all&quot;) ## *** : The Hubert index is a graphical method of determining the number of clusters. ## In the plot of Hubert index, we seek a significant knee that corresponds to a ## significant increase of the value of the measure i.e the significant peak in Hubert ## index second differences plot. ## ## *** : The D index is a graphical method of determining the number of clusters. ## In the plot of D index, we seek a significant knee (the significant peak in Dindex ## second differences plot) that corresponds to a significant increase of the value of ## the measure. ## ## ******************************************************************* ## * Among all indices: ## * 5 proposed 2 as the best number of clusters ## * 6 proposed 3 as the best number of clusters ## * 1 proposed 4 as the best number of clusters ## * 4 proposed 5 as the best number of clusters ## * 1 proposed 8 as the best number of clusters ## * 3 proposed 9 as the best number of clusters ## ## ***** Conclusion ***** ## ## * According to the majority rule, the best number of clusters is 3 ## ## ## ******************************************************************* fviz_nbclust(cluster_30_indexes) + theme_minimal() + labs(title = &quot;Frequency of Optimal Clusters using 30 indexes in NbClust Package&quot;) ## Among all indices: ## =================== ## * 2 proposed 0 as the best number of clusters ## * 1 proposed 1 as the best number of clusters ## * 5 proposed 2 as the best number of clusters ## * 6 proposed 3 as the best number of clusters ## * 1 proposed 4 as the best number of clusters ## * 4 proposed 5 as the best number of clusters ## * 1 proposed 8 as the best number of clusters ## * 3 proposed 9 as the best number of clusters ## * 3 proposed NA&#39;s as the best number of clusters ## ## Conclusion ## ========================= ## * According to the majority rule, the best number of clusters is 3 . Not obvious, basically still undecided between 2 and 3, but according to the absolute majority rule the &quot;best&quot; number is 3 12.3 TL;DR k-means clustering Artwork by @allison_horst 12.4 Other resources: optional but recommended Eigenfaces ClusterDucks Little book for Multivariate Analysis 'explor' is an R package to allow interactive exploration of multivariate analysis results The Mathematics Behind Principal Component Analysis (6 min read) K-means cluster analysis 12.4.1 Multidimensional Scaling in R (not examinable) Multidimensional scaling (MDS) is actually the more general technique of dimension reduction. PCA is a special case of MDS! To carry out MDS in R library(ggfortify) ## Plotting Multidimensional Scaling (for interest) ## stats::cmdscale performs Classical MDS data(&quot;eurodist&quot;) ## road distances (in km) between 21 cities in Europe. autoplot(eurodist) ## Plotting Classical (Metric) Multidimensional Scaling autoplot(cmdscale(eurodist, eig = TRUE)) autoplot(cmdscale(eurodist, eig = TRUE), label = TRUE, shape = FALSE, label.size = 3) ## Plotting Non-metric Multidimensional Scaling ## MASS::isoMDS and MASS::sammon perform Non-metric MDS library(MASS) autoplot(sammon(eurodist)) ## Initial stress : 0.01705 ## stress after 10 iters: 0.00951, magic = 0.500 ## stress after 20 iters: 0.00941, magic = 0.500 autoplot(sammon(eurodist), shape = FALSE, label = TRUE,label.size = 3) ## Initial stress : 0.01705 ## stress after 10 iters: 0.00951, magic = 0.500 ## stress after 20 iters: 0.00941, magic = 0.500 ## Have a go at interpreting these plots based on the geography of the cities :-) "],["dimension-reduction.html", "13 Dimension reduction 13.1 Learning Objectives 13.2 Dimension reduction 13.3 PCA in R 13.4 Reality check: reducing noise... 13.5 Other resources: optional but recommended", " 13 Dimension reduction 13.1 Learning Objectives Explain the aims and motivation behind Principal Component Analysis (PCA) and its relevance in biology Write R code to carry out PCA Interpret principal component scores and describe a subject with a high or low score Interpret R output from PCA Interpret and communicate, to both a statistical and non-statistical audience, dimaension reduction techniques 13.2 Dimension reduction Reduction of dimensions is needed when there are far too many features in a dataset, making it hard to distinguish between the important ones that are relevant to the output and the redundant or not-so important ones. Reducing the dimensions of data is called dimensionality reduction. So the aim is to find the best low-dimensional representation of the variation in a multivariate (lots and lots of variables) data set, but how do we do this? One way is termed Principal Component Analysis (PCA). PCA is a feature extraction method that reduces the dimensionality of the data (number of variables) by creating new uncorrelated variables while minimizing loss of information on the original variables. Think of a baguette. The baguette pictured here represents two data dimensions: 1) the length of the bread and 2) the height of the bread (we'll ignore depth of bread for now). Think of the baguette as your data; when we carry out PCA we're rotating our original axes (x- and y-coordinates) to capture as much of the variation in our data as possible. This results in new uncorrelated variables that each explain a % of variation in our data; the procedure is designed so that the first new variable (PC1) explains the most, the second (PC2) the second most and so on. Now rather than a baguette think of data; the baguette above represent the shape of the scatter between the two variables plotted below. The rotating grey axes represent the PCA procedure, essentially searching for the best rotation of the original axes to represent the variation in the data as best it can. Mathematically the Euclidean distance (e.g., the distance between points \\(p\\) and \\(q\\) in Euclidean space, \\(\\sqrt{(p-q)^2}\\)) between the points and the rotating axes is being minimized (i.e., the shortest possible across all points), see the blue lines. Once this distance is minimized across all points we &quot;settle&quot; on our new axes (the black tiled axes). Luckily we can do this all in R! 13.3 PCA in R 13.3.1 Using the palmerpenguins data ## getting rid of NAs penguins_nafree &lt;- penguins %&gt;% drop_na() When carrying out PCA we're only interested in numeric variables, so let's just plot those. We can use the piping operator %&gt;% to do this with out creating a new data frame library(GGally) penguins_nafree %&gt;% dplyr::select(species, where(is.numeric)) %&gt;% ggpairs(aes(color = species), columns = c(&quot;flipper_length_mm&quot;, &quot;body_mass_g&quot;, &quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;)) Using prcomp() There are three basic types of information we obtain from Principal Component Analysis: PC scores: the coordinates of our samples on the new PC axis: the new uncorrelated variables (stored in pca$x) Eigenvalues: (see above) represent the variance explained by each PC; we can use these to calculate the proportion of variance in the original data that each axis explains Variable loadings (eigenvectors): these reflect the weight that each variable has on a particular PC and can be thought of as the correlation between the PC and the original variable Before we carry out PCA we should scale out data. WHY? pca &lt;- penguins_nafree %&gt;% dplyr::select(where(is.numeric), -year) %&gt;% ## year makes no sense here so we remove it and keep the other numeric variables scale() %&gt;% ## scale the variables prcomp() ## print out a summary summary(pca) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 1.6569 0.8821 0.60716 0.32846 ## Proportion of Variance 0.6863 0.1945 0.09216 0.02697 ## Cumulative Proportion 0.6863 0.8809 0.97303 1.00000 This output tells us that we obtain 4 principal components, which are called PC1 PC2, PC3, and PC4 (this is as expected because we used the 4 original numeric variables!). Each of these PCs explains a percentage of the total variation (Proportion of Variance) in the dataset: PC1 explains \\(\\sim\\) 68% of the total variance, which means that just over half of the information in the dataset (5 variables) can be encapsulated by just that one Principal Component. PC2 explains \\(\\sim\\) 19% of the variance. PC3 explains \\(\\sim\\) 9% of the variance. PC4 explains \\(\\sim\\) 2% of the variance. From the Cumulative Proportion row we see that by knowing the position of a sample in relation to just PC1 and PC2 we can get a pretty accurate view on where it stands in relation to other samples, as just PC1 and PC2 explain 88% of the variance. The loadings (relationship) between the initial variables and the principal components are stored in pca$rotation: pca$rotation ## PC1 PC2 PC3 PC4 ## bill_length_mm 0.4537532 -0.60019490 -0.6424951 0.1451695 ## bill_depth_mm -0.3990472 -0.79616951 0.4258004 -0.1599044 ## flipper_length_mm 0.5768250 -0.00578817 0.2360952 -0.7819837 ## body_mass_g 0.5496747 -0.07646366 0.5917374 0.5846861 Here we can see that bill_length_mm has a strong positive relationship with PC1, whereas bill_depth_mm has a strong negative relationship. Both fliper_length_mm and body_mass_g also have a strong positive relationship with PC1. Plotting this we get The new variables (PCs) are stored in pca$x, lets plot some of them alongside the loadings using a biplot. For PC1 vs PC2: library(factoextra) ## install this package first fviz_pca_biplot(pca, geom = &quot;point&quot;) + geom_point (alpha = 0.2) Now for PC2 vs PC3 fviz_pca_biplot(pca, axes = c(2,3),geom = &quot;point&quot;) + geom_point (alpha = 0.2) But how many PCs (new variables) do we keep? The whole point of this exercise is to reduce the number of variables we need to explain the variation in our data. So how many of these new variables (PCs) do we keep? To assess this we can use the information printed above alongside a screeplot: fviz_screeplot(pca) Principal components from the original variables Recall that the principal components are a linear combination of the (statndardised) variables. So for PC1 loadings1 &lt;- pca$rotation[,1] loadings1 ## bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## 0.4537532 -0.3990472 0.5768250 0.5496747 Therefore, the first Principle Component will be \\(0.454\\times Z1 -0.399 \\times Z2 + 0.5768 \\times Z3 + 0.5497 \\times Z3\\) where \\(Z1\\), \\(Z2\\), \\(Z3\\). and \\(Z4\\) are the scaled numerical variables form the penguins dataset (i.e., bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g). To compute this we use R: scaled_vars &lt;- penguins_nafree %&gt;% dplyr::select(where(is.numeric), -year) %&gt;% scale() %&gt;% as_tibble() ## By &quot;Hand&quot; by_hand &lt;- loadings1[1]*scaled_vars$&quot;bill_length_mm&quot; + loadings1[2]*scaled_vars$&quot;bill_depth_mm&quot; + loadings1[3]*scaled_vars$&quot;flipper_length_mm&quot; + loadings1[4]*scaled_vars$&quot;body_mass_g&quot; ## From PCA pc1 &lt;- pca$x[,1] plot(by_hand,pc1) 13.3.2 Athletes You'll find the athletes.csv file on CANVAS. athletes &lt;- read_csv(&quot;athletes.csv&quot;) athletes ## # A tibble: 33 x 10 ## m100 long weight highj m400 m110 disc pole javel m1500 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 11.2 7.43 15.5 2.27 48.9 15.1 49.3 4.7 61.3 269. ## 2 10.9 7.45 15.0 1.97 47.7 14.5 44.4 5.1 61.8 273. ## 3 11.2 7.44 14.2 1.97 48.3 14.8 43.7 5.2 64.2 263. ## 4 10.6 7.38 15.0 2.03 49.1 14.7 44.8 4.9 64.0 285. ## 5 11.0 7.43 12.9 1.97 47.4 14.4 41.2 5.2 57.5 257. ## 6 10.8 7.72 13.6 2.12 48.3 14.2 43.1 4.9 52.2 274. ## 7 11.2 7.05 14.1 2.06 49.3 14.4 41.7 5.7 61.6 291. ## 8 11.0 6.95 15.3 2 48.2 14.4 41.3 4.8 63 266. ## 9 11.2 7.12 14.5 2.03 49.2 14.7 42.4 4.9 66.5 270. ## 10 11.2 7.28 15.2 1.97 48.6 14.8 48.0 5.2 59.5 292. ## # ‚Ä¶ with 23 more rows athletes %&gt;% ggpairs() corrplot::corrplot(cor(athletes), method = &quot;ellipse&quot;, type = &quot;upper&quot;) pca &lt;- athletes %&gt;% scale() %&gt;% prcomp() summary(pca) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 1.8488 1.6144 0.97123 0.9370 0.74607 0.70088 0.65620 ## Proportion of Variance 0.3418 0.2606 0.09433 0.0878 0.05566 0.04912 0.04306 ## Cumulative Proportion 0.3418 0.6025 0.69679 0.7846 0.84026 0.88938 0.93244 ## PC8 PC9 PC10 ## Standard deviation 0.55389 0.51667 0.31915 ## Proportion of Variance 0.03068 0.02669 0.01019 ## Cumulative Proportion 0.96312 0.98981 1.00000 ## standard deviations of newly rotated variables pca$sdev ## [1] 1.8488478 1.6144328 0.9712345 0.9370279 0.7460742 0.7008762 0.6561975 ## [8] 0.5538936 0.5166715 0.3191460 ## p = 10 variables sum(pca$sdev^2) ## [1] 10 screeplot ## screeplot of sdev^2 fviz_screeplot(pca, choice = &quot;eigenvalue&quot;) + geom_hline(yintercept = 1) biplots fviz_pca_biplot(pca,geom = &quot;point&quot;) + geom_point (alpha = 0.2) discus, shot, &amp; javelin more strongly correlated with one another running events more strongly correlated to one another could think of PC1 as a fieldness variable (i.e., strong +ve loadings from field events and -ve loadings from track events) Finding correlation between a PC and the original variable: pca$rotation %*% diag(pca$sdev) ## [,1] [,2] [,3] [,4] [,5] [,6] ## m100 -0.7689031 0.24024073 -0.25977801 0.08276995 -0.329999387 0.02152557 ## long 0.7285412 -0.24552536 -0.16408953 0.22886871 0.275237129 -0.06572987 ## weight 0.4975355 0.78063857 0.09569838 0.10097671 -0.007277715 0.16121594 ## highj 0.3924767 0.04504025 -0.83039242 -0.36351428 -0.001399867 0.05224598 ## m400 -0.6579077 0.56853834 -0.18404546 -0.07550062 0.109647051 -0.22913668 ## m110 -0.8014415 0.11231318 -0.12253105 0.35821666 -0.066253470 0.14752835 ## disc 0.3250132 0.81260004 0.04477360 -0.02397295 0.014442957 0.43097750 ## pole 0.7101094 0.24149011 0.13293514 -0.13489967 -0.534743786 -0.24373698 ## javel 0.3326883 0.60049956 -0.18679561 0.56265307 0.071311293 -0.30659402 ## m1500 -0.3145678 0.67962013 0.21615050 -0.45506038 0.253495250 -0.21049009 ## [,7] [,8] [,9] [,10] ## m100 0.16693567 -0.367626287 0.056004765 -0.03494024 ## long 0.49249877 -0.078245304 -0.023838755 -0.01780972 ## weight -0.07261722 -0.040160366 -0.218281354 -0.20767995 ## highj -0.08866817 0.086094934 0.052734102 -0.03810980 ## m400 0.09274620 0.081333350 -0.336230314 0.10749282 ## m110 0.17883327 0.353939993 0.107074242 -0.08288795 ## disc 0.09447447 -0.005206846 0.086408421 0.17058453 ## pole 0.17931681 0.153358210 0.009126707 0.02103035 ## javel -0.22436043 -0.032413502 0.158202825 0.04178638 ## m1500 0.12262390 -0.004048987 0.236058037 -0.07759028 13.3.3 Ants (from previous chapter) ants &lt;- read_csv(&quot;pitfalls.csv&quot;) ## choose numeric variables only and transform ants_numeric &lt;- ants %&gt;% dplyr::select(where(is.numeric), -Month) %&gt;% mutate( Nyl = log(Nyl + 1), Phe = log(Phe + 1), Tet = log(Tet + 1), Pac = log(Pac + 1)) corrplot::corrplot(cor(ants_numeric), method = &quot;ellipse&quot;,type = &quot;upper&quot;) pca &lt;- ants_numeric %&gt;% scale() %&gt;% prcomp() summary(pca) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 1.5517 0.8361 0.7611 0.56041 ## Proportion of Variance 0.6019 0.1747 0.1448 0.07852 ## Cumulative Proportion 0.6019 0.7767 0.9215 1.00000 ## screeplot of sdev^2 fviz_screeplot(pca, choice = &quot;eigenvalue&quot;) + geom_hline(yintercept = 1) ## reduced space plot (biplot) fviz_pca_biplot(pca,geom = &quot;point&quot;) + geom_point (alpha = 0.2) Phe and Nyl strongly positively correlated PC2 has a large contribution from Tet Additional information fviz_pca_biplot(pca,geom = &quot;point&quot;,alpha = 0.2) + geom_point(aes(color = ants$Location)) + labs(color = &quot;Location&quot;) Central different from North and West? fviz_pca_biplot(pca,geom = &quot;point&quot;,alpha = 0.2) + geom_point(aes(color = ants$Habitat)) + labs(color = &quot;Habitat&quot;) Grass and Scrub intermediate? 13.4 Reality check: reducing noise... set.seed(1234) ## just for reproduciblity noise &lt;- as_tibble(replicate(10,rnorm(200, mean = 50, sd = 10))) noise ## # A tibble: 200 x 10 ## V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 37.9 54.9 37.7 53.1 39.8 37.9 36.0 29.9 47.5 44.7 ## 2 52.8 57.0 50.4 56.1 36.1 53.0 73.8 34.4 37.8 45.0 ## 3 60.8 51.9 45.8 33.1 49.5 34.6 58.7 69.6 49.3 38.5 ## 4 26.5 57.0 41.0 57.8 68.1 56.4 34.6 48.2 63.6 51.3 ## 5 54.3 53.1 54.2 50.1 49.0 57.0 61.3 63.6 45.4 54.7 ## 6 55.1 57.6 51.5 48.2 57.8 30.9 60.4 66.5 50.7 48.1 ## 7 44.3 68.4 64.6 61.1 39.0 59.4 55.9 51.2 57.8 33.8 ## 8 44.5 61.1 38.8 64.8 47.8 47.8 54.0 50.0 34.1 46.0 ## 9 44.4 50.3 44.8 38.5 55.7 43.3 51.4 52.0 36.3 54.1 ## 10 41.1 38.9 49.3 60.1 46.5 54.5 56.2 56.8 40.7 42.8 ## # ‚Ä¶ with 190 more rows corrplot::corrplot(cor(noise), method = &quot;ellipse&quot;,type = &quot;upper&quot;) pca &lt;- noise %&gt;% scale() %&gt;% prcomp() summary(pca) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 1.1890 1.1331 1.0936 1.0281 1.0075 0.96654 0.94742 ## Proportion of Variance 0.1414 0.1284 0.1196 0.1057 0.1015 0.09342 0.08976 ## Cumulative Proportion 0.1414 0.2698 0.3894 0.4951 0.5966 0.68998 0.77974 ## PC8 PC9 PC10 ## Standard deviation 0.90017 0.85665 0.81141 ## Proportion of Variance 0.08103 0.07339 0.06584 ## Cumulative Proportion 0.86078 0.93416 1.00000 fviz_screeplot(pca, choice = &quot;eigenvalue&quot;) + geom_hline(yintercept = 1) 13.5 Other resources: optional but recommended ClusterDucks Little book for Multivariate Analysis 'explor' is an R package to allow interactive exploration of multivariate analysis results The Mathematics Behind Principal Component Analysis (6 min read) 13.5.1 Multidimensional Scaling in R Multidimensional scaling (MDS) is actually the more general technique of dimension reduction. PCA is a special case of MDS! To carry out MDS in R library(ggfortify) ## Plotting Multidimensional Scaling (for interest) ## stats::cmdscale performs Classical MDS data(&quot;eurodist&quot;) ## road distances (in km) between 21 cities in Europe. autoplot(eurodist) ## Plotting Classical (Metric) Multidimensional Scaling autoplot(cmdscale(eurodist, eig = TRUE)) autoplot(cmdscale(eurodist, eig = TRUE), label = TRUE, shape = FALSE, label.size = 3) ## Plotting Non-metric Multidimensional Scaling ## MASS::isoMDS and MASS::sammon perform Non-metric MDS library(MASS) autoplot(sammon(eurodist)) ## Initial stress : 0.01705 ## stress after 10 iters: 0.00951, magic = 0.500 ## stress after 20 iters: 0.00941, magic = 0.500 autoplot(sammon(eurodist), shape = FALSE, label = TRUE,label.size = 3) ## Initial stress : 0.01705 ## stress after 10 iters: 0.00951, magic = 0.500 ## stress after 20 iters: 0.00941, magic = 0.500 ## Have a go at interpreting these plots based on the geography of the cities :-) "]]
