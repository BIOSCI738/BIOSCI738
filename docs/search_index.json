[["index.html", "Advanced Biological Data Analysis Nau mai haere mai welcome to BIOSCI 738", " Advanced Biological Data Analysis University of Auckland Lecturer: Dr.¬†C. M. Jones-Todd Nau mai haere mai welcome to BIOSCI 738 Artwork by @allison_horst With thanks to Prof.¬†Chris Triggs, Dr Kathy Ruggiero, and Prof.¬†James Russell who all freely gave me their previous iterations of the course and told me to do what I wanted with their notes. I did, and they may well find sections of this material hauntingly familiar! "],["useful-information-to-set-you-up-for-your-semester.html", "Useful information to set you up for your semester", " Useful information to set you up for your semester Course outline This is a postgraduate course geared towards students of biology, ecology, and environmental science. It is suited to students with an interest in (bio)statistics who would like to equip themselves with the know-how to be able to correctly prepare experiments, analyse data, interpret their results and draw valid conclusions. The statistical concepts and methods taught in this course will provide students with the tools to make and evaluate scientific discoveries as well as propose and justify decisions based on data. The course builds on assumed knowledge of some fundamental statistical concepts. It is expected that students are comfortable with the statistical content covered in a typical stage 2 biostatistics course (e.g., BIOSCI220). This course will use the programming language R (through RStudio) and students are expected to be familiar with the basics of data import, manipulation, and visualisation using R. If you are unfamiliar with R it is expected that you will prepare accordingly prior to lectures and labs. The course will also introduce you to version control (via git and GitHub); no previous experience with these systems is expected. Learning Outcomes By the end of this course you will be able to: Discuss and critically evaluate the provenance of data and create informative visualisations. Develop and demonstrate effective Rprogramming and version control skills. Create and maintain a reproducible project directory. Describe, analyse and interpret different types of experimental designs identifying the potential sources of variation. Formulate an appropriate hypothesis associated with an experimental design. Perform, interpret, and critique multivariate data techniques. Communicate statistical concepts and experimental outcomes clearly using language appropriate for both a scientific and non-scientific audience. Perform, interpret, and critique a range of statistical regression techniques. Course summary Lectures this semester are in weeks (2‚Äì11) on Mondays 2-4pm in MAC1. Labs are held in 106-014 (Biology Building, Room 014) on Fridays (weeks 2‚Äì11) 2‚Äì3pm Office hours: Mondays 12‚Äì1pm &amp; Thursdays 2‚Äì3pm throughout the semester. Contact me: c.jonestodd@auckland.ac.nz or at science building 303, level 3, office 318 Each week some lecture material will be recorded and uploaded to CANVAS. It is expected that you will have watched these prior to coming along to the in-person lecture "],["module-1.html", "Module 1", " Module 1 Learning objectives Define the difference between R and RStudio Explain what an R function is; describe what an argument to an R function is Explain what an R package is; distinguish between the functions install.packages() and library() Use the appropriate R function to read in a data file Explain the importance of reproducibility in terms of scientific research Use the functionality offered by git and GitHub through RStudio Define and discuss MƒÅori Data Sovereignty principles Define data sovereignty and explain this in relation to a researcher‚Äôs obligation when collecting, displaying, and analysing data Carry out and interpret the outputs of basic exploratory data analysis using in-built R functions Create and communicate informative data visualisations using R Discuss and critique data visualisations Other resources R for Data Science RStudio Education An Introduction to R R for Biologists The Popularity of Data Science Software Happy Git and GitHub for the useR Why data sovereignty matters Indigenous Data Sovereignty and Policy Principles of MƒÅori Data Sovereignty Good data practices for Indigenous data sovereignty and governance. ggplot2 cheatsheet Ten Simple Rules for Better Figures Elegant Graphics for Data Analysis Using ggplot2 to communicate your results Interesting blogs on graphs in the media tidyverse Tidy Data Palmer penguins "],["r-and-rstudio.html", "R and RStudio", " R and RStudio Intro to R &amp; RStudio R is the pheromone to RStudio‚Äôs PDA R is the pheromone to RStudio‚Äôs PDA. R is a language, specifically, a programming language; it‚Äôs the way you can speak to your computer to ask it to carry out certain computations. RStudio is an integrated development environment (IDE). This means it is basically an interface, albeit a fancy one, that makes it easier to communicate with your computer in the language R. The main benefit is the additional features it has that enable you to more efficiently speak R. Note R and RStudio are two different pieces of software; for this course you are expected to download both. As you‚Äôd expect, the PDA depends on the pheromones (i.e., RStudio depends on R) so you have to download R to use RStudio! Why? R It‚Äôs free It‚Äôs open source A general-purpose of programming language Written by statisticians (here in Auckland!) It‚Äôs available for all operating systems (Windows, Linux, and Mac) There is a huge online support network It‚Äôs extremely flexible; if you can code it you can do it! 15,000+ packages available! ‚Ä¶ RStudio ‚ÄúIf R were an airplane, RStudio would be the airport‚Ä¶‚Äù ‚Äî Julie Lowndes, Introduction to RStudio Awesomeness Speaks nicely to R Tab completion Debugging capabilities There is a huge online support network Offers many other features and tools to make your workflow with R easier It facilitates reproducibility ‚Ä¶ Installing R and RStudio As mentioned above RStudio depends on R so there is an order you should follow when you download these software. Download and install R by following these instructions. Make sure you choose the correct operating system. Download and install RStudio by going here choosing RStudio Desktop Open Source License Free and following instructions. Check all is working Open up RStudio from your computer menu, the icon will look something like this (DO NOT use this icon , this is a link to R and will only open a very basic interface) Wait a little and you should see RStudio open up to something similar to the screenshot below Pay close attention to the notes in the screenshot and familiarise yourself with the terms. Finally, in the Console next to the prompt type 1:10 and press enter on your keyboard. Your computer should say something back you (in the Console)! What do you think you were asking it to do? Does the output make sense?1 Getting started As in step 3. above open up RStudio from your computer menu, the icon will look something like this . Using the diagram above identify the different panes: Console where you directly type command in and communicate with your computer (via the language R). Environment pane Files pane Some terminology Running code: the act of telling R to perform an act by giving it commands in the console. Objects: where values are saved in (see later for creating an object. Script: a text file containing a set of commands and comments. Comments: notes written within a Script to better document/explain what‚Äôs happening R errors üò± data &lt;- read.csv(&quot;data_file_not_in_my_working_directory.csv&quot;) ## Warning in file(file, &quot;rt&quot;): cannot open file ## &#39;data_file_not_in_my_working_directory.csv&#39;: No such file or directory ## Error in file(file, &quot;rt&quot;): cannot open the connection library(some_library_I_have_not_installed) ## Error in library(some_library_I_have_not_installed): there is no package called &#39;some_library_I_have_not_installed&#39; some_function_I_spelled_worng(x = x) ## Error in some_function_I_spelled_worng(x = x): could not find function &quot;some_function_I_spelled_worng&quot; an_object_I_have_not_created ## Error in eval(expr, envir, enclos): object &#39;an_object_I_have_not_created&#39; not found What do you think the issues are here üòâ R Scripts (a .r file) Go File &gt; New File &gt; R Script to open up a new Script If you had only three panes showing before, a new (fourth) pane should open up in the top left of RStudio. This file will have a .r extension and is where you can write, edit, and save the R commands you write. It‚Äôs a dedicated text editor for your R code (very useful if you want to save your code to run at a later date). The main difference between typing your code into a Script vs Console is that you edit it and save it for later! Remember though the Console is the pane where you communicate with your computer so all code you write will have to be Run here. There are two ways of running a line of code you‚Äôve written in your Script Ensure your cursor is on the line of code you want to run, hold down Ctrl and press Enter. Ensure your cursor is on the line of code you want to run, then use your mouse to click the Run button (it has a green arrow next to it) on the top right of the Script pane. Type 1:10 in your Script and practise running this line of code using both methods above. Not that if you‚Äôve Run the code successfully then your computer will speak back to you each time via the Console Writing Comments Comments are notes to yourself (future or present) or to someone else and are, typically, written interspersed in your code. Now, the comments you write will typically be in a language your computer doesn‚Äôt understand (e.g., English). So that you can write yourself notes in your Script you need to tell your computer using the R language to ignore them. To do this precede any note you write with #, see below. The # is R for ignore anything after this character. ## IGNORE ME ## I&#39;m a comment ## I repeat I&#39;m a comment ## I am not a cat ## OK let&#39;s run some code 2 + 2 ## [1] 4 ## Hmm maybe I should check this ## @kareem_carr ;-) Now remember when you want to leave your R session you‚Äôll need to Save your Script to use it again. To do this go File &gt; Save As and name your file what you wish (remember too to choose a relevant folder on your computer, or as recommended use the .Rproj set-up as above). Change the RStudio appearance up to your taste Go to Tools &gt; Global Options &gt; Apperance Reproducible research Keep all similar files for the same analysis in the same place NEVER change raw data Good practice Always start with a clean workspace Why? So your ex (code) can‚Äôt come and mess up your life! Go to Tools &gt; Global Options Project-oriented workflow. Recommended: .Rproj Organised Set up each Each assignment/university course as a project Self-contained a project is a folder that contains all relevant files All paths can then be relative to that project Reproducible the project should just work on a different computer Got to Project (top right) &gt; New Project &gt; Create Project Project set-up ‚ö†Ô∏èWarning‚ö†Ô∏è Jenny Bryan will set your computer on fire üî• if you start your script like this rm(list = ls()) This does NOT create a fresh R process it makes your script vulnerable it will come back to bite you You should have seen the numbers 1 to 10 printed out as a sequence.‚Ü© "],["version-control-with-git-and-github.html", "Version control with git and GitHub", " Version control with git and GitHub git the software ‚ÄúTrack Changes features from Microsoft Word on steroids‚Äù ‚Äî Jenny Bryan a version control system manages the evolution of a set of files (tidily) GitHub an online hosting service ‚ÄúThink of it as DropBox but much, much better‚Äù ‚Äî Jenny Bryan home for your Git-based projects on the internet Setup TL;DR Register an account with GitHub https://github.com Make sure you‚Äôve got the latest version of R R.version.string ## [1] &quot;R version 4.1.2 (2021-11-01)&quot; Upgrade RStudio to the new preview version (optional) Install git: follow these instructions Get started Cloning a repository from GitHub using RStudio On GitHub, navigate to the Code tab of the repository On the right side of the screen, click Clone or download Click the Copy to clipboard icon to the right of the repository URL (e.g., https://github.com/STATS-UOA/workshops-biosci738.git) + Open RStudio in your local environment Click File, New Project, Version Control, Git + Paste the repository URL and enter TAB to move to the Project directory name field. I‚Äôve chosen to store this folder on my Desktop, obviously put it wherever you wish :-) Click Create Project. Your Files pane should now look similar to this "],["exploratory-data-analysis.html", "Exploratory Data Analysis", " Exploratory Data Analysis or EDA we will be using tidyverse. ‚Äòtidyverse‚Äô is a collection of R packages that all share underlying design philosophy, grammar, and data structures. They are specifically designed to make data wrangling, manipulation, visualisation, and analysis simpler. Starting out with tidyverse Artwork by [@allison_horst](https://github.com/allisonhorst/) Starting out with tidyverse To install all the packages that belong to the tidyverse run ## request (download) the tidyverse packages from the centralised library install.packages(&quot;tidyverse&quot;) To tell your computer to access the tidyverse functionality in your session run (Note you‚Äôll have to do this each time you start up an R session): ## Get the tidyverse packages from our local library library(tidyverse) Reading in data from a .csv file First off download the paua.csv file from CANVAS To read the data into RStudio In the Environment pane click Import Dataset &gt; ** From Text (readr)** &gt; Browse &gt; Choose your file, remembering which folder you downloaded it to. this is where .Rproj is useful &gt; Another pane should pop up, check the data looks as you might expect &gt; Import Or paua &lt;- read_csv(&quot;paua.csv&quot;) Explore your data Let‚Äôs have a look at your data in the Console paua ## # A tibble: 60 √ó 3 ## Species Length Age ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Haliotis iris 1.8 1.50 ## 2 Haliotis australis 5.4 11.9 ## 3 Haliotis australis 4.8 5.42 ## 4 Haliotis iris 5.75 4.50 ## 5 Haliotis iris 5.65 5.50 ## 6 Haliotis iris 2.8 2.50 ## 7 Haliotis australis 5.9 6.49 ## 8 Haliotis iris 3.75 5.00 ## 9 Haliotis australis 7.2 8.56 ## 10 Haliotis iris 4.25 5.50 ## # ‚Ä¶ with 50 more rows ###Explore your data Using the glimpse() command for an alternative view glimpse(paua) ## Rows: 60 ## Columns: 3 ## $ Species &lt;chr&gt; &quot;Haliotis iris&quot;, &quot;Haliotis australis&quot;, &quot;Haliotis australis&quot;, &quot;‚Ä¶ ## $ Length &lt;dbl&gt; 1.80, 5.40, 4.80, 5.75, 5.65, 2.80, 5.90, 3.75, 7.20, 4.25, 6.‚Ä¶ ## $ Age &lt;dbl&gt; 1.497884, 11.877010, 5.416991, 4.497799, 5.500789, 2.500972, 6‚Ä¶ The pipe operator %&gt;% A nifty tidyverse tool is called the pipe operator %&gt;%. The pipe operator allows us to combine multiple operations in R into a single sequential chain of actions. Say you would like to perform a hypothetical sequence of operations on a hypothetical data frame x using hypothetical functions f(), g(), and h(): Take x then Use x as an input to a function f() then Use the output of this as an input to a function g() then Use the output of this as an input to a function h() So to calculate the mean Age of each Species in the paua dataset we would use paua %&gt;% group_by(Species) %&gt;% summarize(mean_age = mean(Age)) ## # A tibble: 2 √ó 2 ## Species mean_age ## &lt;chr&gt; &lt;dbl&gt; ## 1 Haliotis australis 7.55 ## 2 Haliotis iris 4.40 You would read the sequence above as: Take the paua data.frame then Use this and apply the group_by() function to group by Species Use this output and apply the summarize() function to calculate the mean Age of each group (Species), calling the resulting number mean_age Or to describe my daily routine‚Ä¶ I %&gt;% wake_up(time = &quot;later than I should&quot;) %&gt;% give(who = &quot;Watson&quot; , what = &quot;medication&quot;) %&gt;% make(who= &quot;myself&quot;, what = &quot;coffee&quot;) %&gt;% drink() %&gt;% try(remember_what_I_have_on(date = &quot;today&quot;)) Have a go at writing your own! "],["mƒÅori-data-sovereignty-principles.html", "MƒÅori Data Sovereignty principles", " MƒÅori Data Sovereignty principles ‚Äô‚ÄòData sovereignty is the idea that data are subject to the laws and governance structures within the nation it is collected‚Äô‚Äô ‚ÄúMƒÅori Data Sovereignty has emerged as a critical policy issue as Aotearoa New Zealand develops world-leading linked administrative data resources.‚Äù ‚Äî Andrew Sporle, Maui Hudson, Kiri West. Chapter 5, Indigenous Data Sovereignty and Policy ‚ÄúFor Indigenous peoples, historical encounters with statistics have been fraught, and none more so than when involving official data produced as part of colonial attempts at statecraft.‚Äù ‚Äî Lovett, R., Lee, V., Kukutai, T., Cormack, D., Rainie, S.C. and Walker, J., 2019. Good data practices for Indigenous data sovereignty and governance. Good data, pp.26-36. ‚ÄúMƒÅori Data Sovereignty has emerged as a critical policy issue as Aotearoa New Zealand develops world-leading linked administrative data resources.‚Äù ‚Äî Andrew Sporle, Maui Hudson, Kiri West. Chapter 5, Indigenous Data Sovereignty and Policy ‚ÄúMƒÅori data refers to data produced by MƒÅori or that is about MƒÅori and the environments we have relationships with.‚Äù ‚Äî Te Mana Raraunga Charter Data is a ‚Äúpotential taonga, something precious that needs to be maintained, in relation to its utility‚Äù ‚Äî Dr W. Edwards, TMR website Te Tiriti o Waitangi/Treaty of Waitangi obliges the Government to actively protect taonga, consult with MƒÅori in respect of taonga, give effect to the principle of partnership and recognize MƒÅori rangatiratanga over taonga. Factors that relate to how communities might recognize the taonga nature of any dataset include provenance of the data: does the dataset come from a significant MƒÅori source? opportunity for the data: could the dataset support MƒÅori aspirations for their people or their whenua (land)? utility of the data: does the dataset have multiple uses? MƒÅori Data Sovereignty principles to inform the recognition of MƒÅori rights and interests in data, and the ethical use of data to enhance MƒÅori well-being: Rangatiratanga, authority MƒÅori have an inherent right to exercise control over MƒÅori data and MƒÅori data ecosystems. This right includes, but is not limited to, the creation, collection, access, analysis, interpretation, management, security, dissemination, use and reuse of MƒÅori data. Decisions about the physical and virtual storage of MƒÅori data shall enhance control for current and future generations. Whenever possible, MƒÅori data shall be stored in Aotearoa New Zealand. MƒÅori have the right to data that is relevant and empowers sustainable self-determination and effective self-governance Whakapapa, relationships All data has a whakapapa (genealogy). Accurate metadata should, at minimum, provide information about the provenance of the data, the purpose(s) for its collection, the context of its collection, and the parties involved. The ability to disaggregate MƒÅori data increases its relevance for MƒÅori communities and iwi. MƒÅori data shall be collected and coded using categories that prioritise MƒÅori needs and aspirations. Current decision-making over data can have long-term consequences, good and bad, for future generations of MƒÅori. A key goal of MƒÅori data governance should be to protect against future harm. Whanaungatanga, obligations Individuals‚Äô rights (including privacy rights), risks and benefits in relation to data need to be balanced with those of the groups of which they are a part. In some contexts, collective MƒÅori rights will prevail over those of individuals. Individuals and organisations responsible for the creation, collection, analysis, management, access, security or dissemination of MƒÅori data are accountable to the communities, groups and individuals from whom the data derive Kotahitanga, collective benefit Data ecosystems shall be designed and function in ways that enable MƒÅori to derive individual and collective benefit. Build capacity. MƒÅori Data Sovereignty requires the development of a MƒÅori workforce to enable the creation, collection, management, security, governance and application of data. Connections between MƒÅori and other Indigenous peoples shall be supported to enable the sharing of strategies, resources and ideas in relation to data, and the attainment of common goals. Manaakitanga, reciprocity The collection, use and interpretation of data shall uphold the dignity of MƒÅori communities, groups and individuals. Data analysis that stigmatises or blames MƒÅori can result in collective and individual harm and should be actively avoided. Free, prior and informed consent shall underpin the collection and use of all data from or about MƒÅori. Less defined types of consent shall be balanced by stronger governance arrangements. Kaitiakitanga, guardianship MƒÅori data shall be stored and transferred in such a way that it enables and reinforces the capacity of MƒÅori to exercise kaitiakitanga over MƒÅori data. Ethics. Tikanga, kawa (protocols) and mƒÅtauranga (knowledge) shall underpin the protection, access and use of MƒÅori data. MƒÅori shall decide which MƒÅori data shall be controlled (tapu) or open (noa) access. "],["data-wrangling.html", "Data wrangling", " Data wrangling Common dataframe manipulations in the tidyverse Using dplyr and tidyr tidy data ‚ÄúTidy datasets are all alike, but every messy dataset is messy in its own way.‚Äù ‚Äî Hadley Wickham There are three interrelated rules which make a dataset tidy: Each variable must have its own column Each observation must have its own row Each value must have its own cell [illustrations from the Openscapes blog Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst Why ensure that your data is tidy? Consistency: using a consistent format aids learning and reproducibility Simplicity: it‚Äôs a format that is well understood by R ‚ÄúTidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets.‚Äù ‚Äî Hadley Wickham, Tidy data Introuducing the Palmer penguins library(palmerpenguins) ## contains some nice penguin data penguins ## # A tibble: 344 √ó 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen NA NA NA NA ## 5 Adelie Torgersen 36.7 19.3 193 3450 ## 6 Adelie Torgersen 39.3 20.6 190 3650 ## 7 Adelie Torgersen 38.9 17.8 181 3625 ## 8 Adelie Torgersen 39.2 19.6 195 4675 ## 9 Adelie Torgersen 34.1 18.1 193 3475 ## 10 Adelie Torgersen 42 20.2 190 4250 ## # ‚Ä¶ with 334 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; So, what does this show us? A tibble: 344 x 8: A tibble is a specific kind of data frame in R. The penguin dataset has 344 rows (i.e., 344 different observations). Here, each observation corresponds to a penguin. 8 columns corresponding to 3 variables describing each observation. species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, sex, and year are the different variables of this dataset. We then have a preview of the first 10 rows of observations corresponding to the first 10 penguins. ``... with 334 more rows indicates there are 334 more rows to see, but these have not been printed (likely as it would clog our screen) To learn more about the penguins read the paper that talks all about the data collection. Common dataframe manipulations in the tidyverse, using dplyr and tidyr Even from these first few rows of data we can see that there are some NA values. Let‚Äôs count the number of NAs. Remember the %&gt;% operator? Here we‚Äôre going to be introduced to a few new things the apply() function, the is.na() function, and how R deals with logical values! library(tidyverse) penguins %&gt;% apply(.,2,is.na) %&gt;% apply(.,2,sum) ## species island bill_length_mm bill_depth_mm ## 0 0 2 2 ## flipper_length_mm body_mass_g sex year ## 2 2 11 0 There‚Äôs lot going on in that code! Let‚Äôs break it down Take penguins then Use penguins as an input to the apply() function (this is specified as the first argument using the .) Now the apply() function takes 3 arguments: the data object you want it to apply something to (in our case penguins) the margin you want to apply that something to; 1 stands for rows and 2 stands for columns, and the function you want it to apply (in our case is.na()). So the second line of code is asking R to apply the is.na() function over the columns of penguins is.na() asks for each value it‚Äôs fed is it an NA value; it returns a TRUE if so and a FALSE otherwise The output from the first apply() is then fed to the second apply() (using the .). The sum() function then add them up! R treats a TRUE as a 1 and a FALSE as a 0. So how many NAs do you think there are! Doesn‚Äôt help much. To Now we know there are NA values throughout the data let‚Äôs remove then and create a new NA free version called penguins_nafree. There is a really handy tidyverse (dplyr) function for this! penguins_nafree &lt;- penguins %&gt;% drop_na() penguins_nafree ## # A tibble: 333 √ó 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen 36.7 19.3 193 3450 ## 5 Adelie Torgersen 39.3 20.6 190 3650 ## 6 Adelie Torgersen 38.9 17.8 181 3625 ## 7 Adelie Torgersen 39.2 19.6 195 4675 ## 8 Adelie Torgersen 41.1 17.6 182 3200 ## 9 Adelie Torgersen 38.6 21.2 191 3800 ## 10 Adelie Torgersen 34.6 21.1 198 4400 ## # ‚Ä¶ with 323 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; Below are some other useful manipulation functions; have a look at the outputs and run them yourselves and see if you can work out what they‚Äôre doing. filter(penguins_nafree, island == &quot;Torgersen&quot; ) ## # A tibble: 47 √ó 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen 36.7 19.3 193 3450 ## 5 Adelie Torgersen 39.3 20.6 190 3650 ## 6 Adelie Torgersen 38.9 17.8 181 3625 ## 7 Adelie Torgersen 39.2 19.6 195 4675 ## 8 Adelie Torgersen 41.1 17.6 182 3200 ## 9 Adelie Torgersen 38.6 21.2 191 3800 ## 10 Adelie Torgersen 34.6 21.1 198 4400 ## # ‚Ä¶ with 37 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; summarise(penguins_nafree, avgerage_bill_length = mean(bill_length_mm)) ## # A tibble: 1 √ó 1 ## avgerage_bill_length ## &lt;dbl&gt; ## 1 44.0 group_by(penguins_nafree, species) ## # A tibble: 333 √ó 8 ## # Groups: species [3] ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen 36.7 19.3 193 3450 ## 5 Adelie Torgersen 39.3 20.6 190 3650 ## 6 Adelie Torgersen 38.9 17.8 181 3625 ## 7 Adelie Torgersen 39.2 19.6 195 4675 ## 8 Adelie Torgersen 41.1 17.6 182 3200 ## 9 Adelie Torgersen 38.6 21.2 191 3800 ## 10 Adelie Torgersen 34.6 21.1 198 4400 ## # ‚Ä¶ with 323 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; Often we want to summarise variables by different groups (factors). Below we Take the penguins_nafree data then Use this and apply the group_by() function to group by species Use this output and apply the summarize() function to calculate the mean (using (mean()) bill length (bill_length_mm) of each group (species), calling the resulting number avgerage_bill_length penguins_nafree %&gt;% group_by(species) %&gt;% summarise(avgerage_bill_length = mean(bill_length_mm)) ## # A tibble: 3 √ó 2 ## species avgerage_bill_length ## &lt;fct&gt; &lt;dbl&gt; ## 1 Adelie 38.8 ## 2 Chinstrap 48.8 ## 3 Gentoo 47.6 We can also group by multiple factors, for example, penguins_nafree %&gt;% group_by(island,species) %&gt;% summarise(avgerage_bill_length = mean(bill_length_mm)) ## `summarise()` has grouped output by &#39;island&#39;. You can override using the `.groups` argument. ## # A tibble: 5 √ó 3 ## # Groups: island [3] ## island species avgerage_bill_length ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 Biscoe Adelie 39.0 ## 2 Biscoe Gentoo 47.6 ## 3 Dream Adelie 38.5 ## 4 Dream Chinstrap 48.8 ## 5 Torgersen Adelie 39.0 "],["data-viz.html", "Data Viz", " Data Viz ‚Äú‚Ä¶have obligations in that we have a great deal of power over how people ultimately make use of data, both in the patterns they see and the conclusions they draw.‚Äù ‚Äî Michael Correll, Ethical Dimensions of Visualization Research ‚ÄúClutter and confusion are not attributes of data - they are shortcomings of design.‚Äù ‚Äî Edward Tufte Exploratory and explanatory plots Exploratory plots (for you) data exploration doesn‚Äôt have to look pretty just needs to get to the point explore and discover new data facets formulate new questions For example, Explanatory plots (for others), most common kind of graph used in scientific publications clear purpose designed for the audience make it easy to read (this covers a lot of things) do not distort guide the reader to a particular conclusion answer a specific question support a decision For example, Plots by Cedric Scherer and mentioned on this blog Ten Simple Rules for Better Figures ‚ÄúScientific visualization is classically defined as the process of graphically displaying scientific data. However, this process is far from direct or automatic. There are so many different ways to represent the same data: scatter plots, linear plots, bar plots, and pie charts, to name just a few. Furthermore, the same data, using the same type of plot, may be perceived very differently depending on who is looking at the figure. A more accurate definition for scientific visualization would be a graphical interface between people and data.‚Äù ‚Äî Nicolas P. Rougier, Michael Droettboom, Philip E. Bourne, Ten Simple Rules for Better Figures Know Your Audience Identify Your Message Adapt the Figure to the Support Medium Captions Are Not Optional Do Not Trust the Defaults Use Color Effectively Do Not Mislead the Reader There are formulas to measure how misleading a graph is! Avoid Chartjunk Message Trumps Beauty ‚Äúmessage and readability of the figure is the most important aspect while beauty is only an option‚Äù ‚Äî Nicolas P. Rougier, Michael Droettboom, Philip E. Bourne, Ten Simple Rules for Better Figures Get the Right Tool I‚Äôm an advocate for R üòâ ggplot2 ggplot2 is an R package for producing statistical, or data, graphics; it has an underlying grammar based on the Grammar of Graphics Every ggplot2 plot has three key components: data, A set of aesthetic mappings between variables in the data and visual properties, and At least one layer which describes how to render each observation. Layers are usually created with a geom function. ggplot2 layers Examples Scatter plot using geom_point() ggplot(penguins,aes(x = body_mass_g, y = flipper_length_mm)) + ## data &amp; aesthetics geom_point() + ## geom geom_smooth(method = &#39;lm&#39;, se = FALSE) ## statistics (linear regression line) Boxplot using geom_boxplot() ggplot(penguins,aes(x = species, y = flipper_length_mm)) + ## data &amp; aesthetics geom_boxplot() + ## geom ggtitle(&quot;Flipper length (mm) by species&quot;) + ylab(&quot;Flipper length (mm)&quot;) + xlab(&quot;Species&quot;) + theme_dark() ## theme Scatter plot specifying color using geom_point() ggplot(penguins,aes(x = body_mass_g, y = flipper_length_mm, color = species)) + ## data and aesthetics geom_point() + ## geom geom_smooth(method = &#39;lm&#39;, se = FALSE) ## statistic (linear regression line without intervals) The Good, the Bad, and the Ugly‚Ä¶ box &lt;- ggplot(penguins,aes(x = species, y = flipper_length_mm)) + ## data &amp; aesthetics geom_boxplot() + ## geom ggtitle(&quot;Flipper length (mm) by species&quot;) + ylab(&quot;Flipper length (mm)&quot;) + xlab(&quot;Species&quot;) + theme_dark() ## theme box jitter &lt;- ggplot(penguins,aes(x = species, y = flipper_length_mm)) + ## data &amp; aesthetics geom_jitter() + ## geom ggtitle(&quot;Flipper length (mm) by species&quot;) + ylab(&quot;Flipper length (mm)&quot;) + xlab(&quot;Species&quot;) + theme_dark() ## theme jitter violin &lt;- ggplot(penguins,aes(x = species, y = flipper_length_mm)) + ## data &amp; aesthetics geom_violin() + ## geom ggtitle(&quot;Flipper length (mm) by species&quot;) + ylab(&quot;Flipper length (mm)&quot;) + xlab(&quot;Species&quot;) + theme_dark() ## theme violin ggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm)) + geom_point() + geom_smooth(method = &quot;lm&quot;, col = &quot;blue&quot;, se = FALSE) ## `geom_smooth()` using formula &#39;y ~ x&#39; ggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm, col = species)) + geom_point(size = 2, alpha = 0.5) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + facet_grid(~ sex) + theme_bw() + labs(title = &quot;Flipper Length and Body Mass, by Sex &amp; Species&quot;, subtitle = paste0(nrow(penguins), &quot; of the Palmer Penguins&quot;), x = &quot;Body Mass (g)&quot;, y = &quot;Flipper Length (mm)&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; penguins_nafree &lt;- penguins %&gt;% drop_na() ggplot(penguins_nafree, aes(x = body_mass_g, y = flipper_length_mm, col = species)) + geom_point(size = 2, alpha = 0.5) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + facet_grid(~ sex) + theme_bw() + labs(title = &quot;Flipper Length and Body Mass, by Sex &amp; Species&quot;, subtitle = paste0(nrow(penguins_nafree), &quot; of the Palmer Penguins&quot;), x = &quot;Body Mass (g)&quot;, y = &quot;Flipper Length (mm)&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; "],["module-2.html", "Module 2", " Module 2 Learning objectives List the aims, write out the appropriate null and alternative hypothesis using statistical notation for a permutation (randomization) test Write R code to carry out a permutation test List the aims, write out the appropriate null and alternative hypothesis using statistical notation for a bootstrap procedure Write R code to carry out bootstapping Carry out and interpret tests for the existence of relationships between explanatory variables and the response in a linear model Write R code to fit a linear model with a single continuous explanatory variable Write R code to fit a linear model with a continuous explanatory variable and a factor explanatory variable Interpret estimated effects with reference to confidence intervals from linear regression models. Specifically the interpretation of the intercept the effect of a factor the effect of a one-unit increase in a numeric variable the effect of an x-unit increase in a numeric variable Make a point prediction of the response for a new observation Write R code to fit a linear model with interaction terms in the explanatory variables Interpret estimated effects with reference to confidence intervals from linear regression models. Specifically the interpretation of main effects in a model with an interaction the effect of one variable when others are included in the model Other resources Exploring interactions with continuous predictors in regression models The ASA Statement on p-Values: Context, Process, and Purpose Traditional name Model formula R code Simple regression \\(Y \\sim X_{continuous}\\) lm(Y ~ X) One-way ANOVA \\(Y \\sim X_{categorical}\\) lm(Y ~ X) Two-way ANOVA \\(Y \\sim X1_{categorical} + X2_{categorical}\\) lm(Y ~ X1 + X2) ANCOVA \\(Y \\sim X1_{continuous} + X2_{categorical}\\) lm(Y ~ X1 + X2) Multiple regression \\(Y \\sim X1_{continuous} + X2_{continuous}\\) lm(Y ~ X1 + X2) Factorial ANOVA \\(Y \\sim X1_{categorical} * X2_{categorical}\\) lm(Y ~ X1 * X2) or lm(Y ~ X1 + X2 + X1:X2) "],["permutation-tests.html", "Permutation tests", " Permutation tests The basic approach to permutation tests is straightforward: Choose a statistic to measure the effect in question (e.g., differences between group means) Calculate that test statistic on the observed data. Note this metric can be anything you wish Construct the sampling distribution that this statistic would have if the effect were not present in the population (i.e., the distribution under the Null hypothesis, \\(H_0\\)): For chosen number of times shuffle the data labels calculate the test statistic for the reshuffled data and retain Find the location of your observed statistic in the sampling distribution. The location of observed statistic in sampling distribution is informative: if in the main body of the distribution then the observed statistic could easily have occurred by chance if in the tail of the distribution then the observed statistic would rarely occur by chance and there is evidence that something other than chance is operating. Calculate the proportion of times your reshuffled statistics equal or exceed the observed. This p-value is the probability that we observe a statistic at least as ‚Äúextreme‚Äù as the one we observed State the strength of evidence against the null on the basis of this probability. Significance testing using permutation (randomisation) tests Permutation Test on Two Independent Samples PƒÅua shell lengths Remember the PƒÅua data from Chapter 1 One question we may want to ask is if on average the shell length differs between Species? Scientific question: Are the shell lengths of shells the same in both species? Null hypothesis: The distribution of shell lengths in Haliotis iris the same as in Haliotis australis Test statistic: Difference of sample means means &lt;- paua %&gt;% group_by(Species) %&gt;% summarise(means = mean(Length)) ggplot(paua,aes(x = Species, y = Length)) + geom_violin() + geom_point(alpha = 0.4) + ylab(&quot;Length (cms)&quot;) + xlab(&quot;&quot;) + theme_classic() + geom_point(data = means, aes(x = Species, y = means, color = Species), size = 2) + geom_hline(data = means, aes(yintercept = means, color = Species), lty = 2, alpha = 0.5) + theme(legend.position = &quot;none&quot;) + geom_text(data = means, aes(x = Species, y = means + 0.3, label = paste0(&quot;Species averege = &quot;,round(means,3)), color = Species)) ggplot(paua,aes(x = Length, fill = Species)) + geom_histogram(position = &quot;identity&quot;, alpha = 0.3) + xlab(&quot;Length (cms)&quot;) + ylab(&quot;&quot;) + theme_classic() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. But because the data are skewed and we‚Äôve likely got non-constant variances we may be better off adopting a randomization test, rather than a parametric t-test ## observed differences in means diff_in_means &lt;- (paua %&gt;% group_by(Species) %&gt;% summarise(mean = mean(Length)) %&gt;% summarise(diff = diff(mean)))$diff diff_in_means ## [1] -0.9569444 ## Number of times I want to randomise nreps &lt;- 1000 ## initialize empty array to hold results randomisation_difference_mean &lt;- numeric(nreps) set.seed(1234) ## *****Remove this line for actual analyses***** ## This means that each run with produce the same results and ## agree with the printout that I show. for (i in 1:nreps) { ## the observations data &lt;- data.frame(value = paua$Length) ## randomise labels data$random_labels &lt;-sample(paua$Species, replace = FALSE) ## randomised differences in mean randomisation_difference_mean[i] &lt;- (data %&gt;% group_by(random_labels) %&gt;% summarise(mean = mean(value)) %&gt;% summarise(diff = diff(mean)))$diff } ## results results &lt;- data.frame(randomisation_difference_mean = randomisation_difference_mean) ## How many randomised differences in means are as least as extreme as the one we observed ## absolute value as dealing with two tailed n_exceed &lt;- sum(abs(results$randomisation_difference_mean) &gt;= abs(diff_in_means)) n_exceed ## [1] 1 ## proportion n_exceed/nreps ## [1] 0.001 ggplot(results, aes(x = randomisation_difference_mean)) + geom_histogram() + theme_classic() + ylab(&quot;&quot;) + xlab(&quot;Differences between randomised group means&quot;) + geom_vline(xintercept = diff_in_means, col = &quot;cyan4&quot;, size = 1,alpha = 0.6) + annotate(geom = &#39;text&#39;, label = &quot;Observed difference between means&quot; , x = -Inf, y = Inf, hjust = 0, vjust = 1.5, color = &quot;cyan4&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. How would the parametric t-test have served? t.test(Length ~ Species, data = paua) ## ## Welch Two Sample t-test ## ## data: Length by Species ## t = 3.5404, df = 57.955, p-value = 0.0007957 ## alternative hypothesis: true difference in means between group Haliotis australis and group Haliotis iris is not equal to 0 ## 95 percent confidence interval: ## 0.4158802 1.4980086 ## sample estimates: ## mean in group Haliotis australis mean in group Haliotis iris ## 5.766667 4.809722 Not too different after all Jackal mandible lengths ## Mandible lengths (mm) for golden jackals (Canis aureus) of each sex from the British Museum jackal &lt;- data.frame(mandible_length_mm = c(120, 107, 110, 116, 114, 111, 113, 117, 114, 112, 110, 111, 107, 108, 110, 105, 107, 106, 111, 111), sex = rep(c(&quot;Male&quot;,&quot;Female&quot;), each = 10)) Scientific question: Are the jaw lengths of jackals the same in both sexes? Null hypothesis: The distribution of jaw lengths in male jackals the same as in in females Test statistic: Difference of sample means Rather than a for loop let‚Äôs try this another way. ## observed statistic jackal_mean_diff &lt;- (jackal %&gt;% group_by(sex) %&gt;% summarise(mean = mean(mandible_length_mm)) %&gt;% summarise(diff = diff(mean)))$diff ## Generate all possible combinations ## This time we&#39;re doing ALL possble ones ## rather than a rendom 1000 combinations &lt;- combn(20,10) ## Do the permutations permtest_combinations &lt;- apply(combinations, 2, function(x) mean(jackal$mandible_length_mm[x]) - mean(jackal$mandible_length_mm[-x])) ## Full Permutation test p.value length(permtest_combinations[abs(permtest_combinations) &gt;= jackal_mean_diff]) / choose(20,10) ## [1] 0.003334127 ## Now let&#39;s use 10000 random permutations, sample without replacement ## set up matrix random_perm &lt;- apply(matrix(0, nrow = 10000, ncol = 1), 1, function(x) sample(20)) random_mean_diff &lt;- apply(random_perm, 2, function(x){ z &lt;- jackal$mandible_length_mm[x] mean(z[jackal$sex == &quot;Male&quot;]) - mean(z[jackal$sex == &quot;Female&quot;]) }) random_p.value &lt;- length(random_mean_diff [abs(random_mean_diff) &gt;= jackal_mean_diff]) / 10000 ## note the abs() random_p.value ## [1] 0.0029 ## Now what about a t-test (two-sample) t.test(mandible_length_mm ~ sex, data = jackal)$p.value ## [1] 0.003359952 NOTE: We can extend the randomization test to make inference about any sample statistic (not just the mean) "],["the-bootstrap.html", "The bootstrap", " The bootstrap Recall that the sampling distribution shows us what would happen if we took very many samples under the same conditions. The bootstrap is a procedure for finding the (approximate) sampling distribution from just one sample. In brief, The original sample represents the distribution of the population from which it was drawn. Resamples, taken with replacement from the original sample are representative of what we would get from drawing many samples from the population (the distribution of the statistics calculated from each resample is known as the bootstrap distribution of the statistic). The bootstrap distribution of a statistic represents that statistic‚Äôs sampling distribution. Example: constructing bootstrap confidence intervals Old faithful is a gyser located in Yellowstone National Park, Wyoming. Below is a histogram of the durations of 299 consecutive eruptions. Clearly bimodal! library(tidyverse) MASS::geyser ## waiting duration ## 1 80 4.0166667 ## 2 71 2.1500000 ## 3 57 4.0000000 ## 4 80 4.0000000 ## 5 75 4.0000000 ## 6 77 2.0000000 ## 7 60 4.3833333 ## 8 86 4.2833333 ## 9 77 2.0333333 ## 10 56 4.8333333 ## 11 81 1.8333333 ## 12 50 5.4500000 ## 13 89 1.6166667 ## 14 54 4.8666667 ## 15 90 4.3833333 ## 16 73 1.7666667 ## 17 60 4.6666667 ## 18 83 2.0000000 ## 19 65 4.7333333 ## 20 82 4.2166667 ## 21 84 1.9000000 ## 22 54 4.9666667 ## 23 85 2.0000000 ## 24 58 4.0000000 ## 25 79 2.0000000 ## 26 57 4.0000000 ## 27 88 2.8333333 ## 28 68 4.5000000 ## 29 76 4.0666667 ## 30 78 3.7166667 ## 31 74 3.5166667 ## 32 85 4.4666667 ## 33 75 2.2166667 ## 34 65 4.8833333 ## 35 76 2.6000000 ## 36 58 4.1500000 ## 37 91 2.2000000 ## 38 50 4.7666667 ## 39 87 1.8333333 ## 40 48 4.6000000 ## 41 93 2.2666667 ## 42 54 4.1333333 ## 43 86 2.0000000 ## 44 53 4.0000000 ## 45 78 2.0000000 ## 46 52 4.0000000 ## 47 83 1.8833333 ## 48 60 4.2666667 ## 49 87 2.0833333 ## 50 49 4.4666667 ## 51 80 2.5000000 ## 52 60 4.0000000 ## 53 92 1.7666667 ## 54 43 4.3333333 ## 55 89 2.1833333 ## 56 60 4.4833333 ## 57 84 3.8833333 ## 58 69 3.3333333 ## 59 74 3.7333333 ## 60 71 4.0000000 ## 61 108 1.9500000 ## 62 50 5.2666667 ## 63 77 2.0000000 ## 64 57 4.0000000 ## 65 80 2.0000000 ## 66 61 4.0000000 ## 67 82 2.0000000 ## 68 48 4.0000000 ## 69 81 3.5333333 ## 70 73 2.1666667 ## 71 62 4.5000000 ## 72 79 2.0166667 ## 73 54 4.1500000 ## 74 80 4.2000000 ## 75 73 4.3333333 ## 76 81 1.9333333 ## 77 62 4.6500000 ## 78 81 3.8166667 ## 79 71 4.0333333 ## 80 79 4.1666667 ## 81 81 4.6666667 ## 82 74 1.8166667 ## 83 59 4.0000000 ## 84 81 3.0000000 ## 85 66 4.0000000 ## 86 87 2.0000000 ## 87 53 4.4500000 ## 88 80 2.0500000 ## 89 50 4.2500000 ## 90 87 1.9166667 ## 91 51 4.6666667 ## 92 82 1.7333333 ## 93 58 4.3833333 ## 94 81 1.7666667 ## 95 49 4.6000000 ## 96 92 1.8666667 ## 97 50 4.4500000 ## 98 88 1.6333333 ## 99 62 5.0333333 ## 100 93 1.8166667 ## 101 56 5.1000000 ## 102 89 1.6333333 ## 103 51 4.2833333 ## 104 79 2.0000000 ## 105 58 4.0000000 ## 106 82 2.0000000 ## 107 52 4.5333333 ## 108 88 2.0000000 ## 109 52 4.0000000 ## 110 78 2.9333333 ## 111 69 4.7333333 ## 112 75 3.9000000 ## 113 77 1.9500000 ## 114 53 4.1166667 ## 115 80 1.8000000 ## 116 55 4.6666667 ## 117 87 1.8333333 ## 118 53 4.7000000 ## 119 85 2.1166667 ## 120 61 4.7833333 ## 121 93 1.8166667 ## 122 54 4.1000000 ## 123 76 4.6500000 ## 124 80 4.0000000 ## 125 81 2.0000000 ## 126 59 4.0000000 ## 127 86 4.0000000 ## 128 78 4.2166667 ## 129 71 4.1333333 ## 130 77 3.9333333 ## 131 76 3.7500000 ## 132 94 4.4166667 ## 133 75 2.4666667 ## 134 50 4.1666667 ## 135 83 3.8000000 ## 136 82 4.3166667 ## 137 72 3.8666667 ## 138 77 4.6833333 ## 139 75 1.7000000 ## 140 65 4.9666667 ## 141 79 4.2666667 ## 142 72 4.5833333 ## 143 78 4.0000000 ## 144 77 4.0000000 ## 145 79 4.0000000 ## 146 75 4.0000000 ## 147 78 1.9833333 ## 148 64 4.6000000 ## 149 80 0.8333333 ## 150 49 4.9166667 ## 151 88 1.7333333 ## 152 54 4.5833333 ## 153 85 1.7000000 ## 154 51 4.7500000 ## 155 96 1.8333333 ## 156 50 4.5000000 ## 157 80 1.8666667 ## 158 78 4.4500000 ## 159 81 4.4500000 ## 160 72 4.0000000 ## 161 75 4.8000000 ## 162 78 4.0000000 ## 163 87 4.0000000 ## 164 69 2.0000000 ## 165 55 4.0000000 ## 166 83 1.9333333 ## 167 49 4.5833333 ## 168 82 2.0000000 ## 169 57 3.7000000 ## 170 84 2.8666667 ## 171 57 4.8333333 ## 172 84 3.4500000 ## 173 73 4.3833333 ## 174 78 1.8000000 ## 175 57 4.4000000 ## 176 79 2.4833333 ## 177 57 4.5166667 ## 178 90 2.1000000 ## 179 62 4.3500000 ## 180 87 4.3666667 ## 181 78 1.7833333 ## 182 52 4.9166667 ## 183 98 1.8166667 ## 184 48 4.0000000 ## 185 78 4.0000000 ## 186 79 4.0000000 ## 187 65 3.8666667 ## 188 84 1.8500000 ## 189 50 4.7000000 ## 190 83 2.0166667 ## 191 60 4.4666667 ## 192 80 1.8666667 ## 193 50 4.1666667 ## 194 88 1.9000000 ## 195 50 4.2500000 ## 196 84 3.2500000 ## 197 74 4.2166667 ## 198 76 1.8833333 ## 199 65 4.9833333 ## 200 89 1.8500000 ## 201 49 4.0000000 ## 202 88 1.9666667 ## 203 51 4.7666667 ## 204 78 4.0000000 ## 205 85 2.0000000 ## 206 65 4.0000000 ## 207 75 4.0000000 ## 208 77 2.3833333 ## 209 69 4.4166667 ## 210 92 4.2166667 ## 211 68 4.3666667 ## 212 87 2.0000000 ## 213 61 4.4500000 ## 214 81 1.7500000 ## 215 55 4.5000000 ## 216 93 1.6166667 ## 217 53 4.7000000 ## 218 84 2.5666667 ## 219 70 3.7000000 ## 220 73 4.2333333 ## 221 93 1.9333333 ## 222 50 4.3500000 ## 223 87 4.0000000 ## 224 77 4.0000000 ## 225 74 4.0000000 ## 226 72 4.2166667 ## 227 82 4.0000000 ## 228 74 4.1333333 ## 229 80 1.8833333 ## 230 49 4.4666667 ## 231 91 1.9500000 ## 232 53 4.2166667 ## 233 86 1.7166667 ## 234 49 4.4500000 ## 235 79 4.2500000 ## 236 89 3.9666667 ## 237 87 4.3833333 ## 238 76 1.9666667 ## 239 59 4.4500000 ## 240 80 4.2666667 ## 241 89 1.9166667 ## 242 45 4.4166667 ## 243 93 3.0000000 ## 244 72 4.0000000 ## 245 71 2.0000000 ## 246 54 4.0000000 ## 247 79 3.2833333 ## 248 74 1.8333333 ## 249 65 4.6166667 ## 250 78 1.8333333 ## 251 57 4.6166667 ## 252 87 4.6000000 ## 253 72 4.2500000 ## 254 84 1.9333333 ## 255 47 4.9833333 ## 256 84 1.9666667 ## 257 57 4.3000000 ## 258 87 4.2000000 ## 259 68 4.5333333 ## 260 86 4.4000000 ## 261 75 4.6166667 ## 262 73 2.0000000 ## 263 53 4.0000000 ## 264 82 4.0000000 ## 265 93 3.9166667 ## 266 77 2.0000000 ## 267 54 4.5000000 ## 268 96 1.8000000 ## 269 48 4.0000000 ## 270 89 2.7500000 ## 271 63 4.7333333 ## 272 84 3.9666667 ## 273 76 1.9500000 ## 274 62 4.9666667 ## 275 83 1.8500000 ## 276 50 4.8000000 ## 277 85 4.0000000 ## 278 78 4.0000000 ## 279 78 4.0000000 ## 280 81 4.0000000 ## 281 78 4.0000000 ## 282 76 4.0000000 ## 283 74 4.0000000 ## 284 81 2.0000000 ## 285 66 4.0000000 ## 286 84 1.9333333 ## 287 48 4.3333333 ## 288 93 1.6666667 ## 289 47 4.7666667 ## 290 87 1.9500000 ## 291 51 4.6833333 ## 292 78 1.9333333 ## 293 54 4.4166667 ## 294 87 2.1333333 ## 295 52 4.0833333 ## 296 85 2.0666667 ## 297 58 4.0000000 ## 298 88 4.0000000 ## 299 79 2.0000000 ggplot(data = MASS::geyser, aes(x = duration)) + geom_histogram() + xlab(&quot;Duration of eruptions (m)&quot;) Step 1: Calculating the observed mean eruption duration time: mean &lt;- MASS::geyser %&gt;% summarise(mean = mean(duration)) mean ## mean ## 1 3.460814 Step 2: Construct bootstrap distribution ## Number of times I want to bootstrap nreps &lt;- 1000 ## initialize empty array to hold results bootstrap_means &lt;- numeric(nreps) set.seed(1234) ## *****Remove this line for actual analyses***** ## This means that each run with produce the same results and ## agree with the printout that I show. for (i in 1:nreps) { ## bootstrap. note with replacement bootstrap_sample &lt;- sample(MASS::geyser$duration, replace = TRUE) ## bootstraped mean resample bootstrap_means[i] &lt;- mean(bootstrap_sample) } ## results results &lt;- data.frame(bootstrap_means = bootstrap_means) ggplot(data = results, aes(x = bootstrap_means)) + geom_histogram() + geom_vline(xintercept = as.numeric(mean)) + ggtitle(&quot;Bootstrap distribution&quot;) Bootstrap estimate of bias is the difference between the mean of the boostrap distribution and the value of the statistic in the original sample: bias &lt;- as.numeric(mean) - mean(results$bootstrap_means) bias ## [1] 0.001200111 Bootstrap standard error of a statistic is the standard deviation of its bootstrap distribution: sd(results$bootstrap_means) ## [1] 0.06740607 ## compare to SEM of original data MASS::geyser %&gt;% summarise(sem = sd(duration)/sqrt(length(duration))) ## sem ## 1 0.06638498 Bootstrap \\(t\\) confidence interval. If, for a sample of size \\(n\\) the boostrap distribution is approximately Normal and the estimate of bias is small then an approximate \\(C\\) confidence for the parameter corresponding to the statistic is: \\[\\text{statistic} \\pm t^* \\text{SE}_\\text{bootstrap}\\] where \\(t*\\) is the critical value of the \\(t_{n-1}\\) distribution with area \\(C\\) between \\(-t^*\\) and \\(t^*\\). For \\(C = 0.95\\): as.numeric(mean) + c(-1,1) * qt(0.975,298)*sd(results$bootstrap_means) ## [1] 3.328162 3.593466 So our 95% confidence interval is 3.3 to 3.6. Bootstrap \\(percentile\\) confidence interval. Use the bootstrap distribution itself to determine the limits of the confidence interval by taking the limits of the sorted, central \\(C\\) bulk of the distribution. For \\(C = 0.95\\): sort(results$bootstrap_means)[c(25,975)] ## [1] 3.328428 3.591081 Differences In summary, what is resampling? Any of a variety of methods for doing one of the following Estimating the precision of sample statistics (e.g., bootstrapping) Performing significance tests (e.g., permutation/exact/randomisation tests) Validating models (e.g., bootstrapping, cross validation) Permutation vs bootstrap test The permutation test exploits symmetry under the null hypothesis. A full permutation test p-value is exact, conditional on data values in the combined sample. A bootstrap estimates the probability mechanism that generated the samples under the null hypothesis. A bootstrap does not require any special symmetry or assumption or exchangability. "],["one-and-two-sample-tests.html", "One and two sample tests", " One and two sample tests Again, using the paua.csv data from CANVAS. Recall that the P\\(\\overline{\\text{a}}\\)ua dataset contains the following variables Age of P\\(\\overline{\\text{a}}\\)ua in years (calculated from counting rings in the cone) Length of P\\(\\overline{\\text{a}}\\)ua shell in centimeters Species of P\\(\\overline{\\text{a}}\\)ua: Haliotis iris (typically found in NZ) and Haliotis australis (less commonly found in NZ) library(tidyverse) paua &lt;- read_csv(&quot;paua.csv&quot;) glimpse(paua) ## Rows: 60 ## Columns: 3 ## $ Species &lt;chr&gt; &quot;Haliotis iris&quot;, &quot;Haliotis australis&quot;, &quot;Haliotis australis&quot;, &quot;‚Ä¶ ## $ Length &lt;dbl&gt; 1.80, 5.40, 4.80, 5.75, 5.65, 2.80, 5.90, 3.75, 7.20, 4.25, 6.‚Ä¶ ## $ Age &lt;dbl&gt; 1.497884, 11.877010, 5.416991, 4.497799, 5.500789, 2.500972, 6‚Ä¶ One-Sample t-test Using a violin plot we can look at the distribution of shell Length. We can calculate the average Length of all shells in our sample paua %&gt;% summarise(average_length = mean(Length)) ## # A tibble: 1 √ó 1 ## average_length ## &lt;dbl&gt; ## 1 5.19 What about drawing inference? Do we believe that the average length of P\\(\\overline{\\text{a}}\\)ua shells is, say, 5cm? We know our sample average, but can we make any claims based on this one number? How do we reflect our uncertainty about the population mean? (remember it‚Äôs the population we want to make inference on based on our sample!) Enter the Standard Error of the Mean, SEM, \\(= \\frac{\\sigma}{\\sqrt{n}}\\); where \\(\\sigma = \\sqrt{\\frac{\\Sigma_{i = 1}^n(x_i - \\bar{x})^2}{n-1}}\\) (\\(i = 1,...,n\\)) is the standard deviation (SD) of the sample, \\(n\\) is the sample size, and \\(\\bar{x}\\) is the sample mean. Calculating \\(\\Sigma_{i = 1}^n(x_i - \\bar{x})^2, i = 1,...,n\\) by hand. It‚Äôs the sum squared differences of the distances between the \\(i^{th}\\) observation and the sample mean \\(\\bar{x}\\) (denoted \\(\\mu_x\\) in the GIF below) So using the example values in the GIF ## our sample of values x &lt;- c(1,2,3,5,6,9) ## sample mean sample_mean &lt;- mean(x) sample_mean ## [1] 4.333333 ## distance from mean for each value distance_from_mean &lt;- x - sample_mean distance_from_mean ## [1] -3.3333333 -2.3333333 -1.3333333 0.6666667 1.6666667 4.6666667 ## squared distance from mean for each value squared_distance_from_mean &lt;- distance_from_mean^2 squared_distance_from_mean ## [1] 11.1111111 5.4444444 1.7777778 0.4444444 2.7777778 21.7777778 ## sum of the squared distances sum(squared_distance_from_mean) ## [1] 43.33333 Calculating SD and SEM Now what about the SD? Remember it‚Äôs the \\(\\sqrt{\\frac{\\Sigma_{i = 1}^n(x_i - \\bar{x})^2}{n-1}}\\) so = \\(\\sqrt{\\frac{43.3333333}{n-1}}\\) = \\(\\sqrt{\\frac{43.3333333}{6-1}}\\) = \\(\\sqrt{\\frac{43.3333333}{5}}\\) = 2.9439203. Or we could just use R‚Äôs sd() function sd(x) ## [1] 2.94392 So the SEM is \\(\\frac{\\text{SD}}{\\sqrt{n}}\\) = \\(\\frac{2.9439203}{\\sqrt{6}}\\) In R sd(x)/sqrt(length(x)) ## [1] 1.20185 For the paua data we can simply use the in-built functions in R to calculate the SEM sem &lt;- paua %&gt;% summarise(mean = mean(Length), sem = sd(Length)/sqrt(length(Length))) sem ## # A tibble: 1 √ó 2 ## mean sem ## &lt;dbl&gt; &lt;dbl&gt; ## 1 5.19 0.155 Visualising the uncertainty Recall that the SEM is a measure of uncertainty about the mean. So we can use it to express our uncertainty visually. Typically \\(\\pm\\) twice the SEM is the interval used: Why error bars that are \\(\\pm\\) twice the SEM? This is approximately the 95% confidence interval for the population mean (see lecture) The exact 95% CI is given by \\(\\bar{x}\\) (mean) \\(\\pm\\) \\(t_{df,1 - \\alpha/2}\\) \\(\\times\\) SEM df = degrees of freedom (in this situation df = n - 1) \\(\\alpha\\) = level of significance Each mean has its own confidence interval whose width depends on the SEM for that mean When the df (more on these later) are large (e.g.¬†30 or greater) and \\(\\alpha\\) = 0.05 \\(t_{df,1 - \\alpha/2}\\) = \\(t_{large,0.975}\\) \\(\\approx\\) 2. Hence, the 95% confidence interval for the population mean is approximately \\(\\bar{x}\\) (mean) \\(\\pm\\) 2 \\(\\times\\) SEM Back to our hypothesis test Question: Do we believe that the average length of P\\(\\overline{\\text{a}}\\)ua shells is 5cm? Formalizing into a hypothesis test: Null hypothesis: On average P\\(\\overline{\\text{a}}\\)ua shells are 5cm long Alternative hypothesis: On average P\\(\\overline{\\text{a}}\\)ua shells are not 5cm long Notationally: \\(H_0: \\mu = 5\\) vs \\(H_1: \\mu \\neq 5\\) (\\(\\mu\\) is the proposed mean) Calculating a statistic (we use a t-statistic) t-statistic \\(= \\frac{\\bar{x}- \\mu}{\\text{SEM}}\\) = \\(\\frac{5.1925 - 5}{0.155351}\\) = 1.239 \\(\\bar{x}\\) is the sample mean \\(\\mu\\) is the theoretical value (proposed mean) The corresponding p-value Recall that a p-value is the probability under a specified statistical model that a statistical summary of the data would be equal to or more extreme than its observed value So in this case it‚Äôs the probability, under the null hypothesis (\\(\\mu = 5\\)), that we would observe a statistic as least as extreme as we did. Under our null hypothesis the distribution of the t-statistic is as below. The one calculated from our hypothesis test was 1.2391. Now, remember that our alternative hypotheses was \\(H_1: \\mu \\neq 5\\) so we have to consider both sides of the inequality; hence, anything as least as extreme is either \\(&gt; 1.2391\\) or \\(&lt; -1.2391\\) to our observed statistic (vertical lines). Anything as least as extreme is therefore given by the grey shaded areas. We can calculate the p-value using the pt() function (where q is our calculated t-statistic, and df are the degrees of freedom from above): 2*(1 - pt(q = 1.2391,df = 59)) ## [1] 0.2202152 Or we could do all of the above in one step using R t.test(paua$Length, mu = 5 ) ## ## One Sample t-test ## ## data: paua$Length ## t = 1.2391, df = 59, p-value = 0.2202 ## alternative hypothesis: true mean is not equal to 5 ## 95 percent confidence interval: ## 4.881643 5.503357 ## sample estimates: ## mean of x ## 5.1925 Recall, that the p-value gives the probability that under our null hypothesis we observe anything as least as extreme as what we did (hence the \\(\\times 2\\), think of the grey shaded area in the graph). This probability is \\(\\sim\\) 22%. Do you think what we‚Äôve observed is likely under the null hypothesis? Does this plot help? The proposed mean is shown by the red horizontal line; the dashed line shows the sample mean and the dotted lines \\(\\pm\\) the SEM. Differences between two means Calculating the differences between species means: Haliotis australis average - Haliotis iris average = \\(\\mu_{\\text{Haliotis australis}} - \\mu_{\\text{Haliotis iris}}\\) = 5.767 - 4.81 = 0.957. Doesn‚Äôt really tell us much‚Ä¶ As above the average values are all well and good, but what about variation? Recall the SEM from the one-sample t-test? The same idea holds here, although the calculation is a little bit more complicated (as we have to think about the number of observations in each group). But from the two group SEMs we can calculate the Standard Error of the Difference between two means, SED. Independent samples t-test using t.test() Question: Do we believe that on average the length of P\\(\\overline{\\text{a}}\\)ua shells are equal between species Formalizing into a hypothesis test: Null hypothesis: On average the species‚Äô shells are the same length Alternative hypothesis: they aren‚Äôt! Notationally: \\(H_0: \\mu_{\\text{Haliotis iris}} - \\mu_{\\text{Haliotis australis}} = 0\\) vs \\(H_1: \\mu_{\\text{Haliotis iris}} \\neq \\mu_{\\text{Haliotis australis}}\\) + $\\mu_{j}$ is the average length for species $j =$ (*Haliotis iris*, *Haliotis australis*), Calculate the test statistic: t-statistic = \\(\\frac{\\bar{x}_{\\text{difference}} - \\mu}{\\text{SED}}\\) = \\(\\frac{\\bar{x}_{\\text{difference}} - 0}{\\text{SED}}\\) where \\(\\bar{x}_{\\text{difference}}\\) is the differences between the species` averages. Calculations area a little bit more tricky here so let‚Äôs use R: test &lt;- t.test(Length ~ Species, data = paua) ## printing out the result test ## ## Welch Two Sample t-test ## ## data: Length by Species ## t = 3.5404, df = 57.955, p-value = 0.0007957 ## alternative hypothesis: true difference in means between group Haliotis australis and group Haliotis iris is not equal to 0 ## 95 percent confidence interval: ## 0.4158802 1.4980086 ## sample estimates: ## mean in group Haliotis australis mean in group Haliotis iris ## 5.766667 4.809722 test$p.value ## [1] 0.0007956853 Listed are the t-statistic, t = 3.5403636 and the p-value, p-value = 7.956853310^{-4} for the hypothesis test outlined above. What would you conclude? "],["linear-regression.html", "Linear regression", " Linear regression Some mathematical notation Let‚Äôs consider a linear regression with a simple explanatory variable: \\[Y_i = \\alpha + \\beta_1x_i + \\epsilon_i\\] where \\[\\epsilon_i \\sim \\text{Normal}(0,\\sigma^2).\\] Here for observation \\(i\\) \\(Y_i\\) is the value of the response \\(x_i\\) is the value of the explanatory variable \\(\\epsilon_i\\) is the error term: the difference between \\(Y_i\\) and its expected value \\(\\alpha\\) is the intercept term (a parameter to be estimated), and \\(\\beta_1\\) is the slope: coefficient of the explanatory variable (a parameter to be estimated) Does this remind you of anything? Modeling Bill Depth Remember the penguins from Chapter 2? Key assumptions Independence There is a linear relationship between the response and the explanatory variables The residuals have constant variance The residuals are normally distributed library(tidyverse) library(palmerpenguins) penguins_nafree &lt;- penguins %&gt;% drop_na() ggplot(data = penguins_nafree, aes(x = bill_depth_mm)) + geom_histogram() + theme_classic() + xlab(&quot;Bill depth (mm)&quot;) First off let‚Äôs fit a null (intercept only) model. This in old money would be called a one sample t-test. slm_null &lt;- lm(bill_depth_mm ~ 1, data = penguins_nafree) summary(slm_null)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 17.16486 0.1079134 159.0614 1.965076e-315 Model formula This model, from above, is simply \\[Y_i = \\alpha + \\epsilon_i.\\] Here for observation \\(i\\) \\(Y_i\\) is the value of the response (bill_depth_mm) and \\(\\alpha\\) is a parameter to be estimated (typically called the intercept). Inference The (Intercept) term, 17.1648649, tells us the (estimated) average value of the response (bill_depth_mm), see penguins_nafree %&gt;% summarise(average_bill_depth = mean(bill_depth_mm)) ## # A tibble: 1 √ó 1 ## average_bill_depth ## &lt;dbl&gt; ## 1 17.2 The SEM (Std. Error) = 0.1079134. The hypothesis being tested is \\(H_0:\\) ((Intercept) ) \\(\\text{mean}_{\\text{`average_bill_depth`}} = 0\\) vs.¬†\\(H_1:\\) ((Intercept)) \\(\\text{mean}_{\\text{`average_bill_depth`}} \\neq 0\\) The t-statistic is given by t value = Estimate / Std. Error = 159.0614207 The p-value is given byPr (&gt;|t|) = 1.965076110^{-315}. So the probability of observing a t-statistic as least as extreme given under the null hypothesis (average bill depth = 0) given our data is 1.965076110^{-315}, pretty strong evidence against the null hypothesis I‚Äôd say! Single continuous variable Does bill_length_mm help explain some of the variation in bill_depth_mm? p1 &lt;- ggplot(data = penguins_nafree, aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + ylab(&quot;Bill depth (mm)&quot;) + xlab(&quot;Bill length (mm)&quot;) + theme_classic() p1 slm &lt;- lm(bill_depth_mm ~ bill_length_mm, data = penguins_nafree) Model formula This model is simply \\[Y_i = \\alpha + \\beta_1x_i + \\epsilon_i\\] where for observation \\(i\\) \\(Y_i\\) is the value of the response (bill_depth_mm) and \\(x_i\\) is the value of the explanatory variable (bill_length_mm); As above \\(\\alpha\\) and \\(\\beta_1\\) are parameters to be estimated. We could also write this model as \\[ \\begin{aligned} \\operatorname{bill\\_depth\\_mm} &amp;= \\alpha + \\beta_{1}(\\operatorname{bill\\_length\\_mm}) + \\epsilon \\end{aligned} \\] Fitted model As before we can get out estimated parameters (here \\(\\alpha\\) and \\(\\beta_1\\)) using summary(slm)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.78664867 0.85417308 24.335406 1.026904e-75 ## bill_length_mm -0.08232675 0.01926835 -4.272642 2.528290e-05 Here, the (Intercept): Estimate (\\(\\alpha\\) above) gives us the estimated average bill depth (mm) given the estimated relationship bill length (mm) and bill length. The bill_length_mm : Estimate (\\(\\beta_1\\) above) is the slope associated with bill length (mm). So, here for every 1mm increase in bill length we estimated a 0.082mm decrease (or a -0.082mm increase) in bill depth. ## calculate predicted values penguins_nafree$pred_vals &lt;- predict(slm) ## plot ggplot(data = penguins_nafree, aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + ylab(&quot;Bill depth (mm)&quot;) + xlab(&quot;Bill length (mm)&quot;) + theme_classic() + geom_line(aes(y = pred_vals)) One factor and a continous variable Adding species; remember species is a factor variable! p2 &lt;- ggplot(data = penguins_nafree, aes(y = bill_depth_mm, x = bill_length_mm, color = species)) + geom_point() + ylab(&quot;Bill depth (mm)&quot;) + xlab(&quot;Bill length (mm)&quot;) + theme_classic() p2 slm_sp &lt;- lm(bill_depth_mm ~ bill_length_mm + species, data = penguins_nafree) Model formula Now we have two explanatory variables, so our model formula becomes \\[Y_i = \\beta_0 + \\beta_1z_i + \\beta_2x_i + \\epsilon_i\\] \\[\\epsilon_i \\sim \\text{Normal}(0,\\sigma^2)\\] where for observation \\(i\\) \\(Y_i\\) is the value of the response (bill_depth_mm) \\(z_i\\) is one explanatory variable (bill_length_mm say) \\(x_i\\) is another explanatory variable (species say) \\(\\epsilon_i\\) is the error term: the difference between \\(Y_i\\) and its expected value \\(\\alpha\\), \\(\\beta_1\\), and \\(\\beta_2\\) are all parameters to be estimated. Remember though that when we have factor explanatory variables (e.g., species) we have to use dummy variables, see lecture. Here the Adelie group are the baseline (R does this alphabetically, to change this see previous chapter). So model formula is \\[ \\begin{aligned} \\operatorname{bill\\_depth\\_mm} &amp;= \\alpha + \\beta_{1}(\\operatorname{bill\\_length\\_mm}) + \\beta_{2}(\\operatorname{species}_{\\operatorname{Chinstrap}}) + \\beta_{3}(\\operatorname{species}_{\\operatorname{Gentoo}})\\ + \\\\ &amp;\\quad \\epsilon \\end{aligned} \\] Fitted model summary(slm_sp)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.5652616 0.69092642 15.291442 2.977289e-40 ## bill_length_mm 0.2004431 0.01767974 11.337449 2.258955e-25 ## speciesChinstrap -1.9330779 0.22571878 -8.564099 4.259893e-16 ## speciesGentoo -5.1033153 0.19439523 -26.252267 1.043789e-82 Simpson‚Äôs paradox‚Ä¶ look how the slope associated with bill length (coefficient of bill_length_mm) has switched direction from the model above! Why do you think this is? Here, the (Intercept): Estimate gives us the estimated average bill depth (mm) of the Adelie penguins given the estimated relationship between bill length and bill depth. Technically, this is the estimated bill depth (mm) for Adelie penguins with zero bill length. That is clearly a nonsense way to interpret this as that would be an impossible situation in practice! I would recommend as thinking of this as the y-shift (i.e., height) of the fitted line. The bill_length_mm : Estimate (\\(\\beta_1\\) above) is the slope associated with bill length (mm). So, here for every 1mm increase in bill length we estimated a 0.2mm increase in bill depth. What about the coefficient of the other species levels? Look at the plot below, these values give the shift (up or down) of the parallel lines from the Adelie level. So given the estimated relationship between bill depth and bill length these coefficients are the estimated change from the baseline. ## calculate predicted values penguins_nafree$pred_vals &lt;- predict(slm_sp) ## plot ggplot(data = penguins_nafree, aes(y = bill_depth_mm, x = bill_length_mm, color = species)) + geom_point() + ylab(&quot;Bill depth (mm)&quot;) + xlab(&quot;Bill length (mm)&quot;) + theme_classic() + geom_line(aes(y = pred_vals)) Interactions Recall the (additive) model formula from above \\[Y_i = \\beta_0 + \\beta_1z_i + \\beta_2x_i + \\epsilon_i\\] but what about interactions between variables? For example, \\[Y_i = \\beta_0 + \\beta_1z_i + \\beta_2x_i + \\beta_3z_ix_i + \\epsilon_i\\] Note: to include interaction effects in our model by using either the * or : syntax in our model formula. For example, : denotes the interaction of the variables to its left and right, and * means to include all main effects and interactions, so a*b is the same as a + b + a:b. To specify a model with additive and interaction effects we use slm_int &lt;- lm(bill_depth_mm ~ bill_length_mm*species, data = penguins_nafree) Model formula The model formula is then \\[ \\begin{aligned} \\operatorname{bill\\_depth\\_mm} &amp;= \\alpha + \\beta_{1}(\\operatorname{bill\\_length\\_mm}) + \\beta_{2}(\\operatorname{species}_{\\operatorname{Chinstrap}}) + \\beta_{3}(\\operatorname{species}_{\\operatorname{Gentoo}})\\ + \\\\ &amp;\\quad \\beta_{4}(\\operatorname{bill\\_length\\_mm} \\times \\operatorname{species}_{\\operatorname{Chinstrap}}) + \\beta_{5}(\\operatorname{bill\\_length\\_mm} \\times \\operatorname{species}_{\\operatorname{Gentoo}}) + \\epsilon \\end{aligned} \\] Fitted model summary(slm_int)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.48770713 1.15987305 9.9042797 2.135979e-20 ## bill_length_mm 0.17668344 0.02980564 5.9278518 7.793199e-09 ## speciesChinstrap -3.91856701 2.06730876 -1.8954919 5.890889e-02 ## speciesGentoo -6.36675118 1.77989710 -3.5770333 4.000274e-04 ## bill_length_mm:speciesChinstrap 0.04552828 0.04594283 0.9909769 3.224296e-01 ## bill_length_mm:speciesGentoo 0.03092816 0.04111608 0.7522157 4.524625e-01 This can be written out: \\[ \\begin{aligned} \\operatorname{\\widehat{bill\\_depth\\_mm}} &amp;= 11.49 + 0.18(\\operatorname{bill\\_length\\_mm}) - 3.92(\\operatorname{species}_{\\operatorname{Chinstrap}}) - 6.37(\\operatorname{species}_{\\operatorname{Gentoo}})\\ + \\\\ &amp;\\quad 0.05(\\operatorname{bill\\_length\\_mm} \\times \\operatorname{species}_{\\operatorname{Chinstrap}}) + 0.03(\\operatorname{bill\\_length\\_mm} \\times \\operatorname{species}_{\\operatorname{Gentoo}}) \\end{aligned} \\] The interaction terms (i.e., bill_length_mm:speciesChinstrap and bill_length_mm:speciesGentoo) specify the species specific slopes given the other variables in the model. We can use our dummy variable trick again to interpret the coefficients correctly. In this instance we have one factor explanatory variable, Species, and one continuous explanatory variable, bill length (mm). As before, Adelie is our baseline reference. Let‚Äôs assume we‚Äôre talking about the Adelie penguins, then our equation becomes (using the dummy variable technique) \\[\\widehat{\\text{bill_depth_mm}} = 11.488 + (0.177 \\times \\text{bill_length_mm}).\\] So, the (Intercept): Estimate term (\\(\\hat{\\alpha}\\)), again, specifies the height of the Adelie fitted line and the main effect of bill_length_mm: Estimate (\\(\\hat{\\beta_1}\\)) estimates the relationship (slope) between bill length (mm) and bill depth (mm) for the Adelie penguin. So, here for every 1mm increase in bill length (mm) for the Adelie penguins we estimate, on average, a 0.177mm increase in bill depth (mm). Now, what about Gentoo penguins? Our equation then becomes \\[\\widehat{\\text{bill_depth_mm}} = 11.488 + (0.177 \\times \\text{bill_length_mm}) + (-6.367) + (0.031 \\times \\text{bill_length_mm}).\\] which simplifies to \\[\\widehat{\\text{bill_depth_mm}} = 5.121 + (0.208 \\times \\text{bill_length_mm}).\\] The estimated Gentoo-specific intercept term (y-axis line height) is therefore \\(\\hat{\\alpha} + \\hat{\\beta_3} = 11.488 + (-6.367) = 5.121.\\) The Gentoo-specific bill length (mm) slope is then \\(\\hat{\\beta_1} + \\hat{\\beta_5} = 0.177 + 0.031 = 0.208.\\) So, for every 1mm increase in bill length (mm) for the Gentoo penguins we estimate, on average, a 0.208mm increase in bill depth (mm), a slightly steeper slope than for the estimated Adelie relationship (i.e., we estimate that as Gentoo bills get longer their depth increases at a, slightly, greater rate than those of Adelie penguins). In summary, the main effect of species (i.e., speciesChinstrap: Estimate and speciesGentoo:Estimate ) again give the shift (up or down) of the lines from the Adelie level. However, these lines are no longer parallel (i.e., each species of penguin has a different estimated relationship between bill length and bill depth)! But, now we‚Äôve specified this all singing and dancing interaction model we might ask are the non-parallel lines non-parallel enough to reject the parallel line model? Look at the plot below; what do you think? Other possible models Let‚Äôs assume that we have the same data types as above, specifically, a continuous response (\\(y\\)) and one factor (\\(f\\), with two levels \\(A\\) and \\(B\\)) and one continuous (\\(x\\)) explanatory variable. Assuming that \\(A\\) is the baseline for \\(f\\) the possible models are depicted below. Note that models III and V are forced to have the same intercept for both levels of \\(f\\). In addition, when you have no main effect of \\(x\\), models IV and V, then the model is forced to have no effect of \\(x\\) for the baseline level of \\(f\\) (in this case \\(A\\)). "],["tldr-lm.html", "TL;DR lm()", " TL;DR lm() Artwork by @allison_horst Meet your MLR teaching assistants Interpret coefficients for categorical predictor variables Interpret coefficients for continuous predictor variables Make predictions using the regression model Residuals Check residuals for normality "],["model-comparison-and-selection.html", "Model comparison and selection", " Model comparison and selection Remember that it is always is imperative that we check the underlying assumptions of our model! If our assumptions are not met then basically the maths falls over and we can‚Äôt reliably draw inference from the model (e.g., can‚Äôt trust the parameter estimates etc.). Two of the most important assumption are: equal variances (homogeneity of variance), and normality of residuals. Let‚Äôs look at the fit of the slm model (single continuous explanatory variable) gglm::gglm(slm) # Plot the four main diagnostic plots Do you think the residuals are Normally distributed (look at the QQ plot)? Think of what this model is, do you think it‚Äôs the best we can do? Model comparison and selection Are the non-parallel lines non-parallel enough to reject the parallel line model? Now we can compare nested linear models by hypothesis testing. Luckily the R function anova() automates this. Yes the same idea as we‚Äôve previously learnt about ANOVA! We essentially perform an F-ratio test between the nested models! By nested we mean that one model is a subset of the other (i.e., where some coefficients have been fixed at zero). For example, \\[Y_i = \\beta_0 + \\beta_1z_i + \\epsilon_i\\] is a nested version of \\[Y_i = \\beta_0 + \\beta_1z_i + \\beta_2x_i + \\epsilon_i\\] where \\(\\beta_2\\) has been fixed to zero. As an example consider testing the single explanatory variable model slm against the same model with species included as a variable slm_sp. To carry out the appropriate hypothesis test in R we can run anova(slm,slm_sp) ## Analysis of Variance Table ## ## Model 1: bill_depth_mm ~ bill_length_mm ## Model 2: bill_depth_mm ~ bill_length_mm + species ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 331 1220.16 ## 2 329 299.62 2 920.55 505.41 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 As you‚Äôll see the anova() function takes the two model objects (slm and slm_sp) each as arguments. It returns an ANOVA testing whether the more complex model (slm_sp) is just as good at capturing the variation in the data as the simpler model (slm). The returned p-value should be interpreted as in any other hypothesis test. i.e., the probability of observing a statistic as least as extreme under our null hypothesis (here that each model is as good at capturing the variation in the data). What would we conclude here? I‚Äôd say we have pretty strong evidence against the models being equally good! I‚Äôd definitely plump for slm_sp over slm, looking back at the plots above does this make sense? Now what about slm_int vs slm_sp? anova(slm_sp,slm_int) ## Analysis of Variance Table ## ## Model 1: bill_depth_mm ~ bill_length_mm + species ## Model 2: bill_depth_mm ~ bill_length_mm * species ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 329 299.62 ## 2 327 298.62 2 0.99284 0.5436 0.5812 So it seems both models are just as good at capturing the variation in our data: we‚Äôre happy with the parallel lines! Another way we might compare models is by using the Akaike information criterion (AIC) (you‚Äôll see more of this later in the course). AIC is an estimator of out-of-sample prediction error and can be used as a metric to choose between competing models. Between nested models we‚Äôre looking for the smallest AIC (i.e., smallest out-of-sample prediction error). Typically, a difference of 4 or more is considered to indicate an improvement; this should not be taken as writ however, using multiple comparison techniques is advised. R already has an AIC() function that can be used directly on your lm() model object(s). For example, AIC(slm,slm_sp,slm_int) ## df AIC ## slm 3 1383.4462 ## slm_sp 5 919.8347 ## slm_int 7 922.7294 This backs up what our ANOVA suggested model slm_sp as that preferred! As always it‚Äôs important to do a sanity check! Does this make sense? Have a look at the outputs from these models and see what you think. Just because we‚Äôve chosen a model (the best of a bad bunch perhaps) this doesn‚Äôt let us off the hook. We should check our assumptions gglm::gglm(slm_sp) # Plot the four main diagnostic plots Residuals vs Fitted plot: equal spread? Doesn‚Äôt look too trumpety! Normal quantile-quantile (QQ) plot: skewed? Maybe slightly right skewed (deviation upwards from the right tail) Scale-Location plot: equal spared? I‚Äôd say so. Residuals vs Leverage: ? Maybe a couple of points with high leverage. "],["point-predictions-and-confidence-intervals.html", "Point predictions and confidence intervals", " Point predictions and confidence intervals After all that what do estimated parameters mean? Confidence intervals for parameters For the chosen slm_sp model we can get these simply by using cis &lt;- confint(slm_sp) cis ## 2.5 % 97.5 % ## (Intercept) 9.2060707 11.9244526 ## bill_length_mm 0.1656635 0.2352227 ## speciesChinstrap -2.3771120 -1.4890438 ## speciesGentoo -5.4857298 -4.7209009 By default the 95% intervals are returned (see previous lecture) So this tells us that For every 1mm increase in bill length we estimate the expected bill depth to increases between 0.166 and 0.235 mm We estimate that the expected bill depth of a Chinstrap penguin is between 1.5 and 2.4 mm shallower than the Adelie penguin Point prediction Using the slm_sp model we can make a point prediction for the expected bill depth (mm) for Gentoo penguins with a bill length of 50mm. Recall the model equation \\[ \\begin{aligned} \\operatorname{bill\\_depth\\_mm} &amp;= \\alpha\\ + \\\\ &amp;\\quad \\beta_{1}(\\operatorname{bill\\_length\\_mm})\\ + \\\\ &amp;\\quad \\beta_{2}(\\operatorname{species}_{\\operatorname{Chinstrap}})\\ + \\\\ &amp;\\quad \\beta_{3}(\\operatorname{species}_{\\operatorname{Gentoo}})\\ + \\\\ &amp;\\quad \\epsilon \\end{aligned} \\] We can then simply substitute in the values: \\[\\widehat{\\text{bill depth}} = \\hat{\\alpha} + \\hat{\\beta_1}*50 + \\hat{\\beta_3}*1\\] \\[\\downarrow\\] \\[\\widehat{\\text{bill depth}} = 10.56 + 0.20*50 - 5.10*1\\] \\[\\downarrow\\] \\[15.47\\text{mm}\\] Rather than by hand we can do this easily in R ## create new data frame with data we want to predict to ## the names have to match those in our original data frame newdata &lt;- data.frame(species = &quot;Gentoo&quot;,bill_length_mm = 50) ## use predict() function predict(slm_sp, newdata = newdata) ## more accurate than our by hand version! ## 1 ## 15.4841 What does this look like on a plot "],["p-values-an-intro.html", "p-values, an intro", " p-values, an intro P-values from permutation tests In experimental situations a large p-value (large tail proportion) means that the luck of the randomisation quite often produces group differences as large or even larger than what we‚Äôve got in our data. A small p-value means that the luck of the randomisation draw hardly ever produces group differences as large as we‚Äôve got in our data. Statistical significance does not imply practical significance. Statistical significance says nothing about the size of treatment differences. To estimate the sizes of differences you need confidence intervals. üò± p-values üò± The ASA Statement on p-Values: Context, Process, and Purpose ‚ÄúGood statistical practice, as an essential component of good scientific practice, emphasizes principles of good study design and conduct, a variety of numerical and graphical summaries of data, understanding of the phenomenon under study, interpretation of results in context, complete reporting and proper logical and quantitative understanding of what data summaries mean. No single index should substitute for scientific reasoning.‚Äù ‚Äî ASA Statement on p-Values What is a p-Value? Informally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value p-values can indicate how incompatible the data are with a specified statistical model p-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold Proper inference requires full reporting and transparency A p-value, or statistical significance, does not measure the size of an effect or the importance of a result By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis Type I and Type II errors Type I error (false positive): declare a difference (i.e., reject \\(H_0\\)) when there is no difference (i.e.¬†\\(H_0\\) is true). Risk of the Type I error is determined by the level of significance (which we set!) (i.e., \\(\\alpha =\\text{ P(Type I error)} = \\text{P(false positive)}\\). Type II error (false negative): difference not declared (i.e., \\(H_0\\) not rejected) when there is a difference (i.e., \\(H_0\\) is false). Let \\(\\beta =\\) P(do not reject \\(H_0\\) when \\(H_0\\) is false); so, \\(1-\\beta\\) = P(reject \\(H_0\\) when \\(H_0\\) is false) = P(a true positive), which is the statistical power of the test. Each time we carry out a hypothesis test the probability we get a false positive result (type I error) is given by \\(\\alpha\\) (the level of significance we choose). When we have multiple comparisons to make we should then control the Type I error rate across the entire family of tests under consideration, i.e., control the Family-Wise Error Rate (FWER); this ensures that the risk of making at least one Type I error among the family of comparisons in the experiment is \\(\\alpha\\). State of Nature Don‚Äôt reject \\(H_0\\) reject \\(H_0\\) \\(H_0\\) is true ‚úÖ Type I error \\(H_0\\) is false Type II error ‚úÖ "],["one-way-analysis-of-variance-anova.html", "One-Way Analysis of Variance (ANOVA)", " One-Way Analysis of Variance (ANOVA) Between group SS (SSB) The idea: Assess distances between treatment (surgical condition) means relative to our uncertainty about the actual (true) treatment means. add up the differences: -1.192 + -0.703 + 1.895 = 0. This is always the case! So adding up the differences: -1.192 + -0.703 + 1.895 = 0. Not a great way to measure distances! Sums of Squares? -1.192^2 + -0.703^2 + 1.895^2 add up the squared differences? but‚Ä¶ there are 4 observations in each group (treatment) 4\\(\\times\\)(-1.192)^2 + 4\\(\\times\\)(-0.703)^2 + 4\\(\\times\\)(1.895)^2 This is the Between Groups Sums of Squares or the Between group SS (SSB) So the Between group SS (SSB) = 22.02635 Adding up the differences: -1.192 + -0.703 + 1.895 = 0. This is always the case and that itself gives us information‚Ä¶ We only need to know two of the values to work out the third! So we have only 2 bits of unique information; SSB degrees of freedom = 2 Within group SS (SSW) The Within group SS (SSW) arises from the same idea: To assess distances between treatment (surgical condition) means relative to our uncertainty about the actual (true) treatment means. Procedure: Observation - Treatment mean Square the difference Add them up! Within group SS (SSW) unexplained variance F-statistic Recall the Between group SS (SSB) = 22.02635 So mean SSB = 22.02635 / 2 The within group SS (SSW) = 6.059075 Here we have 3\\(\\times\\) 3 bits of unique information: within groups degrees of freedom is 9. So mean SSW = 6.059/9 Consider the ratio \\({\\frac{{\\text{variation due to treatments}}}{{\\text{unexplained variance}}}} = {\\frac{{\\text{ mean between-group variability}}}{{\\text{mean within-group variability}}}}\\) \\(=\\frac{\\text{mean SSB}}{\\text{mean SSW}}\\) \\(=\\frac{\\text{MSB}}{\\text{MSW}}\\) = \\(=\\frac{\\text{experimental variance}}{\\text{error variance}}\\) 16.3586975 This is the F-statistic! Degrees of freedom (DF) Essentially statistical currency (i.e., unique bits of information). So in the example above we have 3 treatment groups and if we know the mean of two we know the third (i.e., 2 unique bits of info) so SSB df = 2. Now, for SSW df. We have 12 observations (4 in each group); we know the treatment means so if we have three of those observed values in each group we know the fourth: 12 - 3 = 9 (i.e., number of observations - number of df lost due to knowing the cell means). In, summary Traditional name Model formula R code Simple regression \\(Y \\sim X_{continuous}\\) lm(Y ~ X) One-way ANOVA \\(Y \\sim X_{categorical}\\) lm(Y ~ X) Two-way ANOVA \\(Y \\sim X1_{categorical} + X2_{categorical}\\) lm(Y ~ X1 + X2) ANCOVA \\(Y \\sim X1_{continuous} + X2_{categorical}\\) lm(Y ~ X1 + X2) Multiple regression \\(Y \\sim X1_{continuous} + X2_{continuous}\\) lm(Y ~ X1 + X2) Factorial ANOVA \\(Y \\sim X1_{categorical} * X2_{categorical}\\) lm(Y ~ X1 * X2) or lm(Y ~ X1 + X2 + X1:X2) "],["module-3.html", "Module 3", " Module 3 Learning objectives Identify the following experimental unit observational units List and describe the three main principals of experimental design Randomization Replication Blocking Calculate Sums of Squares (between and within groups) given the observations Define and state the appropriate degrees of freedom in a one-way ANOVA scenario Calculate the F-statistics given the appropriate Sums of Squares and degrees of freedom Interpret and discuss a given p-value in the context of a stated hypothesis test Explain between group and within group variation Communicate statistical concepts and experimental outcomes clearly using language appropriate for both a scientific and non-scientific audience Other resources Glass, David J. Experimental Design for Biologists. Second ed.¬†2014. Print. Welham, S. J. Statistical Methods in Biology : Design and Analysis of Experiments and Regression. 2015. Print. Fisher, Ronald Aylmer. The Design of Experiments. 8th ed.¬†Edinburgh: Oliver &amp; Boyd, 1966. Print. O &amp; B Paperbacks. experimantal design lme4 &amp; lmer etc. "],["introduction-to-the-design-and-analysis-of-experiments.html", "Introduction to the design and analysis of experiments", " Introduction to the design and analysis of experiments ‚ÄúA useful property of a test of significance is that it exerts a sobering influence on the type of experimenter who jumps to conclusions on scanty data, and who might otherwise try to make everyone excited about some sensational treatment effect that can well be ascribed to the ordinary variation in [their] experiment.‚Äù ‚Äî Gertrude Mary Cox Key phrases *An experiment is a procedure (or set of actions) where a researcher intentionally changes some factor/treatment/variable to observe the effect of their actions. As mentioned above, the collection of observational data is not experimentation. An experimental unit is the smallest portion of experimental material which is independently perturbed. This is the item under study for which some variable (treatment) is changed. For example this could be a human subject or an agricultural plot. An observational unit (or subsample) is the smallest unit on which a response is measured. If the experimental unit is split after the treatment has been applied (e.g., multiple samples taken from one human subject) then this sample is called a subsample or observational unit. If one measurement is made on each experimental unit then the observational unit = the experimental unit. If multiple measurements are made on each subject (e.g., human) then each experimental unit has &gt;1 observational unit. This is then pseudo- or technical replication (see below). A treatment (or independent variable or factor or treatment factor) is an experimental condition independently applied to an experimental unit. It is one of the variables that is controlled by the researcher during the experiment (e.g., drug type). The values of the treatments within a set are called levels. The dependent variable or response is the output (or thing) that is measured after an experiment. This is what the researcher measures and assesses if changing the treatment(s) (i.e., independent variable(s)) induces any change. An effect is the change in the response variable caused by the controlled changes in the independent variable. Whether the magnitude of the effect (it‚Äôs size) is significant (or or any practical interest) is determined by the researcher after carrying out some appropriate analyses. The three key principles: R. A. Fisher‚Äôs work in the area of experimental design is, perhaps, the most well known in the field. The principles he devised we still abide by today, see below. Note, however, to give a balanced view of the celebrated mathematician many of his views (on eugenics and race in particular) are abhorrent to many. I would urge you to read up on his work in this area and come to your own conclusions. Replication Biological replication: each treatment is independently applied to each of several humans, animals or plants. Why? To generalize results to population. Technical replication: two or more samples from the same biological source which are independently processed. Why? Advantageous if processing steps introduce a lot of variation; increases the precision with which comparisons of relative abundances between treatments are made. Pseudo-replication: one sample from the same biological source, divided into two or more aliquots which are independently measured. Why? Advantageous for noisy measuring instruments; increases the precision with which comparisons of relative abundances between treatments are made. Randomization The main reason to randomise allocation of treatment to experimental units is to protect against bias. We, typically, wish to plan the experiment in such a way that the variations caused by extraneous factors can all be combined under the general heading of ‚Äúchance‚Äù. Doing so ensures that each treatment has the same probability of getting good (or bad) units and thus avoids systematic bias. Random allocation can cancel out population bias; it ensures that any other possible causes for the experimental results are split equally between groups. Typically statistical analysis assumes that observations are independent. This is almost never strictly true in practice but randomisation means that our estimates will behave as if they were based on independent observations. Blocking Blocking helps control variability by making treatment groups more alike. Experimental units are divided into subsets (called blocks) so that units within the same block are more similar than units from different subsets or blocks. Blocking is a technique for dealing with nuisance factors. A nuisance factor is a factor that has some effect on the response, but is of no interest. Setting up an experiment When setting up and experiment and/or considering experimental data there are questions we should always answer the following questions. What are the objectives of the study? Why are you carrying out this experiment? What is the scientific question? What specific hypothesis do you hope to test/investigate? What are the experimental units? What characteristics of the experimental units can be measured? How large is an meaningful effect size in this context? How many treatments are to be studied and what are they? How does the experimenter apply the treatments to the available experimental units and then observe the responses? What are the potential sources of variation? Can you control for the unwanted sources or variation? Can you name all likely sources? Can the resulting design be analyzed or can the desired comparisons be made? **Can you describe and detail the appropriate analysis?* A simple example: Charlotte‚Äôs coffee Last Christmas I was given a gift set of three types of coffee beans. I want to know which makes the darkest coffee; to do this I measure the opacity after the coffee is made. Three types of coffee beans are: Arabica , Liberica , and Robusta . 12 identical cups are chosen and sets of four cups are randomly allocated one of three treatments. Four sets of each type of coffee beans are ground and cups made (in the same way) resulting in a total of 12 cups of coffee, see below. Samples are taken from each cup and the coffee colour is measured. Experiment design: Scientific question: Does coffee colour differ between bean types of coffee beans? There are 3 treatments (types of coffee beans): Arabica, Liberica, and Robusta. In this case the experimental unit would be the coffee cup as each one is allocated a different bean type (treatment). The observational unit changes depending on the scenario (i.e., what and how samples are taken). For example, if a single ml of liquid is taken from each cup and one measurement is taken per ml taken \\(\\rightarrow\\) the observational unit would be the cup; if single ml of liquid is taken from each cup and two subsamples are then taken from each ml, then if measurements are taken per subsample \\(\\rightarrow\\) then the observational unit would be a subsample; if four \\(\\times\\) 1 ml of liquid were taken from each cup and from each ml a measurement is taken \\(\\rightarrow\\) then the observational unity would be each 1ml sample. "],["some-basic-experimental-designs.html", "Some basic experimental designs", " Some basic experimental designs Completely randomised design (CRD) Let‚Äôs consider a completely randomised design with one treatment factor (e.g., coffee bean type). Here, \\(n\\) experimental units (e.g., cups) are divided randomly into \\(t\\) groups. Random allocation can be achieved by simply drawing lots from a hat! To be more rigorous, though, we could use R‚Äôs sample() function (have a go yourself and see if you can work out how to wield sample()). Each group is then given one treatment level (one of the treatment factors). As we have defined only one treatment factor all other known independent variables are kept constant so as to not bias any effects (e.g., cup type and type in Charlotte‚Äôs‚òï experiment). An illustration of a CRD with one tratment factor and three treatment levels (A, B, &amp; C) Designing a CRD using R Let‚Äôs assume you want to set up an experiment similar to the coffee one above using R to do the random allocation of treatments for you. ## create a character vector of bean types beans &lt;- rep(c(&quot;Arabica&quot;,&quot;Liberica&quot;, &quot;Robusta&quot;), each = 4) beans ## [1] &quot;Arabica&quot; &quot;Arabica&quot; &quot;Arabica&quot; &quot;Arabica&quot; &quot;Liberica&quot; &quot;Liberica&quot; ## [7] &quot;Liberica&quot; &quot;Liberica&quot; &quot;Robusta&quot; &quot;Robusta&quot; &quot;Robusta&quot; &quot;Robusta&quot; ## randomly sample the character vector to give the order of coffees set.seed(1234) ## this is ONLY for consistency, remove if doing this yourself allocation &lt;- sample(beans, 12) allocation ## [1] &quot;Robusta&quot; &quot;Robusta&quot; &quot;Liberica&quot; &quot;Liberica&quot; &quot;Arabica&quot; &quot;Liberica&quot; ## [7] &quot;Arabica&quot; &quot;Robusta&quot; &quot;Arabica&quot; &quot;Liberica&quot; &quot;Robusta&quot; &quot;Arabica&quot; Having run the code above your CRD plan is as follows Cup Bean 1 Robusta 2 Robusta 3 Liberica 4 Liberica 5 Arabica 6 Liberica 7 Arabica 8 Robusta 9 Arabica 10 Liberica 11 Robusta 12 Arabica Randomised complete block design (RCBD) Let‚Äôs consider a randomised complete block design with one treatment factor (e.g., coffee bean type). If the treatment factor has \\(t\\) levels there will be \\(b\\) blocks that each contain \\(t\\) experimental units resulting in a total of \\(t\\times b\\) experimental units. For example, let‚Äôs imagine that for the coffee experiment we had two cup types: mugs and heatproof glasses. We might consider the type of receptacle to have an effect on the coffee colour measured, however, we are not interested in this. Therefore, to negate this we block by cup type. This means that any effect due to the blocking factor (cup type) is accounted for by the blocking. For a blocked design we want the \\(t\\) experimental units within each block should be as homogeneous as possible (as similar as possible, so that there is unlikely to be unwanted variation coming into the experiment this way). The variation between blocks (the groups of experimental units) should be large enough (i.e., blocking factors different enough) so that conclusions can be drawn. Allocation of treatments to experimental units is done randomly (i.e., treatments are randomly assigned to units) within each block. An illustration of a CRD with one tratment factor, three treatment levels (A, B, &amp; C), and three blocks (rows) Designing a RCBD using R Let‚Äôs assume you want to set up an experiment similar to the coffee one above; however, now you are in the situation where you have two types of cups (six mugs and six heatproof glasses). Below we use R to do the random allocation of treatments within each block (cup type) for you. Here we have \\(t = 3\\) treatments (bean types) and \\(b = 2\\) blocks (cup types) so we will have \\(t \\times b = 6\\) experimental units in total. set.seed(432) ## this is ONLY for consistency, remove if doing this yourself plan &lt;- data.frame(Beans = rep(c(&quot;Arabica&quot;,&quot;Liberica&quot;, &quot;Robusta&quot;), times = 2), Block = rep(c(&quot;Mug&quot;, &quot;Glass&quot;), each = 3)) %&gt;% ## combine experiment variables group_by(Block) %&gt;% ## group by blocking factor dplyr::sample_n(3) plan ## # A tibble: 6 √ó 2 ## # Groups: Block [2] ## Beans Block ## &lt;chr&gt; &lt;chr&gt; ## 1 Robusta Glass ## 2 Arabica Glass ## 3 Liberica Glass ## 4 Robusta Mug ## 5 Arabica Mug ## 6 Liberica Mug Having run the code above your RCBD plan is as follows Cup Beans Block 1 Robusta Glass 2 Arabica Glass 3 Liberica Glass 1 Robusta Mug 2 Arabica Mug 3 Liberica Mug Factorial design A factorial experiment is one where there are two or more sets of (factor) treatments. Rather than studying each factor separately all combinations of the treatment factors are considered. Factorial designs enable us to infer any interaction effects, which may be present. An interaction effect is one where the effect of one variable depends on the value of another variable (i.e., the effect of one treatment factor on the response variable will change depending on the value of a second treatment factor.) For example, we could add another treatment to the coffee experiment above: grinder type (manual or electric ). So, now we have two treatments, bean type (with three levels) and cup type (with two levels). Recall that in a factorial experiment we want to study all combinations of the levels of each factor: Manual Electric Arabica A.M A.E Liberica L.M L. E Robusta R.M R. E Note is a factorial design has equal numbers of replicates in each group then it is said to be a balanced design; if this is not the case then it is unbalanced. Now, our question about the colour of coffee would be Does the colour of the difference in the colour of the coffee between the beans wall depend on the grinder used? Possible outcomes Interactions between two factor treatments If an interaction effect exists the effect of one factor on the response will change depending on the level of the other factor: The plot above is called an interaction plot, think back to the previous module. Creating such a plot is often very useful when drawing inference; in this instance we can see that the colour of the coffee changes depending on the type of coffee bean used, however, this relationship differs depending on the type of grinder used. For example, Liberica beans produce darker coffee than the other two beans when the electric grinder is used, but weaker coffee when the manual grinder is used. "],["modler-v.-designer-the-maths.html", "Modler v. designer: the maths", " Modler v. designer: the maths A completely randomised design (CRD) Last Christmas I was given a gift set of three types of coffee beans. I want to know which makes the darkest coffee; to do this I measure the opacity after the coffee is made. To work this out I set up a completely randomized experiment where each of 12 cups of are made and the type of coffee used randomly assigned to each cup. Below is the experiment plan I used and and the meaured outcome. ## cup treatment opacity ## 1 1 Type 1 5.3 ## 9 2 Type 3 8.8 ## 3 3 Type 1 6.6 ## 7 4 Type 2 6.3 ## 6 5 Type 2 7.1 ## 5 6 Type 2 7.5 ## 2 7 Type 1 6.0 ## 8 8 Type 2 7.6 ## 4 9 Type 1 4.9 ## 12 10 Type 3 10.3 ## 10 11 Type 3 6.9 ## 11 12 Type 3 9.2 As a linear model As we‚Äôve seen in the previous module that we can write a linear model with a single explanatory variable as \\[Y_i = \\alpha + \\beta_1x_i + \\epsilon_i\\] When dealing with factor variables we use dummy variables and can write the above as \\[Y_{ik} = \\alpha + \\tau_k + \\epsilon_{ik}\\] where \\(\\tau_k\\) is called an effect and represents the difference between the overall average, \\(\\alpha\\), and the average at the \\(k_{th}\\) treatment level. The errors \\(\\epsilon_{ik}\\) are again assumed to be normally distributed and independent due to the randomisation (i.e., \\(\\epsilon_{ik} \\sim N(0, \\sigma^2)\\). Or you might think of the model as \\[Y_{ik} = \\mu_k + \\epsilon_{ik}\\] where \\(Y_{ik}\\) is the response (i.e., observed coffee opacity) for the \\(i^{th}\\) experimental unit (i.e., coffee cup) subjected to the \\(k^{th}\\) level of the treatment factor (i.e., coffee type). Here \\(\\mu_k\\) are the different (cell) means for each level of the treatment factor. See below for an illustration of this for three factor treatment levels (as in the coffee example above). "],["module-4.html", "Module 4", " Module 4 maximum likelihood vs bayes Extra materials An Introduction to Bayesian Thinking "],["least-squares-estimation.html", "Least Squares Estimation", " Least Squares Estimation Some basic matrix algebra This section is a recap only, if you need a more in-depth overiew of matrix algebra then use the extra materials provided at the start of this module. Matrices are commonly written in box brackets or parentheses and are typically denoted by upper case bold letters (e.g., A) with elements represented by the corresponding lower case indexed letters: A= ={\\begin{bmatrix}a_{11}&amp;a_{12}&amp;&amp;a_{1n}\\a_{21}&amp;a_{22}&amp;&amp;a_{2n}\\&amp;&amp;&amp;\\a_{m1}&amp;a_{m2}&amp;&amp;a_{mn}\\end{bmatrix}}={\\begin{pmatrix}a_{11}&amp;a_{12}&amp;&amp;a_{1n}\\a_{21}&amp;a_{22}&amp;&amp;a_{2n}\\&amp;&amp;&amp;\\a_{m1}&amp;a_{m2}&amp;&amp;a_{mn}\\end{pmatrix}}=(a_{ij}) ^{mn}.} The entry in the i-th row and j-th column of a matrix A is sometimes referred to as the i,j, (i,j), or (i,j)th entry of the matrix, and most commonly denoted as ai,j, or aij. Alternative notations for that entry are A[i,j] or Ai,j. For example, the (1,3) entry of the following matrix A is 5 (also denoted a13, a1,3, A[1,3] or A1,3): A= ={ \\[\\begin{bmatrix}4&amp;-7&amp;\\color {red}{5}&amp;0\\\\-2&amp;0&amp;11&amp;8\\\\19&amp;1&amp;-3&amp;12\\end{bmatrix}\\] }} Matrix addition The sum A+B of two m-by-n matrices A and B is calculated entrywise: (A + B)i,j = Ai,j + Bi,j, where 1 ‚â§ i ‚â§ m and 1 ‚â§ j ‚â§ n. {+{ \\[\\begin{bmatrix}0&amp;0&amp;5\\\\7&amp;5&amp;0\\end{bmatrix}\\] }={ \\[\\begin{bmatrix}1+0&amp;3+0&amp;1+5\\\\1+7&amp;0+5&amp;0+0\\end{bmatrix}\\] }={ \\[\\begin{bmatrix}1&amp;3&amp;6\\\\8&amp;5&amp;0\\end{bmatrix}\\] }}{+{ \\[\\begin{bmatrix}0&amp;0&amp;5\\\\7&amp;5&amp;0\\end{bmatrix}\\] }={ \\[\\begin{bmatrix}1+0&amp;3+0&amp;1+5\\\\1+7&amp;0+5&amp;0+0\\end{bmatrix}\\] }={ \\[\\begin{bmatrix}1&amp;3&amp;6\\\\8&amp;5&amp;0\\end{bmatrix}\\] }} Scalar multiplication The product cA of a number c (also called a scalar in the parlance of abstract algebra) and a matrix A is computed by multiplying every entry of A by c: (cA)i,j = c ¬∑ Ai,j. This operation is called scalar multiplication, but its result is not named ‚Äúscalar product‚Äù to avoid confusion, since ‚Äúscalar product‚Äù is sometimes used as a synonym for ‚Äúinner product‚Äù. {2={ \\[\\begin{bmatrix}2\\cdot 1&amp;2\\cdot 8&amp;2\\cdot -3\\\\2\\cdot 4&amp;2\\cdot -2&amp;2\\cdot 5\\end{bmatrix}\\] }={ \\[\\begin{bmatrix}2&amp;16&amp;-6\\\\8&amp;-4&amp;10\\end{bmatrix}\\] }}{2={ \\[\\begin{bmatrix}2\\cdot 1&amp;2\\cdot 8&amp;2\\cdot -3\\\\2\\cdot 4&amp;2\\cdot -2&amp;2\\cdot 5\\end{bmatrix}\\] }={ \\[\\begin{bmatrix}2&amp;16&amp;-6\\\\8&amp;-4&amp;10\\end{bmatrix}\\] }} Matrix transposition The transpose of an m-by-n matrix A is the n-by-m matrix AT (also denoted Atr or tA) formed by turning rows into columns and vice versa: (AT)i,j = Aj,i. {^{ }={ \\[\\begin{bmatrix}1&amp;0\\\\2&amp;-6\\\\3&amp;7\\end{bmatrix}\\] }}{^{ }={ \\[\\begin{bmatrix}1&amp;0\\\\2&amp;-6\\\\3&amp;7\\end{bmatrix}\\] }} Matrix multiplication Multiplication of two matrices is defined if and only if the number of columns of the left matrix is the same as the number of rows of the right matrix. If A is an m-by-n matrix and B is an n-by-p matrix, then their matrix product AB is the m-by-p matrix whose entries are given by dot product of the corresponding row of A and the corresponding column of B:[11] [AB]i,j=ai,1b1,j+ai,2b2,j+‚ãØ+ai,nbn,j=‚àër=1nai,rbr,j,{{i,j}=a{i,1}b_{1,j}+a_{i,2}b_{2,j}++a_{i,n}b_{n,j}={r=1}^{n}a{i,r}b_{r,j},} Note Matrix multiplication satisfies the rules (AB)C = A(BC) (associativity), and (A + B)C = AC + BC as well as C(A + B) = CA + CB (left and right distributivity), whenever the size of the matrices is such that the various products are defined.[13] The product AB may be defined without BA being defined, namely if A and B are m-by-n and n-by-k matrices, respectively, and m ‚â† k. Even if both products are defined, they generally need not be equal, that is: AB ‚â† BA. Matrix representation of a CRD Let‚Äôs consider the CRD outlined in the previous module, we can write the effects model using matrix representation: \\[\\boldsymbol{y} = \\boldsymbol{X\\beta} + \\boldsymbol{\\epsilon}\\] where \\(\\boldsymbol{y} = \\begin{bmatrix} y_{11} \\\\ y_{12} \\\\ y_{13} \\\\ y_{14} \\\\ y_{21} \\\\ y_{22} \\\\ y_{23} \\\\ y_{24} \\\\ y_{31} \\\\ y_{32} \\\\ y_{33} \\\\ y_{34} \\end{bmatrix}\\), \\(\\boldsymbol{X} = \\begin{bmatrix} 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}\\), \\(\\boldsymbol{\\beta} = \\begin{bmatrix} \\alpha \\\\ \\tau_1 \\\\ \\tau_2 \\\\ \\tau_3 \\end{bmatrix}\\), and \\(\\boldsymbol{\\epsilon} = \\begin{bmatrix} \\epsilon_{11} \\\\ \\epsilon_{12} \\\\ \\epsilon_{13} \\\\ \\epsilon_{14} \\\\ \\epsilon_{21} \\\\ \\epsilon_{22} \\\\ \\epsilon_{23} \\\\ \\epsilon_{24} \\\\ \\epsilon_{31} \\\\ \\epsilon_{32} \\\\ \\epsilon_{33} \\\\ \\epsilon_{34} \\end{bmatrix}.\\) where \\(\\boldsymbol{\\epsilon} \\sim \\text{MVN}(\\boldsymbol{0}, \\boldsymbol{\\sigma^2 I})\\). The least squares estimators of \\(\\boldsymbol{\\beta}\\) are the solutions to the \\[\\boldsymbol{X^{&#39;}X\\beta}=\\boldsymbol{X^{&#39;}y}\\]. Recall that for a factor variable we take the one level (the first factor) into the baseline (i.e., the standard) and hence then the coefficients we estimate are compared to it (i.e., the differences in the mean). This is to ensure that the matrix \\(\\boldsymbol{X}\\) is full rank. So \\[\\boldsymbol{X} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\end{bmatrix}\\] Now, with three factor levels the least squares estimators of \\(\\boldsymbol{\\beta}\\) are (note the hat to denote the estimates) \\[(\\boldsymbol{X^{&#39;}X})^{-1}\\boldsymbol{X^{&#39;}y} = \\boldsymbol{\\hat{\\beta}}= \\begin{bmatrix} \\hat{\\alpha} - \\hat{\\tau_1} \\\\ \\hat{\\tau_2} - \\hat{\\tau_1} \\\\ \\hat{\\tau_3} - \\hat{\\tau_1} \\end{bmatrix} \\] A numeric example By hand Using the mask data from the previous chapter we have Mask Droplets Type 1 5.3 Type 1 6.0 Type 1 6.6 Type 1 4.9 Type 2 7.5 Type 2 7.1 Type 2 6.3 Type 2 7.6 Type 3 8.8 Type 3 6.9 Type 3 9.2 Type 3 10.3 so \\(\\boldsymbol{X^{&#39;}X} = \\begin{bmatrix}12&amp;4&amp;4 \\\\4&amp;4&amp;0 \\\\4&amp;0&amp;4 \\\\\\end{bmatrix}\\), \\(\\boldsymbol{X^{&#39;}y} = \\begin{bmatrix}86.5 \\\\28.5 \\\\35.2 \\\\\\end{bmatrix}\\), and \\((\\boldsymbol{X^{&#39;}X})^{-1} = \\begin{bmatrix}0.25&amp;-0.25&amp;-0.25 \\\\-0.25&amp;0.5&amp;0.25 \\\\-0.25&amp;0.25&amp;0.5 \\\\\\end{bmatrix}.\\) Therefore, \\[\\boldsymbol{\\hat{\\beta}} = (\\boldsymbol{X^{&#39;}X})^{-1}\\boldsymbol{X^{&#39;}y} = \\begin{bmatrix} \\hat{\\alpha} - \\hat{\\tau_1} \\\\ \\hat{\\tau_2} - \\hat{\\tau_1} \\\\ \\hat{\\tau_3} - \\hat{\\tau_1} \\end{bmatrix} = \\begin{bmatrix}0.25&amp;-0.25&amp;-0.25 \\\\-0.25&amp;0.5&amp;0.25 \\\\-0.25&amp;0.25&amp;0.5 \\\\\\end{bmatrix} \\times \\begin{bmatrix}86.5 \\\\28.5 \\\\35.2 \\\\\\end{bmatrix} = \\begin{bmatrix}5.7 \\\\1.425 \\\\3.1 \\\\\\end{bmatrix}\\] Using lm in R mod &lt;- lm(Droplets ~ Mask, data = df) summary(mod) ## ## Call: ## lm(formula = Droplets ~ Mask, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.9000 -0.5000 0.1500 0.4188 1.5000 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.7000 0.4935 11.550 1.07e-06 *** ## MaskType 2 1.4250 0.6979 2.042 0.07156 . ## MaskType 3 3.1000 0.6979 4.442 0.00162 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.987 on 9 degrees of freedom ## Multiple R-squared: 0.6872, Adjusted R-squared: 0.6177 ## F-statistic: 9.886 on 2 and 9 DF, p-value: 0.005354 "],["maximum-likelkihood-estimation.html", "Maximum likelkihood estimation", " Maximum likelkihood estimation Now, under the assumptions of a linear model then maximum likelihood estimation is equivalent to least squares. However, (as we‚Äôll see in a later module) we often need to be more flexible! Basic differeniation rules This section is a recap only, if you need a more in-depth overiew of differentiation then use the extra materials provided at the start of this module. For any functions ff and gg and any real numbers aa and bb, the derivative of the function h(x)=af(x)+bg(x){h(x)=af(x)+bg(x)} with respect to xx is h‚Ä≤(x)=af‚Ä≤(x)+bg‚Ä≤(x).{h‚Äô(x)=af‚Äô(x)+bg‚Äô(x).} Special cases include: The constant factor rule (af)‚Ä≤=af‚Ä≤{(af)‚Äò=af‚Äô} The sum rule (f+g)‚Ä≤=f‚Ä≤+g‚Ä≤{(f+g)‚Äò=f‚Äô+g‚Äô} The subtraction rule (f‚àíg)‚Ä≤=f‚Ä≤‚àíg‚Ä≤.{(f-g)‚Äò=f‚Äô-g‚Äô.} The product rule For the functions f and g, the derivative of the function h(x) = f(x) g(x) with respect to x is h‚Ä≤(x)=(fg)‚Ä≤(x)=f‚Ä≤(x)g(x)+f(x)g‚Ä≤(x).{h‚Äô(x)=(fg)‚Äò(x)=f‚Äô(x)g(x)+f(x)g‚Äô(x).} The chain rule The derivative of the function h(x)=f(g(x))h(x)=f(g(x)) is h‚Ä≤(x)=f‚Ä≤(g(x))‚ãÖg‚Ä≤(x).{h‚Äô(x)=f‚Äô(g(x))g‚Äô(x).} Maximum likelihood for a CRD Recall the following CRD equation \\[Y_{ik} = \\mu_k + \\epsilon_{ik}\\] where \\(Y_{ik}\\) is the response for the \\(k^{th}\\) experimental unit (\\(k = 1, ..., r_i\\), where \\(r_i\\) is the number of experimental replications in the \\(i^{th}\\) level of the treatment factor) subjected to the \\(i^{th}\\) level of the treatment factor (\\(i = 1, ..., t\\),). Here \\(\\mu_i\\) are the different (cell) means for each level of the treatment factor. Under the assumptions of a the CRD (i.e., \\(\\epsilon_{ik} \\sim N(0, \\sigma^2)\\)) then (for equal number of replicates) the estimates of the cell means (\\(\\mu_k\\)) are found by minimising the error of the sum of squares \\[SS_{\\epsilon} = \\Sigma_{i=1}^t \\Sigma_{k=1}^{r_i}(y_{ik}-\\mu_i)^2.\\] Taking the partial derivatives of \\(SS_{\\epsilon}\\) with respect to each cell mean, setting to zero, and solving each equation with give us our estimates: \\[\\frac{\\delta SS_{\\epsilon}}{\\delta \\mu_i} = -2 \\Sigma_{i=1}^t \\Sigma_{k=1}^{r_i}(y_{ik}-\\mu_i) = 0.\\] This works out as \\(\\hat{\\mu_i} = \\overline{y_i.}\\) So in our mask example \\(\\hat{\\mu}_{\\text{Type 1 }} = 5.7 ,\\; \\hat{\\mu}_{\\text{Type 2 }} = 7.125 \\; , \\&amp;\\; \\hat{\\mu}_{\\text{Type 3 }} = 8.8 .\\) Compare these estimates to those we obtained via least squares estimation in the previous section. Maximum likelihood in general "],["bayesian.html", "Bayesian", " Bayesian Bayes‚Äô rule The conditional probability of the event \\(A\\) conditional on the event \\(B\\) is given by P(A‚à£B)=P(A&amp;B)P(B). "],["module-5.html", "Module 5", " Module 5 glm and glmer "],["beyond-linear-models-to-generalised-linear-models-glms.html", "Beyond Linear Models to Generalised Linear Models (GLMs)", " Beyond Linear Models to Generalised Linear Models (GLMs) Recall the assumptions of a linear model The \\(i\\)th observation‚Äôs response, \\(Y_i\\), comes from a normal distribution Its mean, \\(\\mu_i\\), is a linear combination of the explanatory terms Its variance, \\(\\sigma^2\\), is the same for all observations Each observation‚Äôs response is independent of all others But, what if we want to rid ourselves from a model with normal errors? The answer: Generalised Linear Models. Counting animals‚Ä¶ A normal distribution does not adequately describe the response, the number of animals It is a continuous distribution, but the response is discrete It is symmetric, but the response is unlikely to be so It is unbounded, and assumes it is plausible for the response to be negative I addition, a linear regression model typically assumes constant variance, but int his situation this unlikely to be the case. So why assume a normal distribution? Let‚Äôs use a Poisson distribution instead. \\[\\begin{equation*} \\mu_i = \\beta_0 + \\beta_1 x_i, \\end{equation*}\\] So \\[\\begin{equation*} Y_i \\sim \\text{Normal}(\\mu_i\\, \\sigma^2), \\end{equation*}\\] becomes \\[\\begin{equation*} Y_i \\sim \\text{Poisson}(\\mu_i), \\end{equation*}\\] The Poisson distribution is commonly used as a general-purpose distribution for counts. A key feature of this distribution is \\(\\text{Var}(Y_i) = \\mu_i\\), so we expect the variance to increase with the mean. Other modelling approaches (not examinable) R function Use glm() Fit a linear model with a specific error structure specified using the family = argument (Poisson, binomial, gamma) gam() Fit a generalised additive model. The R package mgcv must be loaded lme() and nlme() Fit linear and non-linear mixed effects models. The R package nlme must be loaded lmer() Fit linear and generalised linear and non-linear mixed effects models. The package lme4 must be installed and loaded gls() Fit generalised least squares models. The R package nlme must be loaded Model formula syntax In R to specify the model you want to fit you typically create a model formula object; this is usually then passed as the first argument to the model fitting function (e.g., lm()). Some notes on syntax: Consider the model formula example y ~ x + z + x:z. There is a lot going on here: The variable to the left of ~ specifies the response, everything to the right specify the explanatory variables + indicated to include the variable to the left of it and to the right of it (it does not mean they should be summed) : denotes the interaction of the variables to its left and right Additional, some other symbols have special meanings in model formula: * means to include all main effects and interactions, so a*b is the same as a + b + a:b ^ is used to include main effects and interactions up to a specified level. For example, (a + b + c)^2 is equivalent to a + b + c + a:b + a:c + b:c (note (a + b + c)^3 would also add a:b:c) - excludes terms that might otherwise be included. For example, -1 excludes the intercept otherwise included by default, and a*b - b would produce a + a:b Mathematical functions can also be directly used in the model formula to transform a variable directly (e.g., y ~ exp(x) + log(z) + x:z). One thing that may seem counter intuitive is in creating polynomial expressions (e.g., \\(x^2\\)). Here the expression y ~ x^2 does not relate to squaring the explanatory variable \\(x\\) (this is to do with the syntax ^ you see above. To include \\(x^2\\) as a term in our model we have to use the I() (the ‚Äúas-is‚Äù operator). For example, y ~ I(x^2)). "],["module-6.html", "Module 6", " Module 6 Learning objectives Explain the aims and motivation behind cluster analysis and its relevance in biology Write R code to carry out hierarchical and k-means cluster analysis Interpret R output from hierarchical and k-means cluster analysis Interpret and communicate, to both a statistical and non-statistical audience, clustering techniques, specifically, Divisive methods, nonparametric algorithms such as k-means Agglomerative methods, clustering cases and/or variables into a hierarchy of sets (i.e., hierarchical clustering) Explain the aims and motivation behind Principal Component Analysis (PCA) and its relevance in biology Write R code to carry out PCA Interpret principal component scores and describe a subject with a high or low score Interpret R output from PCA Interpret and communicate, to both a statistical and non-statistical audience, dimaension reduction techniques Other resources Eigenfaces ClusterDucks K-means cluster analysis Little book for Multivariate Analysis ‚Äòexplor‚Äô is an R package to allow interactive exploration of multivariate analysis results The Mathematics Behind Principal Component Analysis (6 min read) ClusterDucks Little book for Multivariate Analysis ‚Äòexplor‚Äô is an R package to allow interactive exploration of multivariate analysis results The Mathematics Behind Principal Component Analysis (6 min read) "],["clustering.html", "Clustering", " Clustering So, it‚Äôs all about variation again! And the idea of minimizing it. Cluster analysis, or classification as it is known in the botanical literature, has the apparently simple aim of finding clusters in a data cloud of sampling units in the absence of any a priori information about which point belongs in which cluster. This apparently unambitious aim is unfortunately fraught with problems. The major difficulty is that no one seems to agree on precisely what a cluster is. For a very good reason, the human eye is unexcelled as a pattern recognition device, but we recognise clusters of points in a variety of different ways. For example, it is extremely difficult to think of a single definition that would adequately describe all the clusters in fig 7.1, even though they are quite obvious (I hope). Some workers have stressed the importance of cohesiveness (like fig 7.1a); others contiguity of points (7.1b); yet others have concentrated on distances such that all or most of the distances within a cluster are less than those to any point outside the cluster; and finally others have tried to make the definition so vague that it can include most of the possibilities without the necessity of actually defining anything. Everitt‚Äôs definition (Everitt 1980) seems to come as close to being useful as any: ‚ÄúClusters may be described as continuous regions of (a) space containing a relatively high density of points, separated from other such regions by regions containing a relatively low density of points.‚Äù Unfortunately it does not provide a rationale for a single comprehensive technique that can handle all the data structures shown and satisfy all the requirements of workers. Indeed it is extremely unlikely that any such method could ever be found, for there lies another problem, workers want the technique(s) for a number of different purposes: i) to find groups for classification; ii) to reduce the number of sampling units in an analysis by using a single representative from each cluster of similar individuals; iii) for data exploration and hypothesis generation; iv) for fitting distribution models and estimating their parameters; v) dissecting a continuous data cloud into relatively homogeneous zones; and many more. The only thing the large number of existing techniques have in common is that unlike canonical discriminant analysis (section 10) and discriminant function analysis (not covered in this course) there is no prior information about which sampling unit is in which group. Like the ordination methods of the earlier chapters, cluster analysis techniques operate on an unpartitioned data matrix to find, or impose, structure in the data cloud. One consequence of this variation in definition and use is that cluster analysis as such does not exist. The title refers to an enormous and extraordinarily diverse family of techniques. For someone to say that they used cluster analysis is about as informative as their saying they studied an insect. To cover all the techniques would take a whole (large) book. So for this course I shall content myself with covering some of the common ones and ones I think are potentially most useful. Given the diversity of techniques it is very important to choose the technique with a clear idea of what it is required to do. Like selecting a similarity or distance metric (Section 4), the choice must be made with care after consideration of the nature of the data, your objectives, and the available alternatives. However the most important thing to remember when using a clustering technique is:- you must not believe the result. The pattern you get is at most a plausible way of viewing the data. By using an appropriate method and by employing validation techniques the plausibility can be enhanced, but no cluster analysis can be relied on to produce truth. With real data, different methods will nearly always produce different results. If the structure in the data is fairly obvious then these answers may not differ much, but if there is any ambiguity in the data then the methods may well give contradictory results. Partitioning methods. Though the hierarchical methods have been historically more important, the partitioning methods are becoming increasingly popular, and it is easy to see why. The hierarchical methods are restricted to an often inappropriate nested structure so that at each level (number of clusters) the solution is constrained by the previous one. In the partitioning or segmentation methods the solution at any level is independent of the others and can therefore be globally optimal - if you‚Äôre lucky. For a single run of a partitioning method, the desired number of clusters ( k ) is usually fixed - some techniques do allow some small adjustment in this number during the process. Of course since the correct number of clusters is usually not known, the program is normally run with different values of k and the optimum number of clusters chosen (covered later). There are two major phases to a partitioning method: i) an initial allocation (usually rather arbitrary) into k preliminary clusters; ii) reallocation of each point either to the closest centroid, or so as to optimise some property of the clusters. This is repeated until there is no further improvement, then the program stops. The initial allocation is usually started by choosing k sampling units to use as ‚Äúseeds‚Äù to ‚Äúcrystallise‚Äù the clusters. There are a number of ways to choose these seeds; it depends on the program. As we shall see it is a tremendous advantage if you can put in your own set. These seeds are used as the initial centres of the clusters, points are allocated to the nearest cluster centre, and in most programs the cluster centroid is adjusted as they are added. The methods we consider here (there are others) the k -means methods, run through the sampling units reallocating them to the cluster with the closest centroid; they pass and repass through the data till no further reallocation of points is possible. Some programs then try swapping pairs of points between clusters, to further improve the solution, and to protect against local optima. K-means partitioning methods The k -means methods are generally the fastest clustering methods, but they are inclined to be trapped by local optima and tend to produce equal volume spherical solutions. They are also very ensitive to starting strategy. Some workers suggest that random starting values should not be used. Seber reports a study as having located the global optimum only 3 times from 24 random starts! However their performance in the few Monte Carlo simulation studies that have incorporated them has been good relative to alternative methods, particularly when the solution from a hierarchical method was used as the starting configuration. In fact, it has tended to be better than the best hierarchical methods considered (Ward‚Äôs and average linkage). If the data set is particularly large, a sub-sample of the points could be clustered and the estimated centroids of the resulting clusters used as seeds for the analysis of the full data set. Some programs allow you to vary how the distance to the centroid is measured. Some programs normally use the squared distance which means that it is minimising the trace( W ) where W is the within cluster variance-covariance matrix pooled over all the clusters, i.e.¬†the total within sample variance. This is an appealingly statistical thing to optimise. Hierarchical methods. These methods assume that the groupings in the data cloud have a hierarchical structure. The smaller groups form larger groups which form larger groups and so on - a nested classification. If this assumption is untrue then the techniques can be expected to distort the true structure of the data. Most of the commonly used techniques are members of this group. They are widely available, all the major packages have a selection, and they are relatively easy to use, though often less so to interpret. Hierarchical organisation is often difficult to justify for real data sets. Though there may be more than one level of grouping there may be no reason to assume that they are nested. For example, it has been shown that the clusterings defined by the optimum sum of squares at various levels of k may not be nested for all data sets; so a hierarchical method may be unsuitable for any given data set. There are two approaches to hierarchical clustering, agglomerative and divisive . Agglomerative methods start from the individual sampling units forming them into groups and fusing the groups till there is only one that includes all the points. If we can describe this as working from the bottom up, then the divisive techniques work from the top down. The groups are formed by splitting the data set successively until there are as many groups as points. Hierarchical agglomerative clustering. All of the commonly used hierarchical methods are agglomerative. Most of them operate in the same way: first all sampling units that are zero distance apart are fused into clusters. The threshold for fusion is then raised from zero until two clusters (they may be individual points) are found that are close enough to fuse. The threshold is raised, fusing the clusters as their distance apart is reached until all the clusters have been fused into one big one. Thus the close clusters are fused first, then those further apart, till all have been fused. This process allows the history of the fusions, the hierarchy, to be displayed as a dendrogram. This is an advantage of the agglomerative methods, if the data have a nested structure these techniques lead to a useful way of displaying it. Other advantages are the ready availability of programs and their ability to handle quite large data sets - at reasonable expense. Unlike the optimisation or k -means methods, most of the agglomerative techniques can use a broad range of similarity or distance measures. This of course means that considerable care must be taken to choose the appropriate one; different measures often lead to different results. Inevitably, given the variety of definitions of a cluster, there are a large number of different hierarchical agglomerative techniques. They mainly differ in the details of the fusion rule. For most of them the rule is simply stated: two clusters should be fused if the distance between them has been reached by the threshold. The problem is to estimate that distance. It can be done in a variety of ways and will usually affect the results. As we shall see, different types of clusters need different ways of estimating intercluster distance. We shall consider the four most commonly used methods. Single linkage (nearest neighbour) clustering. Single Linkage (nearest neighbour/minimal jump): Computes the distance between clusters as the smallest distance between any two points in the two clusters The distance between two clusters is the distance between their nearest points (Figure 7.3a).The simplicity of this method makes it easy to program and extremely efficient. It was one of the most popular techniques in the early days of clustering; but since then, despite support from the theoreticians, it has been used less frequently. In general it has not performed well. It identifies clusters on the basis of isolation, how far apart they are at their closest points. This means that if there are any intermediate points then single linkage will fuse the groups without leaving any trace of their separate identities. This is called ‚Äúchaining‚Äù, which leads to characteristic and rather uninformative dendrograms. It is the chief weakness of the method. Its strength is that if the clusters are well separated in the data, then single linkage can handle groups of different shapes and sizes, even long thin straggly ones (e.g.¬†Figure 7.1c) that other methods often cannot recover. It has other advantages, it will give the same clustering after any monotonic transformation of the distance measure - that means that it is fairly robust to the choice of measure. It is insensitive to tied distances - some methods suffer from indeterminacy if there are too many ties; a bit like degenerate solutions in non-metric MDS, (section 6.3.3.iii) and though the results are seldom as pretty, they can be just as meaningless. As a cluster analysis single linkage is usually not very useful (unless the data is of the right type). Many investigations have found it performs badly with even slightly messy data. Complete linkage (farthest neighbour) clustering. Complete Linkage (maximum jump): Calculates the maximum distance betweentwo points from each cluster In many respects complete linkage clustering is the opposite of single linkage. Instead of measuring the distance between two clusters as that between their two nearest members; it uses that between the two farthest members (Figure 7.3b). In consequence the resulting clusters are compact, spherical and well defined. Unlike single linkage it can be sensitive to tied distances. There are similarities, the clustering it gives is also invariant under monotonic transformation of the distances; it is robust to a certain amount of measurement error and choice of distance. Unfortunately it is sensitive to even a single change in the rank order of the distances in the dissimilarity matrix (Seber 1984), and does not cope well with outliers. However, in Monte Carlo simulations, it nearly always performed better than single linkage; though usually not quite as well as Ward‚Äôs or group average. Group average linkage (UPGMA) This is probably the most popular hierarchical clustering method - for a very good reason - it usually works well. It could be thought of as an attempt to avoid the extremes of the single and complete linkage methods. The distance between two clusters is the average of the distances between the members of the two groups (Figure 7.3c). If the distances are Euclidean this is the distance between the centroids plus the within group scatter. As a result this method tends to produce compact spherical clusters. Like its main rival Ward‚Äôs method, average linkage has generally performed well in Monte Carlo simulations, and its continued popularity is because it consistently, though not inevitably, gives adequate results. However, Ward‚Äôs generally performed better, particularly when there was some overlap between the groups. When intermediate points and outliers were removed (‚Äútrimming‚Äù or ‚Äúincomplete coverage‚Äù), group average‚Äôs performance was considerably improved. It performed poorly with mixtures of multivariate normal distributions probably because of the overlap between clusters.. Ward‚Äôs method (incremental sums of squares, minimum variance, agglomerative sums of squares). Ward‚Äôs method: where the goal is to minimize the variance within clusters Ward‚Äôs method is the hierarchical version of the k-means partitioning method. At each fusion it attempts to minimise the increase in total sum of squared distances within the clusters. This is equivalent to minimising the sum of squared within cluster deviations from the centroids - i.e. trace( W ). Since at any one stage it can only fuse those clusters already in existence - it is not allowed to reallocate points - it can only be stepwise optimal. It cannot find the true minimum configuration at each level, so it would not be expected to recover natural clusters as well as the non-hierarchical methods that also minimise trace( W ). A bad start to the agglomeration process can place the algorithm on a path from which it can never reach the global optimum for a given number of clusters. Despite this, Ward‚Äôs method has performed well in simulations; one of the two best hierarchical methods overall. Its chief flaw is a tendency to form clusters of equal size, regardless of the true number. So when the number of points in the clusters are different, group average and complete link may give better results. Like the complete linkage and group average methods it is also biased towards forming spherical clusters; though perhaps not as strongly as they are. It may also be rather sensitive to outliers. However it appears to perform well when there is a lot of overlap, when many of the other techniques have difficulties. It has been found in simulations that Ward‚Äôs performed best of the hierarchical methods at recovering natural clusters, but that the k - means and optimising methods were better. "],["tldr.html", "TL;DR", " TL;DR Goals See measures of (dis)similarity and distances that help us define clusters. Uncover hidden or latent clustering by partitioning the data into tighter sets. Divisive methods: nonparametric algorithms such as k-means to split data into a small number of clusters. Agglomerative methods: clustering cases and/or variables into a hierarchy of sets - hierarchical clustering. Study how to validate clusters through resampling-based bootstrap methods Clustering algorithms The distances are used to construct the clusters. Agglomerative methods, that build a hierarchical clustering tree Partitioning methods that separate the data into subsets Both types of methods require a choice to be made: the number k of clusters. Partitioning methods such as k-means this choice has to be made at the outset whereas for hierarchical clustering this can be deferred to the end of the analysis. k-means K-means clustering involves defining clusters so that the overall variation within a cluster (known as total within-cluster variation) is minimized. How do we define this variation? Typically, using Euclidean distances; the total within-cluster variation, is in this case, is defined as the sum of squared distances Euclidean distances between observations and the corresponding cluster centroid. In summary, this is the procedure The number of clusters (k) are specified k objects from the dataset are selected at random and set as the initial cluster centers or means Each observation is assigned to their closest centroid (based on the Euclidean distance between the object and the centroid) For each of the k clusters the cluster centroid is then updated based on calculating the new mean values of all the data points in the cluster Repeat the two previous steps until 1) the cluster assignments stop changing or 2) the maximum number of iterations is reached Hierarchical clustering Hierarchical clustering is a bottom-up approach: + similar observations and subclasses are assembled iteratively Linnaeus made nested clusters of organisms according to specific characteristics. The order of the labels does not matter within sibling pairs. + Horizontal distances are usually meaningless + Vertical distances can encode some information. In summary, this is the procedure Start with a matrix of distances, (or similarities) between pairs of observations (cases) Choice of distance measure key first step Algorithm: Initial n singleton clusters Scan distance matrix for two closest individuals, group them together Compute distance from cluster of size 2 to remaining n-1 singleton clusters Method Pros Cons Single linkage number of clusters comblike trees. Complete linkage compact clusters one obs. can alter groups Average linkage similar size and variance not robust Centroid robust to outliers smaller number of clusters Ward minimising an inertia clusters small if high variability Identify optimal number of clusters Identifying the appropriate k is important because too many or too few clusters impedes viewing overall trends. Too many clusters can lead to over-fitting (which limits generalizations) while insufficient clusters limits insights into commonality of groups. There are assorted methodologies to identify the appropriate \\(k\\). Tests range from blunt visual inspections to robust algorithms. The optimal number of clusters is ultimately a subjective decision. With ants The data pitfalls.csv is available on CANVAS Data were collected on the distribution of ant species at 30 sites across the Auckland region using pitfall traps. Twenty pitfall traps at each site were left open for ten days and the number of individuals captured counted Data used here are standardised \\(\\text{log}(x + 1)\\) transformed for the four most abundant species: + *Nylanderia spp* + *Pheidole rugosula* + *Tetramorium grassii* + *Pachycondyla sp* At each location twenty pitfall traps were placed in each of four habitats (Forest, Grass, Urban, Scrub) and left for ten days. At the end of this sampling all individuals in the pitfall traps were identified and summed at each site (location x habitat). This sampling protocol was repeated for 3 months over summer 2011. ## Rows: 30 Columns: 8 ## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ## Delimiter: &quot;,&quot; ## chr (3): Location, Habitat, Site ## dbl (5): Month, Nyl, Phe, Tet, Pac ## ## ‚Ñπ Use `spec()` to retrieve the full column specification for this data. ## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 30 √ó 8 ## Location Habitat Month Site Nyl Phe Tet Pac ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 West Forest 1 WF1 0 0 0 157 ## 2 West Grass 1 WG1 0 2 7 37 ## 3 West Urban 1 WU1 3 7 0 0 ## 4 West Forest 2 WF2 0 0 0 31 ## 5 West Grass 2 WG2 5 0 25 0 ## 6 West Forest 3 WF3 0 0 0 21 ## 7 West Grass 3 WG3 0 3 2 1 ## 8 West Urban 3 WU3 0 1 0 0 ## 9 Central Forest 1 CF1 0 0 0 1 ## 10 Central Grass 1 CG1 0 3 22 2 ## # ‚Ä¶ with 20 more rows library(tidyverse) ants &lt;- read_csv(&quot;pitfalls.csv&quot;) Hierarchical clustering Data are species counts, so we will use Bray Curtis measure: pitfall.dist &lt;- vegan::vegdist(ants[,5:8], method = &quot;bray&quot;, binary = FALSE) factoextra::fviz_dist(pitfall.dist) Computing 4 dendrograms Single-linkage single &lt;- ants[,5:8] %&gt;% vegan::vegdist(., method = &quot;bray&quot;, binary = FALSE) %&gt;% hclust(method = &quot;single&quot;) plot(single, labels = ants$Site) Maximum linkage complete &lt;- ants[,5:8] %&gt;% vegan::vegdist(., method = &quot;bray&quot;, binary = FALSE) %&gt;% hclust(method = &quot;complete&quot;) plot(complete, labels = ants$Site) Average linkage (UPGMA) average &lt;- ants[,5:8] %&gt;% vegan::vegdist(., method = &quot;bray&quot;, binary = FALSE) %&gt;% hclust(method = &quot;average&quot;) plot(average, labels = ants$Site) Ward‚Äôs ward &lt;- ants[,5:8] %&gt;% vegan::vegdist(., method = &quot;bray&quot;, binary = FALSE) %&gt;% hclust(method = &quot;ward.D&quot;) plot(ward, labels = ants$Site) WHAT ARE DENDROGRAMS GOOD FOR? Suggesting clusters for further study‚Ä¶ Using the function cutree() to split into clusters and plot: ants$clust4 &lt;- cutree(ward, k = 4) library(ape) ## install pitfall.phylo &lt;- as.phylo(ward) pitfall.phylo$tip.label &lt;- ants$Site ## Set colours colours &lt;- c(&quot;red&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;black&quot;) plot(pitfall.phylo, cex = 0.6, tip.color = colours[ants$clust4], label.offset = 0.05) k-means using the palmerpenguins data library(palmerpenguins) ## getting rid of NAs penguins_nafree &lt;- penguins %&gt;% drop_na() ## introducing a new package GGally, please install ## using install.packages(&quot;GGally&quot;) library(GGally) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 penguins_nafree %&gt;% select(species, where(is.numeric)) %&gt;% ggpairs(columns = c(&quot;flipper_length_mm&quot;, &quot;body_mass_g&quot;, &quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;)) We see that a lot of these variables (e.g., flipper_length_mm, body_mass_g, and bill_length_mm) are relatively strongly (positively) related to one another. Could they actually be telling us the same information? Combined we could think of these three variables all telling us a little about bigness of penguin. Is there a way we could reduce these three variables, into say 1, to represent the bigness of a penguin. We may not need all the information (variation) captured by these variables, but could get away with fewer new uncorrelated variables that represent basically the same information (e.g., penguin bigness), thereby, reducing the dimensionality of the data (more on this later). ## create a data frame of what we&#39;re interested in df &lt;- penguins_nafree %&gt;% select(where(is.numeric), -year) We use the kmeans() function. The first argument of kmeans() should be the dataset you wish to cluster. Below we use data frame df, the penguin data discussed above. But how many clusters do we choose? Let‚Äôs try 1 to 5‚Ä¶ (i.e., using the centers argument). Setting nstart = 25 means that R will try 25 different random starting assignments and then select the best results corresponding to the one with the lowest within cluster variation. ## set the seed so we all start off in the same place set.seed(4321) ## one cluster k1 &lt;- kmeans(df, centers = 1, nstart = 25) ## two clusters k2 &lt;- kmeans(df, centers = 2, nstart = 25) ## three clusters k3 &lt;- kmeans(df, centers = 3, nstart = 25) ## four clusters k4 &lt;- kmeans(df, centers = 4, nstart = 25) ## five clusters k5 &lt;- kmeans(df, centers = 5, nstart = 25) The kmeans() function returns a list of components: cluster, integers indicating the cluster to which each observation is allocated centers, a matrix of cluster centers/means totss, the total sum of squares withinss, within-cluster sum of squares, one component per cluster tot.withinss, total within-cluster sum of squares betweenss, between-cluster sum of squares size, number of observations in each cluster Choosing the number of clusters We have an idea there may be 3 clusters, perhaps, but how do we know this is the best fit? Remember it‚Äôs a subjective choice and we‚Äôll be looking at a few pointers Visual inspection method library(factoextra) ## a new packahe for kmeasn viz, please install ## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa p1 &lt;- fviz_cluster(k1, data = df) p2 &lt;- fviz_cluster(k2, data = df) p3 &lt;- fviz_cluster(k3, data = df) p4 &lt;- fviz_cluster(k4, data = df) p5 &lt;- fviz_cluster(k5, data = df) ## for arranging plots library(patchwork) (p1| p2| p3)/ (p4 | p5) Alternatively, you can use standard pairwise scatter plots to illustrate the clusters compared to the original variables. df %&gt;% mutate(cluster = k3$cluster, species = penguins_nafree$species) %&gt;% ggplot(aes(flipper_length_mm, bill_depth_mm, color = factor(cluster), label = species)) + geom_text() Elbow method Optimal clusters are at the point in which the knee ‚Äúbends‚Äù or in mathematical terms when the marginal total within sum of squares (tot.withinss) for an additional cluster begins to decrease at a linear rate This is easier to see via a plot: fviz_nbclust(df, kmeans, method = &quot;wss&quot;) + labs(subtitle = &quot;Elbow method&quot;) There is a pretty obvious inflection (elbow) at 2 clusters, but maybe at 3 too. We can rule out an optimal number of clusters above 3 as there is then only a minimal marginal reduction in total within sum of squares. However, the model is ambiguous on whether 2 or 3 clusters is optimal‚Ä¶ Silhouette method # Silhouette method fviz_nbclust(df, kmeans, method = &quot;silhouette&quot;)+ labs(subtitle = &quot;Silhouette method&quot;) Gap method # Gap statistic # recommended value: nboot = 500 for your analysis (it will take a while) set.seed(123) ## remove this fviz_nbclust(df, kmeans, nstart = 25, method = &quot;gap_stat&quot;, nboot = 50)+ labs(subtitle = &quot;Gap statistic method&quot;) Basically it‚Äôs up to you to collate all the suggestions and make and informed decision ## Trying all the cluster indecies AHHHHH library(NbClust) cluster_30_indexes &lt;- NbClust(data = df, distance = &quot;euclidean&quot;, min.nc = 2, max.nc = 9, method = &quot;complete&quot;, index =&quot;all&quot;) ## *** : The Hubert index is a graphical method of determining the number of clusters. ## In the plot of Hubert index, we seek a significant knee that corresponds to a ## significant increase of the value of the measure i.e the significant peak in Hubert ## index second differences plot. ## ## *** : The D index is a graphical method of determining the number of clusters. ## In the plot of D index, we seek a significant knee (the significant peak in Dindex ## second differences plot) that corresponds to a significant increase of the value of ## the measure. ## ## ******************************************************************* ## * Among all indices: ## * 5 proposed 2 as the best number of clusters ## * 6 proposed 3 as the best number of clusters ## * 1 proposed 4 as the best number of clusters ## * 4 proposed 5 as the best number of clusters ## * 1 proposed 8 as the best number of clusters ## * 3 proposed 9 as the best number of clusters ## ## ***** Conclusion ***** ## ## * According to the majority rule, the best number of clusters is 3 ## ## ## ******************************************************************* fviz_nbclust(cluster_30_indexes) + theme_minimal() + labs(title = &quot;Frequency of Optimal Clusters using 30 indexes in NbClust Package&quot;) ## Warning in if (class(best_nc) == &quot;numeric&quot;) print(best_nc) else if ## (class(best_nc) == : the condition has length &gt; 1 and only the first element ## will be used ## Warning in if (class(best_nc) == &quot;matrix&quot;) .viz_NbClust(x, print.summary, : the ## condition has length &gt; 1 and only the first element will be used ## Warning in if (class(best_nc) == &quot;numeric&quot;) print(best_nc) else if ## (class(best_nc) == : the condition has length &gt; 1 and only the first element ## will be used ## Warning in if (class(best_nc) == &quot;matrix&quot;) {: the condition has length &gt; 1 and ## only the first element will be used ## Among all indices: ## =================== ## * 2 proposed 0 as the best number of clusters ## * 1 proposed 1 as the best number of clusters ## * 5 proposed 2 as the best number of clusters ## * 6 proposed 3 as the best number of clusters ## * 1 proposed 4 as the best number of clusters ## * 4 proposed 5 as the best number of clusters ## * 1 proposed 8 as the best number of clusters ## * 3 proposed 9 as the best number of clusters ## * 3 proposed NA&#39;s as the best number of clusters ## ## Conclusion ## ========================= ## * According to the majority rule, the best number of clusters is 3 . Not obvious, basically still undecided between 2 and 3, but according to the absolute majority rule the ‚Äúbest‚Äù number is 3 "],["tldr-k-means-clustering.html", "TL;DR k-means clustering", " TL;DR k-means clustering Artwork by @allison_horst "],["dimension-reduction.html", "Dimension reduction", " Dimension reduction Reduction of dimensions is needed when there are far too many features in a dataset, making it hard to distinguish between the important ones that are relevant to the output and the redundant or not-so important ones. Reducing the dimensions of data is called dimensionality reduction. So the aim is to find the best low-dimensional representation of the variation in a multivariate (lots and lots of variables) data set, but how do we do this? PCA is a member of a family of techniques for dimension reduction (ordination). The word ordination was applied to dimension reduction techniques by botanical ecologists whose aim was to identify gradients in species composition in the field. For this reason they wanted to reduce the quadrat √ó species (observations √ó variables) data matrix to a single ordering (hence ordination) of the quadrats which they hoped would reflect the underlying ecological gradient. One way is termed Principal Component Analysis (PCA). PCA is a feature extraction method that reduces the dimensionality of the data (number of variables) by creating new uncorrelated variables while minimizing loss of information on the original variables. Think of a baguette. The baguette pictured here represents two data dimensions: 1) the length of the bread and 2) the height of the bread (we‚Äôll ignore depth of bread for now). Think of the baguette as your data; when we carry out PCA we‚Äôre rotating our original axes (x- and y-coordinates) to capture as much of the variation in our data as possible. This results in new uncorrelated variables that each explain a % of variation in our data; the procedure is designed so that the first new variable (PC1) explains the most, the second (PC2) the second most and so on. Now rather than a baguette think of data; the baguette above represent the shape of the scatter between the two variables plotted below. The rotating grey axes represent the PCA procedure, essentially searching for the best rotation of the original axes to represent the variation in the data as best it can. Mathematically the Euclidean distance (e.g., the distance between points \\(p\\) and \\(q\\) in Euclidean space, \\(\\sqrt{(p-q)^2}\\)) between the points and the rotating axes is being minimized (i.e., the shortest possible across all points), see the blue lines. Once this distance is minimized across all points we ‚Äúsettle‚Äù on our new axes (the black tiled axes). Luckily we can do this all in R! PCA Principal Component Analysis (PCA) is a technique for the analysis of an unstructured sample of multivariate data. Its aim is to display the relative positions of the observations in the data cloud in fewer dimensions (while losing as little information as possible) and to help give insight into the way the observations vary. It is not a hypothesis testing technique (like t-test or Analysis of Variance); it is an exploratory, hypothesis generating tool that describes patterns of variation, and suggests relationships that should be investigated further. Sacling for PCA A major problem with PCA is that the components are not scale invariant. That means if we change the units in which our variables are expressed, we change the components; and not in any simple way either. So, every scaling or adjustment of the variables in preparation for the analysis could (and usually does ) produce a separate component structure. As I showed in section 4 sensible pre- treatment of the data by standardisation or transformation can often increase the interpretability and biological relevance of the results. It is therefore important to choose a standardisation or transformation carefully. In particular PCA will give different results depending on whether we analyse the covariance matrix, where the data have merely been centred (corrected for the column, variable, mean), or the correlation matrix, where the data have been standardised to z -scores (centred and converted into standard deviation units). This is particularly important, as many computer programs to do PCA automatically analyse the correlation matrix . If you do not want that standardisation; you may have to explicitly ask for the covariance matrix. As you would expect, the results from the two analyses will usually be very different Using the palmerpenguins data ## getting rid of NAs penguins_nafree &lt;- penguins %&gt;% drop_na() When carrying out PCA we‚Äôre only interested in numeric variables, so let‚Äôs just plot those. We can use the piping operator %&gt;% to do this with out creating a new data frame library(GGally) penguins_nafree %&gt;% dplyr::select(species, where(is.numeric)) %&gt;% ggpairs(aes(color = species), columns = c(&quot;flipper_length_mm&quot;, &quot;body_mass_g&quot;, &quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;)) Using prcomp() There are three basic types of information we obtain from Principal Component Analysis: PC scores: the coordinates of our samples on the new PC axis: the new uncorrelated variables (stored in pca$x) Eigenvalues: (see above) represent the variance explained by each PC; we can use these to calculate the proportion of variance in the original data that each axis explains Variable loadings (eigenvectors): these reflect the weight that each variable has on a particular PC and can be thought of as the correlation between the PC and the original variable Before we carry out PCA we should scale out data. WHY? pca &lt;- penguins_nafree %&gt;% dplyr::select(where(is.numeric), -year) %&gt;% ## year makes no sense here so we remove it and keep the other numeric variables scale() %&gt;% ## scale the variables prcomp() ## print out a summary summary(pca) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 1.6569 0.8821 0.60716 0.32846 ## Proportion of Variance 0.6863 0.1945 0.09216 0.02697 ## Cumulative Proportion 0.6863 0.8809 0.97303 1.00000 This output tells us that we obtain 4 principal components, which are called PC1 PC2, PC3, and PC4 (this is as expected because we used the 4 original numeric variables!). Each of these PCs explains a percentage of the total variation (Proportion of Variance) in the dataset: PC1 explains \\(\\sim\\) 68% of the total variance, which means that just over half of the information in the dataset (5 variables) can be encapsulated by just that one Principal Component. PC2 explains \\(\\sim\\) 19% of the variance. PC3 explains \\(\\sim\\) 9% of the variance. PC4 explains \\(\\sim\\) 2% of the variance. From the Cumulative Proportion row we see that by knowing the position of a sample in relation to just PC1 and PC2 we can get a pretty accurate view on where it stands in relation to other samples, as just PC1 and PC2 explain 88% of the variance. The loadings (relationship) between the initial variables and the principal components are stored in pca$rotation: pca$rotation ## PC1 PC2 PC3 PC4 ## bill_length_mm 0.4537532 -0.60019490 -0.6424951 0.1451695 ## bill_depth_mm -0.3990472 -0.79616951 0.4258004 -0.1599044 ## flipper_length_mm 0.5768250 -0.00578817 0.2360952 -0.7819837 ## body_mass_g 0.5496747 -0.07646366 0.5917374 0.5846861 Here we can see that bill_length_mm has a strong positive relationship with PC1, whereas bill_depth_mm has a strong negative relationship. Both fliper_length_mm and body_mass_g also have a strong positive relationship with PC1. Plotting this we get The new variables (PCs) are stored in pca$x, lets plot some of them alongside the loadings using a biplot. For PC1 vs PC2: library(factoextra) ## install this package first fviz_pca_biplot(pca, geom = &quot;point&quot;) + geom_point (alpha = 0.2) Now for PC2 vs PC3 fviz_pca_biplot(pca, axes = c(2,3),geom = &quot;point&quot;) + geom_point (alpha = 0.2) But how many PCs (new variables) do we keep? The whole point of this exercise is to reduce the number of variables we need to explain the variation in our data. So how many of these new variables (PCs) do we keep? To assess this we can use the information printed above alongside a screeplot: fviz_screeplot(pca) Principal components from the original variables Recall that the principal components are a linear combination of the (statndardised) variables. So for PC1 loadings1 &lt;- pca$rotation[,1] loadings1 ## bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## 0.4537532 -0.3990472 0.5768250 0.5496747 Therefore, the first Principle Component will be \\(0.454\\times Z1 -0.399 \\times Z2 + 0.5768 \\times Z3 + 0.5497 \\times Z3\\) where \\(Z1\\), \\(Z2\\), \\(Z3\\). and \\(Z4\\) are the scaled numerical variables form the penguins dataset (i.e., bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g). To compute this we use R: scaled_vars &lt;- penguins_nafree %&gt;% dplyr::select(where(is.numeric), -year) %&gt;% scale() %&gt;% as_tibble() ## By &quot;Hand&quot; by_hand &lt;- loadings1[1]*scaled_vars$&quot;bill_length_mm&quot; + loadings1[2]*scaled_vars$&quot;bill_depth_mm&quot; + loadings1[3]*scaled_vars$&quot;flipper_length_mm&quot; + loadings1[4]*scaled_vars$&quot;body_mass_g&quot; ## From PCA pc1 &lt;- pca$x[,1] plot(by_hand,pc1) Athletes You‚Äôll find the athletes.csv file on CANVAS. athletes &lt;- read_csv(&quot;athletes.csv&quot;) athletes ## # A tibble: 33 √ó 10 ## m100 long weight highj m400 m110 disc pole javel m1500 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 11.2 7.43 15.5 2.27 48.9 15.1 49.3 4.7 61.3 269. ## 2 10.9 7.45 15.0 1.97 47.7 14.5 44.4 5.1 61.8 273. ## 3 11.2 7.44 14.2 1.97 48.3 14.8 43.7 5.2 64.2 263. ## 4 10.6 7.38 15.0 2.03 49.1 14.7 44.8 4.9 64.0 285. ## 5 11.0 7.43 12.9 1.97 47.4 14.4 41.2 5.2 57.5 257. ## 6 10.8 7.72 13.6 2.12 48.3 14.2 43.1 4.9 52.2 274. ## 7 11.2 7.05 14.1 2.06 49.3 14.4 41.7 5.7 61.6 291. ## 8 11.0 6.95 15.3 2 48.2 14.4 41.3 4.8 63 266. ## 9 11.2 7.12 14.5 2.03 49.2 14.7 42.4 4.9 66.5 270. ## 10 11.2 7.28 15.2 1.97 48.6 14.8 48.0 5.2 59.5 292. ## # ‚Ä¶ with 23 more rows athletes %&gt;% ggpairs() corrplot::corrplot(cor(athletes), method = &quot;ellipse&quot;, type = &quot;upper&quot;) pca &lt;- athletes %&gt;% scale() %&gt;% prcomp() summary(pca) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 1.8488 1.6144 0.97123 0.9370 0.74607 0.70088 0.65620 ## Proportion of Variance 0.3418 0.2606 0.09433 0.0878 0.05566 0.04912 0.04306 ## Cumulative Proportion 0.3418 0.6025 0.69679 0.7846 0.84026 0.88938 0.93244 ## PC8 PC9 PC10 ## Standard deviation 0.55389 0.51667 0.31915 ## Proportion of Variance 0.03068 0.02669 0.01019 ## Cumulative Proportion 0.96312 0.98981 1.00000 ## standard deviations of newly rotated variables pca$sdev ## [1] 1.8488478 1.6144328 0.9712345 0.9370279 0.7460742 0.7008762 0.6561975 ## [8] 0.5538936 0.5166715 0.3191460 ## p = 10 variables sum(pca$sdev^2) ## [1] 10 screeplot ## screeplot of sdev^2 fviz_screeplot(pca, choice = &quot;eigenvalue&quot;) + geom_hline(yintercept = 1) biplots fviz_pca_biplot(pca,geom = &quot;point&quot;) + geom_point (alpha = 0.2) discus, shot, &amp; javelin more strongly correlated with one another running events more strongly correlated to one another could think of PC1 as a fieldness variable (i.e., strong +ve loadings from field events and -ve loadings from track events) Finding correlation between a PC and the original variable: pca$rotation %*% diag(pca$sdev) ## [,1] [,2] [,3] [,4] [,5] [,6] ## m100 -0.7689031 0.24024073 -0.25977801 0.08276995 -0.329999387 0.02152557 ## long 0.7285412 -0.24552536 -0.16408953 0.22886871 0.275237129 -0.06572987 ## weight 0.4975355 0.78063857 0.09569838 0.10097671 -0.007277715 0.16121594 ## highj 0.3924767 0.04504025 -0.83039242 -0.36351428 -0.001399867 0.05224598 ## m400 -0.6579077 0.56853834 -0.18404546 -0.07550062 0.109647051 -0.22913668 ## m110 -0.8014415 0.11231318 -0.12253105 0.35821666 -0.066253470 0.14752835 ## disc 0.3250132 0.81260004 0.04477360 -0.02397295 0.014442957 0.43097750 ## pole 0.7101094 0.24149011 0.13293514 -0.13489967 -0.534743786 -0.24373698 ## javel 0.3326883 0.60049956 -0.18679561 0.56265307 0.071311293 -0.30659402 ## m1500 -0.3145678 0.67962013 0.21615050 -0.45506038 0.253495250 -0.21049009 ## [,7] [,8] [,9] [,10] ## m100 0.16693567 -0.367626287 0.056004765 -0.03494024 ## long 0.49249877 -0.078245304 -0.023838755 -0.01780972 ## weight -0.07261722 -0.040160366 -0.218281354 -0.20767995 ## highj -0.08866817 0.086094934 0.052734102 -0.03810980 ## m400 0.09274620 0.081333350 -0.336230314 0.10749282 ## m110 0.17883327 0.353939993 0.107074242 -0.08288795 ## disc 0.09447447 -0.005206846 0.086408421 0.17058453 ## pole 0.17931681 0.153358210 0.009126707 0.02103035 ## javel -0.22436043 -0.032413502 0.158202825 0.04178638 ## m1500 0.12262390 -0.004048987 0.236058037 -0.07759028 Ants ## Rows: 30 Columns: 8 ## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ## Delimiter: &quot;,&quot; ## chr (3): Location, Habitat, Site ## dbl (5): Month, Nyl, Phe, Tet, Pac ## ## ‚Ñπ Use `spec()` to retrieve the full column specification for this data. ## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message. ants &lt;- read_csv(&quot;pitfalls.csv&quot;) ## choose numeric variables only and transform ants_numeric &lt;- ants %&gt;% dplyr::select(where(is.numeric), -Month) %&gt;% mutate( Nyl = log(Nyl + 1), Phe = log(Phe + 1), Tet = log(Tet + 1), Pac = log(Pac + 1)) corrplot::corrplot(cor(ants_numeric), method = &quot;ellipse&quot;,type = &quot;upper&quot;) pca &lt;- ants_numeric %&gt;% scale() %&gt;% prcomp() summary(pca) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 1.5517 0.8361 0.7611 0.56041 ## Proportion of Variance 0.6019 0.1747 0.1448 0.07852 ## Cumulative Proportion 0.6019 0.7767 0.9215 1.00000 ## screeplot of sdev^2 fviz_screeplot(pca, choice = &quot;eigenvalue&quot;) + geom_hline(yintercept = 1) ## reduced space plot (biplot) fviz_pca_biplot(pca,geom = &quot;point&quot;) + geom_point (alpha = 0.2) Phe and Nyl strongly positively correlated PC2 has a large contribution from Tet Additional information fviz_pca_biplot(pca,geom = &quot;point&quot;,alpha = 0.2) + geom_point(aes(color = ants$Location)) + labs(color = &quot;Location&quot;) Central different from North and West? fviz_pca_biplot(pca,geom = &quot;point&quot;,alpha = 0.2) + geom_point(aes(color = ants$Habitat)) + labs(color = &quot;Habitat&quot;) Grass and Scrub intermediate? Reality check: reducing noise‚Ä¶ set.seed(1234) ## just for reproduciblity noise &lt;- as_tibble(replicate(10,rnorm(200, mean = 50, sd = 10))) ## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0. ## Using compatibility `.name_repair`. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. noise ## # A tibble: 200 √ó 10 ## V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 37.9 54.9 37.7 53.1 39.8 37.9 36.0 29.9 47.5 44.7 ## 2 52.8 57.0 50.4 56.1 36.1 53.0 73.8 34.4 37.8 45.0 ## 3 60.8 51.9 45.8 33.1 49.5 34.6 58.7 69.6 49.3 38.5 ## 4 26.5 57.0 41.0 57.8 68.1 56.4 34.6 48.2 63.6 51.3 ## 5 54.3 53.1 54.2 50.1 49.0 57.0 61.3 63.6 45.4 54.7 ## 6 55.1 57.6 51.5 48.2 57.8 30.9 60.4 66.5 50.7 48.1 ## 7 44.3 68.4 64.6 61.1 39.0 59.4 55.9 51.2 57.8 33.8 ## 8 44.5 61.1 38.8 64.8 47.8 47.8 54.0 50.0 34.1 46.0 ## 9 44.4 50.3 44.8 38.5 55.7 43.3 51.4 52.0 36.3 54.1 ## 10 41.1 38.9 49.3 60.1 46.5 54.5 56.2 56.8 40.7 42.8 ## # ‚Ä¶ with 190 more rows corrplot::corrplot(cor(noise), method = &quot;ellipse&quot;,type = &quot;upper&quot;) pca &lt;- noise %&gt;% scale() %&gt;% prcomp() summary(pca) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 1.1890 1.1331 1.0936 1.0281 1.0075 0.96654 0.94742 ## Proportion of Variance 0.1414 0.1284 0.1196 0.1057 0.1015 0.09342 0.08976 ## Cumulative Proportion 0.1414 0.2698 0.3894 0.4951 0.5966 0.68998 0.77974 ## PC8 PC9 PC10 ## Standard deviation 0.90017 0.85665 0.81141 ## Proportion of Variance 0.08103 0.07339 0.06584 ## Cumulative Proportion 0.86078 0.93416 1.00000 fviz_screeplot(pca, choice = &quot;eigenvalue&quot;) + geom_hline(yintercept = 1) "],["multidimensional-scaling-mds.html", "Multidimensional Scaling (MDS)", " Multidimensional Scaling (MDS) Multidimensional scaling (MDS) is an extended family of techniques that try to reproduce the relative positions of a set of points in a reduced space given, not the points themselves, but only a matrix with interpoint distances ( dissimilarities ) - see section 3 . This sounds easier than it is. These distances might be measured with error, or even be non-Euclidean. With PCA there would be at least one Euclidean configuration of points that would exactly reproduce the dissimilarity matrix - the original data matrix. Indeed there would be an infinity, any rotation or reflection of the original data would leave the interpoint distances unchanged. This configuration might lie in a high dimension hyperspace but at least it would be exact. The problem of approximating it in fewer dimensions is not too difficult. However, if the matrix of dissimilarities is not Euclidean, then there is no guarantee that there is any exact configuration, let alone an adequate approximation. It is this kind of problem the scaling techniques are designed to solve. The origins of many of these techniques are in psychology (psychometrics to be precise) so the extensive literature is littered with terms like stimulus, subject, attribute and preference, which makes reading it rather stressful. Principal Coordinates. Principal coordinates (PCO) is closely related to PC. It finds a configuration of points that optimally reproduces the distance matrix in a Euclidean space of few dimensions. Though it works best on a Euclidean dissimilarity matrix, it will nearly always produce useful results from non- Euclidean ones. Intuitive explanation Assume for the moment that the dissimilarity matrix is Euclidean. We can therefore try to imagine a cloud of points hanging in a space of unknown dimensionality. These points have no coordinates since as yet there are no axes (only distances were given). The problem is to impose a set of axes on the space and then locate the points on them. By a cunning transformation of the dissimilarity matrix, it can be made equivalent to a matrix YY ‚Äò, a cross-product matrix of a set of coordinates Y . An eigenanalysis of this cross-product matrix (like PCA and CA) will give a diagonal matrix of eigenvalues Œõ and a matrix of eigenvectors V , so that YY‚Äô = V Œõ V‚Äô (see chapter 1). The coordinates are given by Y = V Œõ 1/2 , the elements of the scaled eigenvectors (the Œõ 1/2 rescales the eigenvectors to have a sum of squared coefficients equal to the eigenvalue). Like PCA the eigenvectors identify orthogonal axes that run down the major axes of the cloud of data points; though they will not be the old axes rotated, there are no old axes. As in PCA the size of the eigenvalues will give the variation of the data points along the associated axis (eigenvector). It is therefore comparatively easy to select a reduced space to display the relationships among the data points. If the original distance matrix was Pythagorean then the results will be identical to a PCA on thedata matrix from which the distances were calculated. If the dissimilarity matrix is non-Euclidean, then there may be problems with the interpretation of the eigenvalues and in extreme cases the plots themselves, this is discussed in section 5.0. Relationships with PCA. The final stage of PCO, an eigenanalysis of a cross products matrix YY‚Äô , seems very reminiscent of PCA - an eigenanalysis of a crossproducts matrix Y‚ÄôY . If the original dissimilarity matrix actually contains Euclidean (Pythagorean) distances, then PCA and PCO are doing exactly the same thing, reproducing a cloud of points in fewer dimensions on orthogonal axes while optimally preserving their relative positions. It is reassuring that in such a case, a PCA on the covariance matrix will is identical to PCO on the dissimilarity matrix. The major conceptual difference between the two 63techniques is that PCA uses the eigenvectors to project the original data points (vectors) into the reduced space. In PCO the scaled eigenvectors are the vectors of observation scores in reduced space; but their positions in the reduced space and the positions of the corresponding axes will be identical. Metric Scaling. Metric scaling tries to produce a set of coordinates (a configuration of points) in a reduced number of dimensions whose matrix of interpoint Euclidean distances approximates the original dissimilarity matrix as closely as possible. The eigenanalysis technique Principal Coordinates (PCO) does this directly. PCO is a metric scaling technique (it is sometimes called classical or Torgerson scaling ). However, the term metric scaling is more commonly applied when computer intensive iterative algorithms are used to do the job rather than eigenanalysis. The results will seldom be very different from doing a PCO on the same dissimilarity matrix. Intuitive explanation The simplest approach to metric scaling is by repeated approximation, and is always done on a computer. The computer can be imagined as guessing an initial configuration in a high dimension space (less than p -1). It then calculates the distance matrix for these initial points. The elements of this matrix are then regressed against the elements of the given dissimilarity matrix. If by some extraordinary stroke of luck (or fudging) the fit is extremely good then this configuration will do. However, it is far, far, more likely that the fit will not be adequate. The computer then shifts each point in the configuration slightly so that its interpoint distances will fit the given dissimilarity matrix better, calculates the distances, measures the fit and calculates the improvement. If there has been little or no improvement, or the fit is adequate, then a solution has been found and the process stops. If not, then it goes through more iterations till it finds the best fitting set of coordinates (or gives up in disgust). This process is repeated for spaces of fewer dimensions, and the goodness of fit plotted against the number of dimensions (like a scree graph - section 5.3). The appropriate number of dimensions for the reduced space is chosen, and the corresponding configuration plotted and (hopefully) interpreted. Notice it does not calculate one high dimensional solution and extract all lower dimensional solutions from it as PCA or PCO does; the configuration for each reduced space is calculated anew each time. In fact, if the dissimilarities are exactly Euclidean, then a p -1 dimension solution, if subjected to a PCA, will give similar lower dimensional configurations to separate scalings for each solution. In other words a metric scaling on a Euclidean distance matrix will give the same results as a PCA. Non-metric scaling. Under certain circumstances trying to preserve the actual dissimilarities might be too restrictive or even pointless. For example if there is large error in the dissimilarity estimates, if the dissimilarities or the data they were based on were ranks (ordinal), then the magnitude of the distances are too crude to be worth preserving. A method that preserved only the rank order of the dissimilarities would be more appropriate. The algorithm to do this is virtually the same as the one given above for metric scaling.. The sole difference is that the linear regression that fitted the estimated distances for the solution to the dissimilarities is now replaced with an order preserving regression - Kruskal‚Äôs least squares monotonic transformation (Kruskal 1964), sometimes known as optimal scaling. Which to use: metric or non-metric? Both metric and non-metric methods have their strengths. Non-metric methods can handle ordinal data or other lower quality dissimilarities, and are robust to outliers. On the other hand they are more prone to local minima and degenerate solutions. As Gower (1987) points out: when the number of sampling units is large, preserving the rank order is usually essentially the same as preserving distances; in which case it doesn‚Äôt much matter which is used. A metric scaling will always have a higher stress than the corresponding global non-metric solution, but will often be more accurate. Sometimes, with non-Euclidean distances, the relationship between the fitted Euclidean distances and the dissimilarities is non-linear. In which case a linear metric scaling may 73not be adequate and a non-metric method will usually be appropriate. Such a situation can be recognised from the shape of the scatter diagram of the fitted distances against the dissimilarities. Examples in R Consider data which are not represented as points in a feature space: Where we are only provided with (dis)similarity matrices between objects (e.g., chemical compounds, images, trees, or other complex objects) Where there are no obvious coordinates in (continuous) n-dimensional space . Distances (in km) between North Island cities File north_island_distances.csv can be found on CANVAS ## New names: ## * `` -&gt; ...1 ## Rows: 11 Columns: 12 ## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ## Delimiter: &quot;,&quot; ## chr (1): ...1 ## dbl (11): Auckland, Gisborne, Hamilton, Hastings, Napier, Rotorua, Tauranga,... ## ## ‚Ñπ Use `spec()` to retrieve the full column specification for this data. ## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message. library(tidyverse) ni &lt;- read_csv(&quot;north_islands_distances.csv&quot;)[1,-] library(pheatmap) pheatmap(ni, cluster_rows = TRUE, treeheight_row = 2, treeheight_col = 2, fontsize_row = 12, fontsize_col = 12, cellwidth = 26, cellheight = 26) mds &lt;- cmdscale(ni, eig = TRUE) mds ## $points ## [,1] [,2] ## [1,] 259.23245 67.43013 ## [2,] -107.54173 -285.70950 ## [3,] 129.07943 42.71295 ## [4,] -173.12950 -25.15974 ## [5,] -150.83765 -34.70680 ## [6,] 37.39858 -18.39760 ## [7,] 118.78535 -85.88683 ## [8,] -192.73988 181.50600 ## [9,] -385.83172 167.76477 ## [10,] 49.93256 -140.17112 ## [11,] 415.65212 130.61774 ## ## $eig ## [1] 5.249373e+05 1.953521e+05 4.217767e+04 1.872276e+04 1.222717e+03 ## [6] 2.910383e-11 -1.399691e+02 -4.733140e+02 -1.103819e+04 -1.883151e+04 ## [11] -2.462990e+04 ## ## $x ## NULL ## ## $ac ## [1] 0 ## ## $GOF ## [1] 0.8600209 0.9206005 Eckmans colour perception (1954) Data may from objects for which we have similarities but no underlying (geometric) space. Here the goal is to understand the underlying dimensionality of colour perception. Similarities for 14 colours, with wavelengths from 434 to 674nm based on rating by 31 subjects Each pair of colours was rated on a 5-point scale: 0 = no similarity up to 4 = identical. After averaging over 31 raters the similarities were divided by 4 such that they are within the unit interval. Data available on CANVAS ## Rows: 14 Columns: 14 ## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ## Delimiter: &quot;,&quot; ## dbl (14): w4, 34.w4, 45.w4, 65.w4, 72.w4, 90.w5, 04.w5, 37.w5, 55.w5, 84.w60... ## ## ‚Ñπ Use `spec()` to retrieve the full column specification for this data. ## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## w4 34.w4 45.w4 65.w4 72.w4 ## 1 0.00 0.14 0.58 0.58 0.82 ## 2 0.14 0.00 0.50 0.56 0.78 ## 3 0.58 0.50 0.00 0.19 0.53 ## 4 0.58 0.56 0.19 0.00 0.46 ## 5 0.82 0.78 0.53 0.46 0.00 library(tidyverse) ekman &lt;- read_csv(&quot;ekman.csv&quot;) ekman.mds &lt;- cmdscale(ekman, eig = TRUE) ekman.mds round(ekman.mds$eig,2) autoplot(ekman.mds) Distances (in km) between 21 cities in Europe library(ggfortify) ## Plotting Multidimensional Scaling (for interest) ## stats::cmdscale performs Classical MDS data(&quot;eurodist&quot;) ## road distances (in km) between 21 cities in Europe. autoplot(eurodist) ## Plotting Classical (Metric) Multidimensional Scaling autoplot(cmdscale(eurodist, eig = TRUE)) autoplot(cmdscale(eurodist, eig = TRUE), label = TRUE, shape = FALSE, label.size = 3) ## Plotting Non-metric Multidimensional Scaling ## MASS::isoMDS and MASS::sammon perform Non-metric MDS library(MASS) autoplot(sammon(eurodist)) autoplot(sammon(eurodist), shape = FALSE, label = TRUE,label.size = 3) ## Have a go at interpreting these plots based on the geography of the cities :-) "],["non-metric-multidimensional-scaling.html", "Non-metric Multidimensional Scaling", " Non-metric Multidimensional Scaling Multidimensional scaling aims to minimize the difference between the squared distances \\(D^2\\) from the distance matrix \\(D\\), and the squared distances between the points with their new coordinates. Unfortunately, this objective tends to be sensitive to outliers: one single data point with large distances to everyone else can dominate, and thus skew, the whole analysis. So how should we seek a more robust criterion? disregard the actual values of the distances require only the relative rankings of the original and the new distances are as similar as possible. Such a rank based approach is robust: its sensitivity to outliers is reduced! Robust ordination, or non metric multidimensional scaling (NMDS), attempts to embed the points in a new space such that the order of the reconstructed distances in the new map is the same as the ordering of the original distance matrix. NMDS looks for a transformation f() of the given dissimilarities, distances d.¬†The quality of the approximation can be measured by the standardized residual sum of squares (STRESS) function: \\(\\text{Stress}^2 = \\frac{\\Sigma(f(d) - \\tilde{d})^2}{\\Sigma d^2}\\) where \\(f(d)\\approx \\tilde{d}\\). NMDS is not sequential: we have to specify the underlying dimensionality k at the outset (like kmeans) optimization is run to maximize the reconstruction of the distances in k dimensions. there is no notion of percentage of variation explained by individual axes as provided in PCA. as for kmeans Make a screeplot for \\(k = 1,2,3,...\\) and looking at how well the STRESS drops. because each calculation of a NMDS result requires a new optimization that is both random and dependent on the value of k, we repeat the process M times Examples in R Use the function metaMDS from the vegan package; metaMDS performs NMDS, and tries to find a stable solution using several random starts. In addition, it standardizes the scaling in the result, so that the configurations are easier to interpret. Illustration with k = 2 library(vegan) nMDS.2 &lt;- replicate(100, metaMDS(ekman, k = 2, autotransform = FALSE)) stressplot(metaMDS(ekman, k = 2, autotransform = FALSE), pch = 20, cex = 2) ## Run 0 stress 0.02310251 ## Run 1 stress 0.02310251 ## ... New best solution ## ... Procrustes: rmse 1.613474e-06 max resid 2.653004e-06 ## ... Similar to previous best ## Run 2 stress 0.02310251 ## ... Procrustes: rmse 3.270629e-06 max resid 6.636766e-06 ## ... Similar to previous best ## Run 3 stress 0.02310251 ## ... Procrustes: rmse 1.828142e-06 max resid 3.125328e-06 ## ... Similar to previous best ## Run 4 stress 0.02310251 ## ... Procrustes: rmse 1.083607e-06 max resid 2.104326e-06 ## ... Similar to previous best ## Run 5 stress 0.02310251 ## ... Procrustes: rmse 1.330889e-06 max resid 2.522211e-06 ## ... Similar to previous best ## Run 6 stress 0.02310251 ## ... Procrustes: rmse 2.661504e-06 max resid 4.233961e-06 ## ... Similar to previous best ## Run 7 stress 0.02310251 ## ... Procrustes: rmse 5.070932e-07 max resid 8.269046e-07 ## ... Similar to previous best ## Run 8 stress 0.02310251 ## ... Procrustes: rmse 2.14105e-06 max resid 3.850841e-06 ## ... Similar to previous best ## Run 9 stress 0.02310251 ## ... Procrustes: rmse 4.179598e-06 max resid 6.733372e-06 ## ... Similar to previous best ## Run 10 stress 0.02310251 ## ... Procrustes: rmse 3.010374e-06 max resid 5.431574e-06 ## ... Similar to previous best ## Run 11 stress 0.02310251 ## ... Procrustes: rmse 1.046163e-06 max resid 1.580493e-06 ## ... Similar to previous best ## Run 12 stress 0.02310251 ## ... Procrustes: rmse 6.604759e-06 max resid 1.111471e-05 ## ... Similar to previous best ## Run 13 stress 0.02310251 ## ... Procrustes: rmse 2.046216e-06 max resid 3.303987e-06 ## ... Similar to previous best ## Run 14 stress 0.02310251 ## ... Procrustes: rmse 1.756297e-06 max resid 2.511059e-06 ## ... Similar to previous best ## Run 15 stress 0.02310251 ## ... Procrustes: rmse 7.030217e-07 max resid 1.314074e-06 ## ... Similar to previous best ## Run 16 stress 0.02310251 ## ... Procrustes: rmse 3.897324e-06 max resid 6.532695e-06 ## ... Similar to previous best ## Run 17 stress 0.02310251 ## ... Procrustes: rmse 1.444798e-06 max resid 2.431135e-06 ## ... Similar to previous best ## Run 18 stress 0.02310251 ## ... New best solution ## ... Procrustes: rmse 6.231313e-07 max resid 1.076696e-06 ## ... Similar to previous best ## Run 19 stress 0.02310251 ## ... Procrustes: rmse 3.876489e-06 max resid 6.507571e-06 ## ... Similar to previous best ## Run 20 stress 0.02310251 ## ... Procrustes: rmse 1.774289e-06 max resid 3.188385e-06 ## ... Similar to previous best ## *** Solution reached "],["correspondence-analysis-ca.html", "Correspondence Analysis (CA)", " Correspondence Analysis (CA) CA is a special case of metric MDS where the distance measure is the chi-square distance. It is conceptually similar to principal component analysis but where the data are categorical, counts, rather than continuous. CA is traditionally applied to contingency tables where rows and columns are treated equivalently; it decomposes the chi-square statistic associated with this table into orthogonal factors. Correspondence analysis is usually the best way to follow up on a significant chi-square test. HairEyeColor ## , , Sex = Male ## ## Eye ## Hair Brown Blue Hazel Green ## Black 32 11 10 3 ## Brown 53 50 25 15 ## Red 10 10 7 7 ## Blond 3 30 5 8 ## ## , , Sex = Female ## ## Eye ## Hair Brown Blue Hazel Green ## Black 36 9 5 2 ## Brown 66 34 29 14 ## Red 16 7 7 7 ## Blond 4 64 5 8 HC.df &lt;- as.data.frame.matrix(HairEyeColor[ , , 2]) HC.df ## Brown Blue Hazel Green ## Black 36 9 5 2 ## Brown 66 34 29 14 ## Red 16 7 7 7 ## Blond 4 64 5 8 chisq.test(HC.df) ## Warning in chisq.test(HC.df): Chi-squared approximation may be incorrect ## ## Pearson&#39;s Chi-squared test ## ## data: HC.df ## X-squared = 106.66, df = 9, p-value &lt; 2.2e-16 library(ade4) coaHC &lt;- dudi.coa(HC.df, scannf = FALSE, nf = 2) The first axis shows a contrast between black haired and blonde haired students, mirrored by the brown eye, blue eye contrast. In CA the two categories, rows and columns play symmetric roles and we interpret the proximity of Blue eyes and Blond hair as showing strong co-occurence of these categories. Biplot barycentric scaling Row points at the centre of gravity of the column levels with their respective weights Blue eyes at centre of gravity of the (Black, Brown, Red, Blond) with weights proportional to (9,34,7,64), the hair counts for blue eyes. The Blond row point is very heavily weighted so Blond hair and Blue eyes close together "],["mds-summary.html", "MDS summary", " MDS summary Multivariate data Distance methods are useful when data consist of associations (similarities/distances) among observations. There are many measures of distance, and their choice, much like transformations, is important in the outcome of the analysis. Many multivariate methods are just special cases of one another, with special names to match. If data are, approximately, continuous measurements we can use PCA to produce a lower dimensional representation. For general data we construct a matrix of distances between sample points and use the distance matrix to construct a geometric representation; distances between geometric points approximate distances from distance matrix. For count data Correspondence Analysis (CA) widely used. PCO and metric MDS will usually provide almost identical answers if Kruskal‚Äôs STRESS is used Metric or non-metric? + Metric has few advantages over Principal Coordinates Analysis (unless many negative eigenvalues) + Non-metric does better with fewer dimensions but can be more prone to sub-optimal solutions. How many dimensions? + STRESS &lt;10% is ‚Äúgood representation‚Äù + Scree diagram "],["linear-discriminant-analysis-lda.html", "Linear Discriminant Analysis (LDA)", " Linear Discriminant Analysis (LDA) LDA is a supervised learning technique: The main goal is to predict some feature of interest using sing one or more variables (the predictors) Example in R ## Rows: 144 Columns: 7 ## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ## Delimiter: &quot;,&quot; ## dbl (7): id, relwt, glufast, glutest, steady, insulin, group ## ## ‚Ñπ Use `spec()` to retrieve the full column specification for this data. ## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message. library(tidyverse) diabetes &lt;- read_csv(&quot;diabetes.csv&quot;) The data diabetes$group &lt;- factor(diabetes$group) diabetes ## # A tibble: 144 √ó 7 ## id relwt glufast glutest steady insulin group ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1 0.81 80 356 124 55 3 ## 2 3 0.94 105 319 143 105 3 ## 3 5 1 90 323 240 143 3 ## 4 7 0.91 100 350 221 119 3 ## 5 9 0.99 97 379 142 98 3 ## 6 11 0.9 91 353 221 53 3 ## 7 13 0.96 78 290 136 142 3 ## 8 15 0.74 86 312 208 68 3 ## 9 17 1.1 90 364 152 76 3 ## 10 19 0.83 85 296 116 60 3 ## # ‚Ä¶ with 134 more rows Some variables can predict group of a patient ggplot(reshape2::melt(diabetes, id.vars = c(&quot;id&quot;, &quot;group&quot;)), aes(x = value, col = group)) + geom_density() + facet_wrap( ~variable, ncol = 1, scales = &quot;free&quot;) + theme(legend.position = &quot;bottom&quot;) Possible classification rules? ggplot(diabetes, mapping = aes(x = insulin, y = glutest)) + theme_bw() + geom_point(aes(colour = group), size = 3) + labs( x = &quot;insulin&quot; , y = &quot;glutest&quot;) + theme(axis.title = element_text( size = 16), axis.text = element_text(size = 12)) Carrying out LDA Some similarity to regression library(MASS) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:patchwork&#39;: ## ## area ## The following object is masked from &#39;package:dplyr&#39;: ## ## select diabetes_lda &lt;- lda(group ~ insulin + glutest, data = diabetes) diabetes_lda ## Call: ## lda(group ~ insulin + glutest, data = diabetes) ## ## Prior probabilities of groups: ## 1 2 3 ## 0.2222222 0.2500000 0.5277778 ## ## Group means: ## insulin glutest ## 1 320.9375 1027.3750 ## 2 208.9722 493.9444 ## 3 114.0000 349.9737 ## ## Coefficients of linear discriminants: ## LD1 LD2 ## insulin -0.004463900 -0.01591192 ## glutest -0.005784238 0.00480830 ## ## Proportion of trace: ## LD1 LD2 ## 0.9677 0.0323 Components of diabetes_lda diabetes_lda$prior gives the prior probabilities of belonging to each group. By default these reflect the proportions of membership in the data: prop.table(table(diabetes$group)) ## ## 1 2 3 ## 0.2222222 0.2500000 0.5277778 ‚Äì&gt; randomly chosen subject has probability 0.52 of coming from group 3 diabetes_lda$mean gives the means of each predictor in each group: Proportion of Trace gives the percentage separation achieved by each discriminant function diabetes_lda$scaling contains the linear discriminant functions (i.e., the linear combination of variables giving best separation between groups): diabetes_lda$scaling ## LD1 LD2 ## insulin -0.004463900 -0.01591192 ## glutest -0.005784238 0.00480830 i.e., LD1: \\(-0.00446 \\times \\text{insulin} - 0.00578 \\times \\text{glutest}\\) LD2: \\(-0.01591 \\times \\text{insulin} + 0.00481 \\times \\text{glutest}\\) How well does LDA do on training data? ghat &lt;- predict(diabetes_lda)$class table(prediced = ghat, observed = diabetes$group) ## observed ## prediced 1 2 3 ## 1 25 0 0 ## 2 6 24 6 ## 3 1 12 70 The missclassification rate is therefore mean(ghat != diabetes$group) ## [1] 0.1736111 Prediction diabetes.pred &lt;- predict(diabetes_lda) str(diabetes.pred) ## List of 3 ## $ class : Factor w/ 3 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 3 3 3 3 3 3 3 3 3 3 ... ## $ posterior: num [1:144, 1:3] 1.04e-06 1.06e-06 2.47e-06 3.26e-06 4.77e-06 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:144] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. ..$ : chr [1:3] &quot;1&quot; &quot;2&quot; &quot;3&quot; ## $ x : num [1:144, 1:2] 1.62 1.61 1.42 1.37 1.29 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:144] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. ..$ : chr [1:2] &quot;LD1&quot; &quot;LD2&quot; $class: predicted group for each observation $posterior: probability of falling into each group $x: matrix with 2 columns one for each LD score Output Every possible point is classified to one of three groups The divisions between groups are linear (Linear Discriminant Analysis) The three ellipses represent the class centres and the covariance matrix of the LDA model. Note there is only one covariance matrix, which is the same for all three classes. This results in the sizes and orientations of the ellipses being the same for the three classes (only their centres differ) the ellipses represent contours of equal class membership probability. A key assumption of LDA is that the correlations between variables are the same in each group (i.e., common covariance matrix). Recall that, by default, the prior probabilities are the initial proportions. What if we set equal prior probabilities? The confusion matrix/missclassification rate: equal.ghat &lt;- predict(diabetes_lda, prior = rep(1,3)/3)$class table(predicted = equal.ghat,observed = diabetes$group) ## observed ## predicted 1 2 3 ## 1 25 0 0 ## 2 7 28 9 ## 3 0 8 67 ## missclassification rate mean(equal.ghat != diabetes$group) ## [1] 0.1666667 There are now 8 cases classified as Group 3 with prior weights classified as Group 2 with equal weights \\(\\rightarrow\\) bias towards group with larger initial size. "]]
