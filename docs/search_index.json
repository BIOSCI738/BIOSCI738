[["index.html", "bluebook BIOSCI738 Preface 0.1 Course Overview 0.2 Key Topics", " bluebook BIOSCI738 Charlotte Jones-Todd Semester 1, 2021, University of Auckland Preface Artwork by @allison_horst 0.1 Course Overview This is a postgraduate course in statistical methods geared towards the needs of students of biology, ecology, and environmental science. Whether heading to research or industry, it is imperative that biology students have the statistical and computational skills to apply and interpret fundamental statistical concepts and analyses to assess and critique their experiments and other data. This course is suited to students with an interest in (bio)statistics who would like to equip themselves with the tools and know-how to be able to correctly prepare experiments, analyse data, interpret their results and draw valid conclusions. The statistical concepts and methods taught in this course will provide students with the tools to make and evaluate scientific discoveries as well as propose and justify decisions based on data. Some basic statistical knowledg is assumed. This course will use the programming language R; R is a free software environment for statistical computing and graphics. Students are strongly encouraged to use R through the freely available IDE (integrated development environment) RStudio. Students will have access to R and RStudio in university computing laboratories, but are also encouraged to download and install R and RStudio on their own devices. 0.2 Key Topics Taught material will be delivered, each week, via 2 hour lectures. Each week there will also be a 1 hour practical workshop focused on the material covered in the lecture. A list of topics and concepts covert in this course is given below. Exploratory Data Analysis and Communication Data wrangling Data visualisation Experimental Design and Statistical Inference Introduction to design and analysis of experiments Comparison procedures: pairwise comparisons of means, one-way ANOVA Multiple comparison procedures (controlling errors in hypothesis testing) Multiple regression with continuous and categorical explanatory variables Mixed models; incorporating fixed and random effects Resampling procedures: randomisation, permutation, and bootstrapping Multivariate Analysis Cluster analysis Unsupervised learning: principal components analysis, dimension reduction Ordination: multidimensional scaling, correspondence analysis Supervised learning: discriminant analysis Networks and graphs "],["r-rstudio-and-git.html", "1 R, RStudio, and git 1.1 Learning objectives 1.2 Intro to R &amp; RStudio 1.3 Getting started 1.4 Reproducible research 1.5 Version control with git and GitHub 1.6 Exploratory Data Analysis 1.7 Other resources", " 1 R, RStudio, and git 1.1 Learning objectives Define the difference between R and RStudio Explain what an R function is; describe what an argument to an R function is Explain what an R package is; distinguish between the functions install.packages() and library() Use the appropriate R function to read in a data file Explain the importance of reproducibility in terms of scientific research Use the functionality offered by git and GitHub through RStudio 1.2 Intro to R &amp; RStudio R is the pheromone to RStudio‚Äôs PDA R is the pheromone to RStudio‚Äôs PDA. R is a language, specifically, a programming language; it‚Äôs the way you can speak to your computer to ask it to carry out certain computations. RStudio is an integrated development environment (IDE). This means it is basically an interface, albeit a fancy one, that makes it easier to communicate with your computer in the language R. The main benefit is the additional features it has that enable you to more efficiently speak R. Note R and RStudio are two different pieces of software; for this course you are expected to download both. As you‚Äôd expect, the PDA depends on the pheromones (i.e., RStudio depends on R) so you have to download R to use RStudio! 1.2.1 Why? R It‚Äôs free It‚Äôs open source A general-purpose of programming language Written by statisticians (here in Auckland!) It‚Äôs available for all operating systems (Windows, Linux, and Mac) There is a huge online support network It‚Äôs extremely flexible; if you can code it you can do it! 15,000+ packages available! ‚Ä¶ RStudio ‚ÄúIf R were an airplane, RStudio would be the airport‚Ä¶‚Äù ‚Äî Julie Lowndes, Introduction to RStudio Awesomeness Speaks nicely to R Tab completion Debugging capabilities There is a huge online support network Offers many other features and tools to make your workflow with R easier It facilitates reproducibility ‚Ä¶ 1.2.2 Installing R and RStudio As mentioned above RStudio depends on R so there is an order you should follow when you download these software. Download and install R by following these instructions. Make sure you choose the correct operating system. Download and install RStudio by going here choosing RStudio Desktop Open Source License Free and following instructions. Check all is working Open up RStudio from your computer menu, the icon will look something like this (DO NOT use this icon , this is a link to R and will only open a very basic interface) Wait a little and you should see RStudio open up to something similar to the screenshot below Pay close attention to the notes in the screenshot and familiarise yourself with the terms. Finally, in the Console next to the prompt type 1:10 and press enter on your keyboard. Your computer should say something back you (in the Console)! What do you think you were asking it to do? Does the output make sense?1 1.3 Getting started As in step 3. above open up RStudio from your computer menu, the icon will look something like this . Using the diagram above identify the different panes: Console where you directly type command in and communicate with your computer (via the language R). Environment pane Files pane Some terminology Running code: the act of telling R to perform an act by giving it commands in the console. Objects: where values are saved in (see later for creating an object. Script: a text file containing a set of commands and comments. Comments: notes written within a Script to better document/explain what‚Äôs happening 1.3.1 R errors üò± data &lt;- read.csv(&quot;data_file_not_in_my_working_directory.csv&quot;) ## Warning in file(file, &quot;rt&quot;): cannot open file ## &#39;data_file_not_in_my_working_directory.csv&#39;: No such file or directory ## Error in file(file, &quot;rt&quot;): cannot open the connection library(some_library_I_have_not_installed) ## Error in library(some_library_I_have_not_installed): there is no package called &#39;some_library_I_have_not_installed&#39; some_function_I_spelled_worng(x = x) ## Error in some_function_I_spelled_worng(x = x): could not find function &quot;some_function_I_spelled_worng&quot; an_object_I_have_not_created ## Error in eval(expr, envir, enclos): object &#39;an_object_I_have_not_created&#39; not found What do you think the issues are here üòâ 1.3.2 R Scripts (a .r file) Go File &gt; New File &gt; R Script to open up a new Script If you had only three panes showing before, a new (fourth) pane should open up in the top left of RStudio. This file will have a .r extension and is where you can write, edit, and save the R commands you write. It‚Äôs a dedicated text editor for your R code (very useful if you want to save your code to run at a later date). The main difference between typing your code into a Script vs Console is that you edit it and save it for later! Remember though the Console is the pane where you communicate with your computer so all code you write will have to be Run here. There are two ways of running a line of code you‚Äôve written in your Script Ensure your cursor is on the line of code you want to run, hold down Ctrl and press Enter. Ensure your cursor is on the line of code you want to run, then use your mouse to click the Run button (it has a green arrow next to it) on the top right of the Script pane. Type 1:10 in your Script and practise running this line of code using both methods above. Not that if you‚Äôve Run the code successfully then your computer will speak back to you each time via the Console 1.3.3 Writing Comments Comments are notes to yourself (future or present) or to someone else and are, typically, written interspersed in your code. Now, the comments you write will typically be in a language your computer doesn‚Äôt understand (e.g., English). So that you can write yourself notes in your Script you need to tell your computer using the R language to ignore them. To do this precede any note you write with #, see below. The # is R for ignore anything after this character. ## IGNORE ME ## I&#39;m a comment ## I repeat I&#39;m a comment ## I am not a cat ## OK let&#39;s run some code 2 + 2 ## [1] 4 ## Hmm maybe I should check this ## @kareem_carr ;-) Now remember when you want to leave your R session you‚Äôll need to Save your Script to use it again. To do this go File &gt; Save As and name your file what you wish (remember too to choose a relevant folder on your computer, or as recommended use the .Rproj set-up as above). 1.3.4 Change the RStudio appearance up to your taste Go to Tools &gt; Global Options &gt; Apperance 1.4 Reproducible research Keep all similar files for the same analysis in the same place NEVER change raw data 1.4.1 Good practice Always start with a clean workspace Why? So your ex (code) can‚Äôt come and mess up your life! Go to Tools &gt; Global Options Project-oriented workflow. Recommended: .Rproj Organised Set up each Each assignment/university course as a project Self-contained a project is a folder that contains all relevant files All paths can then be relative to that project Reproducible the project should just work on a different computer Got to Project (top right) &gt; New Project &gt; Create Project Project set-up ‚ö†Ô∏èWarning‚ö†Ô∏è Jenny Bryan will set your computer on fire üî• if you start your script like this rm(list = ls()) This does NOT create a fresh R process it makes your script vulnerable it will come back to bite you 1.5 Version control with git and GitHub All workshops will use these tools git the software ‚ÄúTrack Changes features from Microsoft Word on steroids‚Äù ‚Äî Jenny Bryan a version control system manages the evolution of a set of files (tidily) GitHub an online hosting service ‚ÄúThink of it as DropBox but much, much better‚Äù ‚Äî Jenny Bryan home for your Git-based projects on the internet 1.5.1 Setup TL;DR Register an account with GitHub https://github.com Make sure you‚Äôve got the latest version of R R.version.string ## [1] &quot;R version 4.0.4 (2021-02-15)&quot; Upgrade RStudio to the new preview version (optional) Install git: follow these instructions Get started 1.5.2 Cloning a repository from GitHub using RStudio On GitHub, navigate to the Code tab of the repository On the right side of the screen, click Clone or download Click the Copy to clipboard icon to the right of the repository URL (e.g., https://github.com/STATS-UOA/workshops-biosci738.git) Open RStudio in your local environment Click File, New Project, Version Control, Git Paste the repository URL and enter TAB to move to the Project directory name field. I‚Äôve chosen to store this folder on my Desktop, obviously put it wherever you wish :-) Click Create Project. Your Files pane should now look similar to this 1.6 Exploratory Data Analysis or EDA we will be using tidyverse. ‚Äòtidyverse‚Äô is a collection of R packages that all share underlying design philosophy, grammar, and data structures. They are specifically designed to make data wrangling, manipulation, visualisation, and analysis simpler. 1.6.1 Starting out with tidyverse Artwork by [@allison_horst](https://github.com/allisonhorst/) Starting out with tidyverse To install all the packages that belong to the tidyverse run ## request (download) the tidyverse packages from the centralised library install.packages(&quot;tidyverse&quot;) To tell your computer to access the tidyverse functionality in your session run (Note you‚Äôll have to do this each time you start up an R session): ## Get the tidyverse packages from our local library library(tidyverse) 1.6.2 Reading in data from a .csv file First off download the paua.csv file from CANVAS To read the data into RStudio In the Environment pane click Import Dataset &gt; ** From Text (readr)** &gt; Browse &gt; Choose your file, remembering which folder you downloaded it to. this is where .Rproj is useful &gt; Another pane should pop up, check the data looks as you might expect &gt; Import Or paua &lt;- read_csv(&quot;paua.csv&quot;) 1.6.3 Explore your data Let‚Äôs have a look at your data in the Console paua ## # A tibble: 60 x 3 ## Species Length Age ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Haliotis iris 1.8 1.50 ## 2 Haliotis australis 5.4 11.9 ## 3 Haliotis australis 4.8 5.42 ## 4 Haliotis iris 5.75 4.50 ## 5 Haliotis iris 5.65 5.50 ## 6 Haliotis iris 2.8 2.50 ## 7 Haliotis australis 5.9 6.49 ## 8 Haliotis iris 3.75 5.00 ## 9 Haliotis australis 7.2 8.56 ## 10 Haliotis iris 4.25 5.50 ## # ‚Ä¶ with 50 more rows ###Explore your data Using the glimpse() command for an alternative view glimpse(paua) ## Rows: 60 ## Columns: 3 ## $ Species &lt;chr&gt; &quot;Haliotis iris&quot;, &quot;Haliotis australis&quot;, &quot;Haliotis australis&quot;, &quot;‚Ä¶ ## $ Length &lt;dbl&gt; 1.80, 5.40, 4.80, 5.75, 5.65, 2.80, 5.90, 3.75, 7.20, 4.25, 6.‚Ä¶ ## $ Age &lt;dbl&gt; 1.497884, 11.877010, 5.416991, 4.497799, 5.500789, 2.500972, 6‚Ä¶ 1.6.4 The pipe operator %&gt;% A nifty tidyverse tool is called the pipe operator %&gt;%. The pipe operator allows us to combine multiple operations in R into a single sequential chain of actions. Say you would like to perform a hypothetical sequence of operations on a hypothetical data frame x using hypothetical functions f(), g(), and h(): Take x then Use x as an input to a function f() then Use the output of this as an input to a function g() then Use the output of this as an input to a function h() So to calculate the mean Age of each Species in the paua dataset we would use paua %&gt;% group_by(Species) %&gt;% summarize(mean_age = mean(Age)) ## # A tibble: 2 x 2 ## Species mean_age ## &lt;chr&gt; &lt;dbl&gt; ## 1 Haliotis australis 7.55 ## 2 Haliotis iris 4.40 You would read the sequence above as: Take the paua data.frame then Use this and apply the group_by() function to group by Species Use this output and apply the summarize() function to calculate the mean Age of each group (Species), calling the resulting number mean_age Or to describe my daily routine‚Ä¶ I %&gt;% wake_up(time = &quot;later than I should&quot;) %&gt;% give(who = &quot;Watson&quot; , what = &quot;medication&quot;) %&gt;% make(who= &quot;myself&quot;, what = &quot;coffee&quot;) %&gt;% drink() %&gt;% try(remember_what_I_have_on(date = &quot;today&quot;)) Have a go at writing your own! 1.7 Other resources R for Data Science RStudio Education An Introduction to R R for Biologists The Popularity of Data Science Software Happy Git and GitHub for the useR Artwork by @allison_horst You should have seen the numbers 1 to 10 printed out as a sequence.‚Ü© "],["mƒÅori-data-sovereignty-principles.html", "2 MƒÅori Data Sovereignty principles 2.1 Learning objectives 2.2 MƒÅori Data Sovereignty principles 2.3 MƒÅori Data Sovereignty principles 2.4 Resources", " 2 MƒÅori Data Sovereignty principles ‚Äô‚ÄòData sovereignty is the idea that data are subject to the laws and governance structures within the nation it is collected‚Äô‚Äô ‚ÄúMƒÅori Data Sovereignty has emerged as a critical policy issue as Aotearoa New Zealand develops world-leading linked administrative data resources.‚Äù ‚Äî Andrew Sporle, Maui Hudson, Kiri West. Chapter 5, Indigenous Data Sovereignty and Policy Te Tiriti o Waitangi/Treaty of Waitangi obliges the Government to actively protect taonga, consult with MƒÅori in respect of taonga, give effect to the principle of partnership and recognize MƒÅori rangatiratanga over taonga. Factors that relate to how communities might recognize the taonga nature of any dataset include provenance of the data: does the dataset come from a significant MƒÅori source? opportunity for the data: could the dataset support MƒÅori aspirations for their people or their whenua (land)? utility of the data: does the dataset have multiple uses? 2.1 Learning objectives Define data sovereignty and explain this in relation to a researcher‚Äôs obligation when collecting, displaying, and analysing data Define and discuss MƒÅori Data Sovereignty principles 2.2 MƒÅori Data Sovereignty principles ‚ÄúFor Indigenous peoples, historical encounters with statistics have been fraught, and none more so than when involving official data produced as part of colonial attempts at statecraft.‚Äù ‚Äî Lovett, R., Lee, V., Kukutai, T., Cormack, D., Rainie, S.C. and Walker, J., 2019. Good data practices for Indigenous data sovereignty and governance. Good data, pp.26-36. ‚ÄúMƒÅori Data Sovereignty has emerged as a critical policy issue as Aotearoa New Zealand develops world-leading linked administrative data resources.‚Äù ‚Äî Andrew Sporle, Maui Hudson, Kiri West. Chapter 5, Indigenous Data Sovereignty and Policy 2.3 MƒÅori Data Sovereignty principles ‚ÄúMƒÅori data refers to data produced by MƒÅori or that is about MƒÅori and the environments we have relationships with.‚Äù ‚Äî Te Mana Raraunga Charter Data is a ‚Äúpotential taonga, something precious that needs to be maintained, in relation to its utility‚Äù ‚Äî Dr W. Edwards, TMR website MƒÅori Data Sovereignty principles to inform the recognition of MƒÅori rights and interests in data, and the ethical use of data to enhance MƒÅori well-being: Rangatiratanga, authority MƒÅori have an inherent right to exercise control over MƒÅori data and MƒÅori data ecosystems. This right includes, but is not limited to, the creation, collection, access, analysis, interpretation, management, security, dissemination, use and reuse of MƒÅori data. Decisions about the physical and virtual storage of MƒÅori data shall enhance control for current and future generations. Whenever possible, MƒÅori data shall be stored in Aotearoa New Zealand. MƒÅori have the right to data that is relevant and empowers sustainable self-determination and effective self-governance Whakapapa, relationships All data has a whakapapa (genealogy). Accurate metadata should, at minimum, provide information about the provenance of the data, the purpose(s) for its collection, the context of its collection, and the parties involved. The ability to disaggregate MƒÅori data increases its relevance for MƒÅori communities and iwi. MƒÅori data shall be collected and coded using categories that prioritise MƒÅori needs and aspirations. Current decision-making over data can have long-term consequences, good and bad, for future generations of MƒÅori. A key goal of MƒÅori data governance should be to protect against future harm. Whanaungatanga, obligations Individuals‚Äô rights (including privacy rights), risks and benefits in relation to data need to be balanced with those of the groups of which they are a part. In some contexts, collective MƒÅori rights will prevail over those of individuals. Individuals and organisations responsible for the creation, collection, analysis, management, access, security or dissemination of MƒÅori data are accountable to the communities, groups and individuals from whom the data derive Kotahitanga, collective benefit Data ecosystems shall be designed and function in ways that enable MƒÅori to derive individual and collective benefit. Build capacity. MƒÅori Data Sovereignty requires the development of a MƒÅori workforce to enable the creation, collection, management, security, governance and application of data. Connections between MƒÅori and other Indigenous peoples shall be supported to enable the sharing of strategies, resources and ideas in relation to data, and the attainment of common goals. Manaakitanga, reciprocity The collection, use and interpretation of data shall uphold the dignity of MƒÅori communities, groups and individuals. Data analysis that stigmatises or blames MƒÅori can result in collective and individual harm and should be actively avoided. Free, prior and informed consent shall underpin the collection and use of all data from or about MƒÅori. Less defined types of consent shall be balanced by stronger governance arrangements. Kaitiakitanga, guardianship MƒÅori data shall be stored and transferred in such a way that it enables and reinforces the capacity of MƒÅori to exercise kaitiakitanga over MƒÅori data. Ethics. Tikanga, kawa (protocols) and mƒÅtauranga (knowledge) shall underpin the protection, access and use of MƒÅori data. MƒÅori shall decide which MƒÅori data shall be controlled (tapu) or open (noa) access. 2.4 Resources Why data sovereignty matters Indigenous Data Sovereignty and Policy Principles of MƒÅori Data Sovereignty Good data practices for Indigenous data sovereignty and governance. "],["data-wrangling-and-vizualisation.html", "3 Data wrangling and vizualisation 3.1 Learning objectives 3.2 Common dataframe manipulations in the tidyverse 3.3 Data Viz 3.4 Ten Simple Rules for Better Figures 3.5 ggplot2 3.6 Resources", " 3 Data wrangling and vizualisation 3.1 Learning objectives Carry out, and interpret the outputs of, basic exploratory data analysis using in-built R functions Discuss the ethics of data vizualisation Create and communicate informative data visualisations using R Discuss and critique data visualisations 3.2 Common dataframe manipulations in the tidyverse 3.2.1 Using dplyr and tidyr tidy data ‚ÄúTidy datasets are all alike, but every messy dataset is messy in its own way.‚Äù ‚Äî Hadley Wickham There are three interrelated rules which make a dataset tidy: Each variable must have its own column Each observation must have its own row Each value must have its own cell [illustrations from the Openscapes blog Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst Why ensure that your data is tidy? Consistency: using a consistent format aids learning and reproducibility Simplicity: it‚Äôs a format that is well understood by R ‚ÄúTidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets.‚Äù ‚Äî Hadley Wickham, Tidy data 3.2.2 Introuducing the Palmer penguins library(palmerpenguins) ## contains some nice penguin data penguins ## # A tibble: 344 x 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen NA NA NA NA ## 5 Adelie Torgersen 36.7 19.3 193 3450 ## 6 Adelie Torgersen 39.3 20.6 190 3650 ## 7 Adelie Torgersen 38.9 17.8 181 3625 ## 8 Adelie Torgersen 39.2 19.6 195 4675 ## 9 Adelie Torgersen 34.1 18.1 193 3475 ## 10 Adelie Torgersen 42 20.2 190 4250 ## # ‚Ä¶ with 334 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; So, what does this show us? A tibble: 344 x 8: A tibble is a specific kind of data frame in R. The penguin dataset has 344 rows (i.e., 344 different observations). Here, each observation corresponds to a penguin. 8 columns corresponding to 3 variables describing each observation. species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, sex, and year are the different variables of this dataset. We then have a preview of the first 10 rows of observations corresponding to the first 10 penguins. ``... with 334 more rows indicates there are 334 more rows to see, but these have not been printed (likely as it would clog our screen) To learn more about the penguins read the paper that talks all about the data collection. 3.2.3 Common dataframe manipulations in the tidyverse, using dplyr and tidyr Even from these first few rows of data we can see that there are some NA values. Let‚Äôs count the number of NAs. Remember the %&gt;% operator? Here we‚Äôre going to be introduced to a few new things the apply() function, the is.na() function, and how R deals with logical values! library(tidyverse) penguins %&gt;% apply(.,2,is.na) %&gt;% apply(.,2,sum) ## species island bill_length_mm bill_depth_mm ## 0 0 2 2 ## flipper_length_mm body_mass_g sex year ## 2 2 11 0 There‚Äôs lot going on in that code! Let‚Äôs break it down Take penguins then Use penguins as an input to the apply() function (this is specified as the first argument using the .) Now the apply() function takes 3 arguments: the data object you want it to apply something to (in our case penguins) the margin you want to apply that something to; 1 stands for rows and 2 stands for columns, and the function you want it to apply (in our case is.na()). So the second line of code is asking R to apply the is.na() function over the columns of penguins is.na() asks for each value it‚Äôs fed is it an NA value; it returns a TRUE if so and a FALSE otherwise The output from the first apply() is then fed to the second apply() (using the .). The sum() function then add them up! R treats a TRUE as a 1 and a FALSE as a 0. So how many NAs do you think there are! Doesn‚Äôt help much. To Now we know there are NA values throughout the data let‚Äôs remove then and create a new NA free version called penguins_nafree. There is a really handy tidyverse (dplyr) function for this! penguins_nafree &lt;- penguins %&gt;% drop_na() penguins_nafree ## # A tibble: 333 x 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen 36.7 19.3 193 3450 ## 5 Adelie Torgersen 39.3 20.6 190 3650 ## 6 Adelie Torgersen 38.9 17.8 181 3625 ## 7 Adelie Torgersen 39.2 19.6 195 4675 ## 8 Adelie Torgersen 41.1 17.6 182 3200 ## 9 Adelie Torgersen 38.6 21.2 191 3800 ## 10 Adelie Torgersen 34.6 21.1 198 4400 ## # ‚Ä¶ with 323 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; Below are some other useful manipulation functions; have a look at the outputs and run them yourselves and see if you can work out what they‚Äôre doing. filter(penguins_nafree, island == &quot;Torgersen&quot; ) ## # A tibble: 47 x 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen 36.7 19.3 193 3450 ## 5 Adelie Torgersen 39.3 20.6 190 3650 ## 6 Adelie Torgersen 38.9 17.8 181 3625 ## 7 Adelie Torgersen 39.2 19.6 195 4675 ## 8 Adelie Torgersen 41.1 17.6 182 3200 ## 9 Adelie Torgersen 38.6 21.2 191 3800 ## 10 Adelie Torgersen 34.6 21.1 198 4400 ## # ‚Ä¶ with 37 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; summarise(penguins_nafree, avgerage_bill_length = mean(bill_length_mm)) ## # A tibble: 1 x 1 ## avgerage_bill_length ## &lt;dbl&gt; ## 1 44.0 group_by(penguins_nafree, species) ## # A tibble: 333 x 8 ## # Groups: species [3] ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen 36.7 19.3 193 3450 ## 5 Adelie Torgersen 39.3 20.6 190 3650 ## 6 Adelie Torgersen 38.9 17.8 181 3625 ## 7 Adelie Torgersen 39.2 19.6 195 4675 ## 8 Adelie Torgersen 41.1 17.6 182 3200 ## 9 Adelie Torgersen 38.6 21.2 191 3800 ## 10 Adelie Torgersen 34.6 21.1 198 4400 ## # ‚Ä¶ with 323 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; Often we want to summarise variables by different groups (factors). Below we Take the penguins_nafree data then Use this and apply the group_by() function to group by species Use this output and apply the summarize() function to calculate the mean (using (mean()) bill length (bill_length_mm) of each group (species), calling the resulting number avgerage_bill_length penguins_nafree %&gt;% group_by(species) %&gt;% summarise(avgerage_bill_length = mean(bill_length_mm)) ## # A tibble: 3 x 2 ## species avgerage_bill_length ## &lt;fct&gt; &lt;dbl&gt; ## 1 Adelie 38.8 ## 2 Chinstrap 48.8 ## 3 Gentoo 47.6 We can also group by multiple factors, for example, penguins_nafree %&gt;% group_by(island,species) %&gt;% summarise(avgerage_bill_length = mean(bill_length_mm)) ## `summarise()` has grouped output by &#39;island&#39;. You can override using the `.groups` argument. ## # A tibble: 5 x 3 ## # Groups: island [3] ## island species avgerage_bill_length ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 Biscoe Adelie 39.0 ## 2 Biscoe Gentoo 47.6 ## 3 Dream Adelie 38.5 ## 4 Dream Chinstrap 48.8 ## 5 Torgersen Adelie 39.0 3.3 Data Viz ‚Äú‚Ä¶have obligations in that we have a great deal of power over how people ultimately make use of data, both in the patterns they see and the conclusions they draw.‚Äù ‚Äî Michael Correll, Ethical Dimensions of Visualization Research ‚ÄúClutter and confusion are not attributes of data - they are shortcomings of design.‚Äù ‚Äî Edward Tufte 3.3.1 Exploratory and explanatory plots Exploratory plots (for you) data exploration doesn‚Äôt have to look pretty just needs to get to the point explore and discover new data facets formulate new questions For example, Explanatory plots (for others), most common kind of graph used in scientific publications clear purpose designed for the audience make it easy to read (this covers a lot of things) do not distort guide the reader to a particular conclusion answer a specific question support a decision For example, Plots by Cedric Scherer and mentioned on this blog 3.4 Ten Simple Rules for Better Figures ‚ÄúScientific visualization is classically defined as the process of graphically displaying scientific data. However, this process is far from direct or automatic. There are so many different ways to represent the same data: scatter plots, linear plots, bar plots, and pie charts, to name just a few. Furthermore, the same data, using the same type of plot, may be perceived very differently depending on who is looking at the figure. A more accurate definition for scientific visualization would be a graphical interface between people and data.‚Äù ‚Äî Nicolas P. Rougier, Michael Droettboom, Philip E. Bourne, Ten Simple Rules for Better Figures Know Your Audience Identify Your Message Adapt the Figure to the Support Medium Captions Are Not Optional Do Not Trust the Defaults Use Color Effectively Do Not Mislead the Reader There are formulas to measure how misleading a graph is! Avoid Chartjunk Message Trumps Beauty ‚Äúmessage and readability of the figure is the most important aspect while beauty is only an option‚Äù ‚Äî Nicolas P. Rougier, Michael Droettboom, Philip E. Bourne, Ten Simple Rules for Better Figures Get the Right Tool I‚Äôm an advocate for R üòâ 3.5 ggplot2 ggplot2 is an R package for producing statistical, or data, graphics; it has an underlying grammar based on the Grammar of Graphics Every ggplot2 plot has three key components: data, A set of aesthetic mappings between variables in the data and visual properties, and At least one layer which describes how to render each observation. Layers are usually created with a geom function. 3.5.1 ggplot2 layers 3.5.2 Examples Scatter plot using geom_point() ggplot(penguins,aes(x = body_mass_g, y = flipper_length_mm)) + ## data &amp; aesthetics geom_point() + ## geom geom_smooth(method = &#39;lm&#39;, se = FALSE) ## statistics (linear regression line) Boxplot using geom_boxplot() ggplot(penguins,aes(x = species, y = flipper_length_mm)) + ## data &amp; aesthetics geom_boxplot() + ## geom ggtitle(&quot;Flipper length (mm) by species&quot;) + ylab(&quot;Flipper length (mm)&quot;) + xlab(&quot;Species&quot;) + theme_dark() ## theme Scatter plot specifying color using geom_point() ggplot(penguins,aes(x = body_mass_g, y = flipper_length_mm, color = species)) + ## data and aesthetics geom_point() + ## geom geom_smooth(method = &#39;lm&#39;, se = FALSE) ## statistic (linear regression line without intervals) 3.5.3 The Good, the Bad, and the Ugly‚Ä¶ box &lt;- ggplot(penguins,aes(x = species, y = flipper_length_mm)) + ## data &amp; aesthetics geom_boxplot() + ## geom ggtitle(&quot;Flipper length (mm) by species&quot;) + ylab(&quot;Flipper length (mm)&quot;) + xlab(&quot;Species&quot;) + theme_dark() ## theme box jitter &lt;- ggplot(penguins,aes(x = species, y = flipper_length_mm)) + ## data &amp; aesthetics geom_jitter() + ## geom ggtitle(&quot;Flipper length (mm) by species&quot;) + ylab(&quot;Flipper length (mm)&quot;) + xlab(&quot;Species&quot;) + theme_dark() ## theme jitter violin &lt;- ggplot(penguins,aes(x = species, y = flipper_length_mm)) + ## data &amp; aesthetics geom_violin() + ## geom ggtitle(&quot;Flipper length (mm) by species&quot;) + ylab(&quot;Flipper length (mm)&quot;) + xlab(&quot;Species&quot;) + theme_dark() ## theme violin ggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm)) + geom_point() + geom_smooth(method = &quot;lm&quot;, col = &quot;blue&quot;, se = FALSE) ## `geom_smooth()` using formula &#39;y ~ x&#39; ggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm, col = species)) + geom_point(size = 2, alpha = 0.5) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + facet_grid(~ sex) + theme_bw() + labs(title = &quot;Flipper Length and Body Mass, by Sex &amp; Species&quot;, subtitle = paste0(nrow(penguins), &quot; of the Palmer Penguins&quot;), x = &quot;Body Mass (g)&quot;, y = &quot;Flipper Length (mm)&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; penguins_nafree &lt;- penguins %&gt;% drop_na() ggplot(penguins_nafree, aes(x = body_mass_g, y = flipper_length_mm, col = species)) + geom_point(size = 2, alpha = 0.5) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + facet_grid(~ sex) + theme_bw() + labs(title = &quot;Flipper Length and Body Mass, by Sex &amp; Species&quot;, subtitle = paste0(nrow(penguins_nafree), &quot; of the Palmer Penguins&quot;), x = &quot;Body Mass (g)&quot;, y = &quot;Flipper Length (mm)&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; 3.6 Resources ggplot2 cheatsheet Ten Simple Rules for Better Figures Elegant Graphics for Data Analysis Using ggplot2 to communicate your results Interesting blogs on graphs in the media tidyverse Tidy Data Palmer penguins "],["introduction-to-the-design-and-analysis-of-experiments.html", "4 Introduction to the design and analysis of experiments 4.1 Learning Objectives 4.2 Key phrases 4.3 Three key principles: 4.4 One-Way Analysis of Variance (ANOVA) 4.5 üò± p-values üò± 4.6 Terminology and issues 4.7 Resources", " 4 Introduction to the design and analysis of experiments 4.1 Learning Objectives Identify the following experimental unit observational units List and describe the three main principals of experimental design Randomization Replication Blocking Calculate Sums of Squares (between and within groups) given the observations Define and state the appropriate degrees of freedom in a one-way ANOVA scenario Calculate the F-statistics given the appropriate Sums of Squares and degrees of freedom Interpret and discuss a given p-value in the context of a stated hypothesis test Explain between group and within group variation Communicate statistical concepts and experimental outcomes clearly using language appropriate for both a scientific and non-scientific audience 4.2 Key phrases Experimental unit Smallest portion of experimental material which is independently perturbed Treatment The experimental condition independently applied to an experimental unit Observational unit The smallest unit on which a response is measured. If one measurement is made on each rat: Observational unit = Experimental unit. If Multiple measurements are made on each rat: Each experimental unit has &gt;1 observational unit (pseudo- or technical replication). 4.3 Three key principles: 4.3.1 Replication Biological replication: each treatment is independently applied to each of several humans, animals or plants To generalize results to population Technical replication: two or more samples from the same biological source which are independently processed Advantageous if processing steps introduce a lot of variation Increases the precision with which comparisons of relative abundances between treatments are made Pseudo-replication: one sample from the same biological source, divided into two or more aliquots which are independently measured Advantageous for noisy measuring instruments Increases the precision with which comparisons of relative abundances between treatments are made 4.3.2 Randomization Protects against bias Plan the experiment in such a way that the variations caused by extraneous factors can all be combined under the general heading of ‚Äúchance‚Äù. Ensures that each treatment has the same probability of getting good (or bad) units and thus avoids systematic bias random allocation can cancel out population bias; it ensures that any other possible causes for the experimental results are split equally between groups typically statistical analysis assumes that observations are independent. This is almost never strictly true in practice but randomisation means that our estimates will behave as if they were based on independent observations 4.3.3 Blocking Blocking helps control variability by making treatment groups more alike. Experimental units are divided into subsets (called blocks) so that units within the same block are more similar than units from different subsets or blocks. Blocking is a technique for dealing with nuisance factors. A nuisance factor is a factor that has some effect on the response, but is of no interest (e.g., age class). 4.4 One-Way Analysis of Variance (ANOVA) 4.4.1 Between group SS (SSB) The idea: Assess distances between treatment (surgical condition) means relative to our uncertainty about the actual (true) treatment means. add up the differences: -1.192 + -0.703 + 1.895 = 0. This is always the case! So adding up the differences: -1.192 + -0.703 + 1.895 = 0. Not a great way to measure distances! Sums of Squares? -1.192^2 + -0.703^2 + 1.895^2 add up the squared differences? but‚Ä¶ there are 4 observations in each group (treatment) 4\\(\\times\\)(-1.192)^2 + 4\\(\\times\\)(-0.703)^2 + 4\\(\\times\\)(1.895)^2 This is the Between Groups Sums of Squares or the Between group SS (SSB) So the Between group SS (SSB) = 22.02635 Adding up the differences: -1.192 + -0.703 + 1.895 = 0. This is always the case and that itself gives us information‚Ä¶ We only need to know two of the values to work out the third! So we have only 2 bits of unique information; SSB degrees of freedom = 2 4.4.2 Within group SS (SSW) The Within group SS (SSW) arises from the same idea: To assess distances between treatment (surgical condition) means relative to our uncertainty about the actual (true) treatment means. Procedure: Observation - Treatment mean Square the difference Add them up! Within group SS (SSW) unexplained variance 4.4.3 F-statistic Recall the Between group SS (SSB) = 22.02635 So mean SSB = 22.02635 / 2 The within group SS (SSW) = 6.059075 Here we have 3\\(\\times\\) 3 bits of unique information: within groups degrees of freedom is 9. So mean SSW = 6.059/9 Consider the ratio \\({\\frac{{\\text{variation due to treatments}}}{{\\text{unexplained variance}}}} = {\\frac{{\\text{ mean between-group variability}}}{{\\text{mean within-group variability}}}}\\) \\(=\\frac{\\text{mean SSB}}{\\text{mean SSW}}\\) \\(=\\frac{\\text{MSB}}{\\text{MSW}}\\) = \\(=\\frac{\\text{experimental variance}}{\\text{error variance}}\\) 16.3586975 This is the F-statistic! 4.4.4 Degrees of freedom (DF) Essentially statistical currency (i.e., unique bits of information). So in the example above we have 3 treatment groups and if we know the mean of two we know the third (i.e., 2 unique bits of info) so SSB df = 2. Now, for SSW df. We have 12 observations (4 in each group); we know the treatment means so if we have three of those observed values in each group we know the fourth: 12 - 3 = 9 (i.e., number of observations - number of df lost due to knowing the cell means). 4.5 üò± p-values üò± The ASA Statement on p-Values: Context, Process, and Purpose ‚ÄúGood statistical practice, as an essential component of good scientific practice, emphasizes principles of good study design and conduct, a variety of numerical and graphical summaries of data, understanding of the phenomenon under study, interpretation of results in context, complete reporting and proper logical and quantitative understanding of what data summaries mean. No single index should substitute for scientific reasoning.‚Äù ‚Äî ASA Statement on p-Values What is a p-Value? Informally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value p-values can indicate how incompatible the data are with a specified statistical model p-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold Proper inference requires full reporting and transparency A p-value, or statistical significance, does not measure the size of an effect or the importance of a result By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis 4.6 Terminology and issues Type I error (false positive): declare a difference (i.e., reject \\(H_0\\)) when there is no difference (i.e.¬†\\(H_0\\) is true). Risk of the Type I error is determined by the level of significance (which we set!) (i.e., \\(\\alpha =\\text{ P(Type I error)} = \\text{P(false positive)}\\). Type II error (false negative): difference not declared (i.e., \\(H_0\\) not rejected) when there is a difference (i.e., \\(H_0\\) is false). Let \\(\\beta =\\) P(do not reject \\(H_0\\) when \\(H_0\\) is false); so, \\(1-\\beta\\) = P(reject \\(H_0\\) when \\(H_0\\) is false) = P(a true positive), which is the statistical power of the test. Each time we carry out a hypothesis test the probability we get a false positive result (type I error) is given by \\(\\alpha\\) (the level of significance we choose). When we have multiple comparisons to make we should then control the Type I error rate across the entire family of tests under consideration, i.e., control the Family-Wise Error Rate (FWER); this ensures that the risk of making at least one Type I error among the family of comparisons in the experiment is \\(\\alpha\\). State of Nature Don‚Äôt reject \\(H_0\\) reject \\(H_0\\) \\(H_0\\) is true ‚úÖ Type I error \\(H_0\\) is false Type II error ‚úÖ 4.7 Resources Looking forward Traditional name Model formula R code Simple regression \\(Y \\sim X_{continuous}\\) lm(Y ~ X) One-way ANOVA \\(Y \\sim X_{categorical}\\) lm(Y ~ X) Two-way ANOVA \\(Y \\sim X1_{categorical} + X2_{categorical}\\) lm(Y ~ X1 + X2) ANCOVA \\(Y \\sim X1_{continuous} + X2_{categorical}\\) lm(Y ~ X1 + X2) Multiple regression \\(Y \\sim X1_{continuous} + X2_{continuous}\\) lm(Y ~ X1 + X2) Factorial ANOVA \\(Y \\sim X1_{categorical} * X2_{categorical}\\) lm(Y ~ X1 * X2) or lm(Y ~ X1 + X2 + X1:X2) Glass, David J. Experimental Design for Biologists. Second ed.¬†2014. Print. Welham, S. J. Statistical Methods in Biology : Design and Analysis of Experiments and Regression. 2015. Print. Fisher, Ronald Aylmer. The Design of Experiments. 8th ed.¬†Edinburgh: Oliver &amp; Boyd, 1966. Print. O &amp; B Paperbacks. "],["a-completely-randomomised-design.html", "5 A completely randomomised design 5.1 Learning Objectives 5.2 Analysis of a Completely Randomised Design in R: aov() and lm()", " 5 A completely randomomised design 5.1 Learning Objectives Describe a Completely Randomised (experimental) Design Carry out linear regression in R with one categorical explanatory variable (one-way ANOVA) and draw the appropriate inference Communicate statistical concepts and experimental outcomes clearly using language appropriate for both a scientific and non-scientific audience 5.2 Analysis of a Completely Randomised Design in R: aov() and lm() library(tidyverse) rats &lt;- read_csv(&quot;crd_rats_data.csv&quot;) rats %&gt;% group_by(Surgery) %&gt;% summarise(avg = mean(logAUC)) ## # A tibble: 3 x 2 ## Surgery avg ## &lt;fct&gt; &lt;dbl&gt; ## 1 C 8.46 ## 2 P 8.95 ## 3 S 11.5 5.2.1 aov() rats_aov &lt;- aov(logAUC ~ Surgery, data = rats) summary(rats_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Surgery 2 22.026 11.013 16.36 0.00101 ** ## Residuals 9 6.059 0.673 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Inference Hypothesis: We test the Null hypothesis, \\(H_0\\), population (Surgery) means are the same on average verses the alternative hypothesis, \\(H_1\\), that at least one differs from the others! Probability of getting an F-statistic at least as extreme as the one we observe (think of the area under the tails of the curve below) p-value Pr(&gt;F)= 0.001 tells us we have sufficient evidence to reject \\(H_0\\) at the 1% level of significance 5.2.2 lm() lm() rats_lm &lt;- lm(logAUC ~ Surgery, data = rats) Inference summary(rats_lm)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.4600 0.4102531 20.6214144 6.930903e-09 ## SurgeryP 0.4900 0.5801856 0.8445574 4.202408e-01 ## SurgeryS 3.0875 0.5801856 5.3215734 4.799872e-04 Which pairs of means are different? Pair-wise comparisons of means Use two-sample t-tests We need to calculate our observed t-value where \\(\\text{t-value} = \\frac{\\text{Sample Difference}_{ij} - \\text{Difference assuming } H_0 \\text{ is true}_{ij}}{\\text{SE of } \\text{Sample Difference}_{ij}}\\) where \\(\\text{Sample Difference}_{ij}\\) = Difference between pair of sample means Compute the p-value for observed t-value (Intercept) = \\(\\text{mean}_C\\) = 8.46 SE of (Intercept) = SE of \\(\\text{mean}_C\\) = SEM = 0.4102531 \\(\\text{Surgery}_P\\) = \\(\\text{mean}_P\\) ‚Äì \\(\\text{mean}_C\\) = 0.49 SE of \\(\\text{Surgery}_P\\) = SE of (\\(\\text{mean}_P\\) - \\(\\text{mean}_C\\) ) = SED = 0.5801856 Hypotheses being tested The t value and Pr (&gt;|t|) are the t - and p-value for testing the null hypotheses: Mean abundance is zero for C population No difference between the population means of P and C No difference between the population means of S and C We‚Äôre interested in 2 and 3, but not necessarily 1! Two-sample t -tests for pairwise comparisons of means SurgeryP : t value = Estimate √∑ Std.Error = 0.8446; Pr (&gt;|t|) = 0.4202 F-test: anova(rats_lm) ## Analysis of Variance Table ## ## Response: logAUC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Surgery 2 22.0263 11.0132 16.359 0.001006 ** ## Residuals 9 6.0591 0.6732 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The same as aov() in fact aov() is calling lm() in the background. 5.2.2.1 Diagnostic plots Carrying out any linear regression we have some key assumptions Independence There is a linear relationship between the response and the explanatory variables The residuals have constant variance The residuals are normally distributed gglm::gglm(rats_lm) # Plot the four main diagnostic plots Residuals vs Fitted plot You are basically looking for no pattern or structure in your residuals (e.g., a ‚Äústarry‚Äù night). You definitely don‚Äôt want to see is the scatter increasing around the zero line (dashed line) as the fitted values get bigger (e.g., think of a trumpet, a wedge of cheese, or even a slice of pizza) which would indicate unequal variances (heteroscedacity). Normal quantile-quantile (QQ) plot This plot shows the sorted residuals versus expected order statistics from a standard normal distribution. Samples should be close to a line; points moving away from 45 degree line at the tails suggest the data are from a skewed distribution. Scale-Location plot (\\(\\sqrt{\\text{|standardized residuals vs Fitted|}}\\)) Another way to check the homoskedasticity (constant-variance) assumption. We want the line to be roughly horizontal. If this is the case then the average magnitude of the standardized residuals isn‚Äôt changing much as a function of the fitted values. We‚Äôd also like the spread around the line not to vary much with the fitted values; then the variability of magnitudes doesn‚Äôt vary much as a function of the fitted values. Residuals vs Leverage plot (standardized residuals vs Leverage) This can help detect outliers in a linear regression model. For linear regression model leverage measures how sensitive a fitted value is to a change in the true response. We‚Äôre looking at how the spread of standardized residuals changes as the leverage. This can also be used to detect heteroskedasticity and non-linearity: the spread of standardized residuals shouldn‚Äôt change as a function of leverage. In addition, points with high leverage may be influential: that is, deleting them would change the model a lot. "],["multiple-comparisons.html", "6 Multiple comparisons 6.1 Learning objectives 6.2 Adjustments for multiple testing", " 6 Multiple comparisons 6.1 Learning objectives Discuss and critique methods for controlling errors in hypothesis testing, for example Fisher‚Äôs LSD and the Bonferroni Correction Detail and draw inference form multiple comparison procedures such as Tukey‚Äôs HSD and Dunnett‚Äôs test Describe the family wise error rate (FWER) and false discover rate (FDR) in the context of multiple comparisons Calculate the marginal means for a balanced and unbalanced design Communicate statistical concepts and experimental outcomes clearly using language appropriate for both a scientific and non-scientific audience 6.2 Adjustments for multiple testing Recall that each time we carry out a hypothesis test the probability we get a false positive result (type I error) is given by \\(\\alpha\\) (the level of significance we choose). When we have multiple comparisons to make we should then control the Type I error rate across the entire family of tests under consideration, i.e., control the Family-Wise Error Rate (FWER); this ensures that the risk of making at least one Type I error among the family of comparisons in the experiment is \\(\\alpha\\). State of Nature Don‚Äôt reject \\(H_0\\) reject \\(H_0\\) \\(H_0\\) is true ‚úÖ Type I error \\(H_0\\) is false Type II error ‚úÖ or‚Ä¶ The familywise error rate (FWER) is the risk of making at least one Type I error among the family of comparisons in the experiment. Now let‚Äôs consider carrying out \\(m\\) independent t-tests and let for any single test, let Pr(commit a Type 1 error) \\(= \\alpha_c\\) be the per comparison error rate (PCER). So for a single test the probability a correct decision is made is \\(1 - \\alpha_c\\). Therefore for \\(m\\) independent t-tests the probability of committing no Type I errors is \\((1 - \\alpha_c)^m\\) and the probability of committing at least one Type I error is \\(1 -(1 - \\alpha_c)^m = \\alpha_F\\) which is the upper limit of the FWER. 6.2.1 Classification of multiple hypothesis tests Suppose we have a number \\(m\\) of null hypotheses, \\(H_1, H_2, ..., H_m\\). Using the traditional parlence we reject the null hypothesis if the test is declared significant and do not reject the null hypothesis if the test is non-significant. Now, summing each type of outcome over all \\(H_i (i = 1.,..,m)\\) yields the following random variables: Null hypothesis is true (H0) Alternative hypothesis is true (HA) Total Test is declared significant V S R Test is declared non-significant U T m - R Total \\(m_{0}\\) \\(m - m_0\\) m \\(m\\) is the total number hypotheses tested \\(m_{0}\\) is the number of true null hypotheses, an unknown parameter \\(m - m_0\\) is the number of true alternative hypotheses \\(V\\) is the number of false positives (Type I error) (also called false discoveries) \\(S\\) is the number of true positives (also called true discoveries) \\(T\\) is the number of false negatives (Type II error) \\(U\\) is the number of true negatives \\(R=V+S\\) is the number of rejected null hypotheses (also called discoveries, either true or false) 6.2.2 Using the predictmeans package Recall, rats_lm &lt;- lm(logAUC ~ Surgery, data = rats) coef(rats_lm) ## (Intercept) SurgeryP SurgeryS ## 8.4600 0.4900 3.0875 \\[ \\begin{aligned} \\operatorname{logAUC} &amp;= \\alpha + \\beta_{1}(\\operatorname{Surgery}_{\\operatorname{P}}) + \\beta_{2}(\\operatorname{Surgery}_{\\operatorname{S}}) + \\epsilon \\end{aligned} \\] Using the predictmeans package # Load predictmeans (assumes already installed) library(predictmeans) Fisher‚Äôs, Least Significant Difference (LSD) Carry out post-hoc tests only if the ANOVA F-test is significant. If so declare significant \\(100\\alpha\\%\\) any pairwise difference &gt; LSD. This does not control the FWER. tukey &lt;- predictmeans(rats_lm , modelterm = &quot;Surgery&quot;, adj = &quot;tukey&quot;,pairwise = TRUE) Bonferroni correction We reject the \\(H_0\\) for which the p-value, p-val, is p-val \\(&lt; \\alpha_c = \\frac{\\alpha_f}{n_c}\\) where \\(\\alpha_f\\) is the FWER and \\(n_c\\) is the number of pairwise comparisons. Howerer, this makes no assumptions about independence between tests. bonferroni &lt;- predictmeans(rats_lm , modelterm = &quot;Surgery&quot;, adj = &quot;bonferroni&quot;,pairwise = TRUE) 6.2.3 Multiple comparison procedures Tukey‚Äôs Honest Significant Difference (HSD) This compares the mean of every treatment with the mean of every other treatmen and uses a studentized range distribution compated with a t-distribution for Fisher‚Äôs LSD and the Bonferroni correction. Here Tukey‚Äôs studentixed range (TSR) \\(=q_{m,df}(1 - \\frac{\\alpha}{2})\\sqrt{2\\times \\frac{\\text{residual MS}}{\\text{# reps}}}\\) TukeyHSD(aov(logAUC~Surgery, data = rats)) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = logAUC ~ Surgery, data = rats) ## ## $Surgery ## diff lwr upr p adj ## P-C 0.4900 -1.1298813 2.109881 0.6863267 ## S-C 3.0875 1.4676187 4.707381 0.0012479 ## S-P 2.5975 0.9776187 4.217381 0.0039400 False Discovert Rate (FDR) The FDR controls the expected (mean) proportion of false discoveries amongst the \\(R\\) (out of \\(m\\)) hypotheses declared significant. Consider testing \\(m\\) null hypotheses with corresponding p-values \\(P_1, P_2,...,P_m\\); we then order then so that \\(P_{(1)} &lt; P_{(2)} &lt;...&lt;P_{(m)}\\) (where \\(P_{(i)}\\) is the \\(i^{th}\\) largest \\(i=1,...,m\\)). The \\(i^{th}\\) ordered p-value is calculated as \\(\\frac{i}{m}q^*\\) and the \\(i^{th}\\) null hypotesis is rejected if \\(P_i \\leq \\frac{i}{m}q^*\\) "],["factorial-experiments.html", "7 Factorial experiments 7.1 Learning objectives 7.2 Factorial design (as a CRD) 7.3 TL;DR, Model formula syntax in R", " 7 Factorial experiments 7.1 Learning objectives Describe and discuss factorial experiments with both equal and unequal replication Carry out linear regression in R with two categorical explanatory variables and an interaction (two-way ANOVA with interaction) and draw the appropriate inference Calculate the marginal means for a balanced and unbalanced design 7.2 Factorial design (as a CRD) Example Scientific Objective Global metabolic profiling and comparison of relative abundances of proteins in the inner and outer left ventricle wall of diabetic and healthy male Wistar rats. 7.2.1 Equal replications (balanced design) Analysis using lm() factorial &lt;- read_csv(&quot;factorial_expt.csv&quot;) Fitting models with interaction terms glimpse(factorial) ## Rows: 12 ## Columns: 5 ## $ Disease &lt;chr&gt; &quot;Healthy&quot;, &quot;Healthy&quot;, &quot;Healthy&quot;, &quot;Healthy&quot;, &quot;Healthy&quot;, &quot;Health‚Ä¶ ## $ Organ &lt;chr&gt; &quot;innerLV&quot;, &quot;outerLV&quot;, &quot;innerLV&quot;, &quot;outerLV&quot;, &quot;innerLV&quot;, &quot;outerL‚Ä¶ ## $ Animal &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 ## $ Sample &lt;dbl&gt; 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2 ## $ logAUC &lt;dbl&gt; 9.40, 8.83, 10.33, 10.49, 9.74, 10.98, 7.92, 9.37, 8.69, 11.31‚Ä¶ ## change to factors (saves errors with predictmeans) factorial$Disease &lt;- as.factor(factorial$Disease) factorial$Organ &lt;- as.factor(factorial$Organ) ## shorthand version fac_lm &lt;- lm(logAUC ~ Disease*Organ, data = factorial) ## longhand version fac_lm_lh &lt;- lm(logAUC ~ Disease + Organ +Disease:Organ, data = factorial) ## both are the SAME cbind(&quot;short hand&quot; = coef(fac_lm),&quot;long hand&quot; = coef(fac_lm_lh)) ## short hand long hand ## (Intercept) 7.873333 7.873333 ## DiseaseHealthy 1.950000 1.950000 ## OrganouterLV 2.116667 2.116667 ## DiseaseHealthy:OrganouterLV -1.840000 -1.840000 So the full model is \\[ \\begin{aligned} \\operatorname{logAUC} &amp;= \\alpha + \\beta_{1}(\\operatorname{Disease}_{\\operatorname{Healthy}}) + \\beta_{2}(\\operatorname{Organ}_{\\operatorname{outerLV}}) + \\beta_{3}(\\operatorname{Disease}_{\\operatorname{Healthy}} \\times \\operatorname{Organ}_{\\operatorname{outerLV}})\\ + \\\\ &amp;\\quad \\epsilon \\end{aligned} \\] And the gobal null hypotheses being tested are: \\(H_0: \\hat{\\mu}_{\\text{Diabetic}} = \\hat{\\mu}_{\\text{Healthy}}\\) \\(H_0: \\hat{\\mu}_{\\text{innerLV}} = \\hat{\\mu}_{\\text{outerLV}}\\) \\(H_0: \\hat{\\mu}_{\\text{Diabetic,innerLV}} = \\hat{\\mu}_{\\text{Diabetic,outerLV}} = \\hat{\\mu}_{\\text{Healthy,innerLV}} = \\hat{\\mu}_{\\text{Healthy,outerLV}}\\) anova(fac_lm) ## Analysis of Variance Table ## ## Response: logAUC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Disease 1 3.1827 3.1827 3.6304 0.09320 . ## Organ 1 4.2960 4.2960 4.9003 0.05775 . ## Disease:Organ 1 2.5392 2.5392 2.8963 0.12719 ## Residuals 8 7.0135 0.8767 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Plotting the fitted model Note with a balanced design ordering of term doesn‚Äôt matter. For example, fac_lm &lt;- lm(logAUC ~ Disease*Organ, data = factorial) anova(fac_lm) ## Analysis of Variance Table ## ## Response: logAUC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Disease 1 3.1827 3.1827 3.6304 0.09320 . ## Organ 1 4.2960 4.2960 4.9003 0.05775 . ## Disease:Organ 1 2.5392 2.5392 2.8963 0.12719 ## Residuals 8 7.0135 0.8767 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 fac_lm_2 &lt;- lm(logAUC ~ Organ*Disease, data = factorial) anova(fac_lm_2) ## Analysis of Variance Table ## ## Response: logAUC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Organ 1 4.2960 4.2960 4.9003 0.05775 . ## Disease 1 3.1827 3.1827 3.6304 0.09320 . ## Organ:Disease 1 2.5392 2.5392 2.8963 0.12719 ## Residuals 8 7.0135 0.8767 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Inference using predictmeans interaction &lt;- predictmeans(fac_lm, modelterm = &quot;Disease:Organ&quot;, pairwise = TRUE) interaction$`Predicted Means` ## Organ innerLV outerLV ## Disease ## Diabetic 7.8733 9.9900 ## Healthy 9.8233 10.1000 interaction$`Standard Error of Means` ## All means have the same Stder ## 0.54058 interaction$`Pairwise LSDs` ## Diabetic:innerLV Diabetic:outerLV Healthy:innerLV ## Diabetic:innerLV 0.00000 -2.11667 -1.95000 ## Diabetic:outerLV 1.76294 0.00000 0.16667 ## Healthy:innerLV 1.76294 1.76294 0.00000 ## Healthy:outerLV 1.76294 1.76294 1.76294 ## Healthy:outerLV ## Diabetic:innerLV -2.22667 ## Diabetic:outerLV -0.11000 ## Healthy:innerLV -0.27667 ## Healthy:outerLV 0.00000 ## attr(,&quot;Significant level&quot;) ## [1] 0.05 ## attr(,&quot;Degree of freedom&quot;) ## [1] 8 ## attr(,&quot;Note&quot;) ## [1] &quot;LSDs matrix has mean differences (row-col) above the diagonal, LSDs (adjusted by &#39;none&#39; method) below the diagonal&quot; ## plot print(interaction$predictmeansPlot) 7.2.2 Unqual replications (unbalanced design) As per lecture slides let‚Äôs set logAUC obvservations 1,2,3, 10 to NA unbalanced &lt;- factorial unbalanced$logAUC[c(1:3,10)] &lt;- NA unbalanced ## # A tibble: 12 x 5 ## Disease Organ Animal Sample logAUC ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Healthy innerLV 1 1 NA ## 2 Healthy outerLV 2 2 NA ## 3 Healthy innerLV 3 1 NA ## 4 Healthy outerLV 4 2 10.5 ## 5 Healthy innerLV 5 1 9.74 ## 6 Healthy outerLV 6 2 11.0 ## 7 Diabetic innerLV 7 1 7.92 ## 8 Diabetic outerLV 8 2 9.37 ## 9 Diabetic innerLV 9 1 8.69 ## 10 Diabetic outerLV 10 2 NA ## 11 Diabetic innerLV 11 1 7.01 ## 12 Diabetic outerLV 12 2 9.29 unbalanced_nafree &lt;- unbalanced %&gt;% drop_na() unbalanced_nafree ## # A tibble: 8 x 5 ## Disease Organ Animal Sample logAUC ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Healthy outerLV 4 2 10.5 ## 2 Healthy innerLV 5 1 9.74 ## 3 Healthy outerLV 6 2 11.0 ## 4 Diabetic innerLV 7 1 7.92 ## 5 Diabetic outerLV 8 2 9.37 ## 6 Diabetic innerLV 9 1 8.69 ## 7 Diabetic innerLV 11 1 7.01 ## 8 Diabetic outerLV 12 2 9.29 unbalanced_nafree %&gt;% group_by(Disease, Organ) %&gt;% tally() ## # A tibble: 4 x 3 ## # Groups: Disease [2] ## Disease Organ n ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 Diabetic innerLV 3 ## 2 Diabetic outerLV 2 ## 3 Healthy innerLV 1 ## 4 Healthy outerLV 2 Analysis using lm() Note: order matters. For example, fac_lm &lt;- lm(logAUC ~ Disease*Organ, data = unbalanced_nafree) anova(fac_lm) ## Analysis of Variance Table ## ## Response: logAUC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Disease 1 7.1102 7.1102 18.4955 0.01264 * ## Organ 1 3.1149 3.1149 8.1027 0.04656 * ## Disease:Organ 1 0.0913 0.0913 0.2376 0.65145 ## Residuals 4 1.5377 0.3844 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 fac_lm_2 &lt;- lm(logAUC ~ Organ*Disease, data = unbalanced_nafree) anova(fac_lm_2) ## Analysis of Variance Table ## ## Response: logAUC ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Organ 1 5.7291 5.7291 14.9029 0.01814 * ## Disease 1 4.4960 4.4960 11.6953 0.02678 * ## Organ:Disease 1 0.0913 0.0913 0.2376 0.65145 ## Residuals 4 1.5377 0.3844 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.3 TL;DR, Model formula syntax in R In R to specify the model you want to fit you typically create a model formula object; this is usually then passed as the first argument to the model fitting function (e.g., lm()). Some notes on syntax: Consider the model formula example y ~ x + z + x:z. There is a lot going on here: The variable to the left of ~ specifies the response, everything to the right specify the explanatory variables + indicated to include the variable to the left of it and to the right of it (it does not mean they should be summed) : denotes the interaction of the variables to its left and right Additional, some other symbols have special meanings in model formula: * means to include all main effects and interactions, so a*b is the same as a + b + a:b ^ is used to include main effects and interactions up to a specified level. For example, (a + b + c)^2 is equivalent to a + b + c + a:b + a:c + b:c (note (a + b + c)^3 would also add a:b:c) - excludes terms that might otherwise be included. For example, -1 excludes the intercept otherwise included by default, and a*b - b would produce a + a:b Mathematical functions can also be directly used in the model formula to transform a variable directly (e.g., y ~ exp(x) + log(z) + x:z). One thing that may seem counter intuitive is in creating polynomial expressions (e.g., x2). Here the expression y ~ x^2 does not relate to squaring the explanatory variable x (this is to do with the syntax ^ you see above. To include x2 as a term in our model we have to use the I() (the ‚Äúas-is‚Äù operator). For example, y ~ I(x^2)). "],["blocking-incorporating-into-design-and-analysis-of.html", "8 Blocking: incorporating into design and analysis of 8.1 Learning objectives 8.2 Blocking 8.3 A Randomised Controlled Block Design (RCBD) 8.4 Fixed or Random???", " 8 Blocking: incorporating into design and analysis of 8.1 Learning objectives Describe a Randomized Complete Block Design (RCBD) Describe a Split-plot Design (RCBD) Carry out analysis of a RCBD in R using aov(), lm(), and lmer() and discuss and compare the three Define a fixed and random effect in the context of experimental design 8.2 Blocking Recall Blocking helps control variability by making treatment groups more alike. Experimental units are divided into subsets (called blocks) so that units within the same block are more similar than units from different subsets or blocks. Blocking is a technique for dealing with nuisance factors. A nuisance factor is a factor that has some effect on the response, but is of no interest (e.g., age class). Key idea Partition known sources of variation which are unimportant to key scientific question(s) to improve precision of comparisons between treatment means. 8.3 A Randomised Controlled Block Design (RCBD) response = systematic component + error component 8.4 Fixed or Random??? Fixed effects Terms (parameters) in a statistical model which are fixed, or non-random, quantities. For example, Treatment group‚Äôs mean response: for the same Treatment, we expect this quantity to be the same from experiment to experiment. Terms with specific levels chosen for the experiment, and the primary aim is unbiased estimation of effects, should be allocated as fixed Random effects Terms (parameters) in a statistical model which are considered as random quantities or variables. Terms associated with the structure of the design should be allocated as random. Terms whose levels are a representative sample from a population, and where the variance of the population is of interest, should be allocated as random 8.4.1 Ignoring an effect You‚Äôll find the rcbd.csv file on CANVAS. library(tidyverse) rcbd &lt;- read_csv(&quot;rcbd.csv&quot;) ## Note: Run should be a factor rcbd$Run &lt;- as.factor(rcbd$Run) glimpse(rcbd) ## Rows: 12 ## Columns: 5 ## $ Run &lt;fct&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4 ## $ Surgery &lt;chr&gt; &quot;C&quot;, &quot;P&quot;, &quot;S&quot;, &quot;C&quot;, &quot;P&quot;, &quot;S&quot;, &quot;C&quot;, &quot;P&quot;, &quot;S&quot;, &quot;C&quot;, &quot;P&quot;, &quot;S&quot; ## $ Rat &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 ## $ logAUC8 &lt;dbl&gt; 9.24, 8.81, 10.75, 3.89, 8.62, 10.24, 8.42, 9.93, 11.68, 8.77,‚Ä¶ ## $ logAUC4 &lt;dbl&gt; 7.55, 9.34, 12.43, 6.59, 7.94, 8.56, 7.32, 8.37, 7.27, 8.24, 1‚Ä¶ One-way vs two-way‚Ä¶ anova(lm(logAUC4 ~ Surgery, data = rcbd)) ## Analysis of Variance Table ## ## Response: logAUC4 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Surgery 2 14.662 7.3308 2.5034 0.1366 ## Residuals 9 26.355 2.9284 lm2 &lt;- lm(logAUC4 ~ Run + Surgery, data = rcbd) anova(lm2) ## Analysis of Variance Table ## ## Response: logAUC4 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Run 3 18.1317 6.0439 4.4097 0.05812 . ## Surgery 2 14.6617 7.3308 5.3487 0.04640 * ## Residuals 6 8.2235 1.3706 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Always check assumptions gglm::gglm(lm2) 8.4.2 Analysis using lmer() from lme4 library(lme4) lmer_mod &lt;- lmer(logAUC8 ~ Surgery + (1|Run), data = rcbd) summary(lmer_mod) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: logAUC8 ~ Surgery + (1 | Run) ## Data: rcbd ## ## REML criterion at convergence: 37.2 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.8525 -0.2273 0.1772 0.4036 1.3309 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Run (Intercept) 1.479 1.216 ## Residual 1.447 1.203 ## Number of obs: 12, groups: Run, 4 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 7.5800 0.8552 8.863 ## SurgeryP 1.9750 0.8506 2.322 ## SurgeryS 3.8500 0.8506 4.526 ## ## Correlation of Fixed Effects: ## (Intr) SrgryP ## SurgeryP -0.497 ## SurgeryS -0.497 0.500 Now what about the Random effects We have two variance components Between Groups (Runs) \\(\\hat{\\sigma^2}_{\\text{Run}}\\) = 1.479 Within Runs (between observations) \\(\\hat{\\sigma_2}\\) = 1.447 Note that aov() presents the same information, but in a different way: summary(aov(logAUC8 ~ Surgery + Error(Run), data = rcbd)) ## ## Error: Run ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals 3 17.65 5.883 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Surgery 2 29.652 14.826 10.25 0.0116 * ## Residuals 6 8.682 1.447 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Within Runs (Residuals) \\(\\hat{\\sigma}_2\\) = 1.447 (same as lmer) Between Run variance = \\(\\hat{\\sigma}^2\\) + \\(3\\:\\hat{\\sigma}^2_{\\text{Run}}\\) so \\(\\hat{\\sigma}^2_{\\text{Run}} = \\frac{5.883 - \\hat{\\sigma}^2 }{3} = \\frac{5.883 - 1.447}{3} = 1.479\\) 8.4.2.1 SEMs &amp; SEDs library(predictmeans) pred_means &lt;- predictmeans(lmer_mod, modelterm = &quot;Surgery&quot;, pairwise = TRUE, plot = FALSE) pred_means ## $`Predicted Means` ## Surgery ## C P S ## 7.580 9.555 11.430 ## ## $`Standard Error of Means` ## All means have the same Stder ## 0.85524 ## ## $`Standard Error of Differences` ## Max.SED Min.SED Aveg.SED ## 0.8505913 0.8505913 0.8505913 ## ## $LSD ## Max.LSD Min.LSD Aveg.LSD ## 2.08132 2.08132 2.08132 ## attr(,&quot;Significant level&quot;) ## [1] 0.05 ## attr(,&quot;Degree of freedom&quot;) ## [1] 6 ## ## $`Pairwise LSDs` ## C P S ## C 0.00000 -1.97500 -3.850 ## P 2.08132 0.00000 -1.875 ## S 2.08132 2.08132 0.000 ## attr(,&quot;Significant level&quot;) ## [1] 0.05 ## attr(,&quot;Degree of freedom&quot;) ## [1] 6 ## attr(,&quot;Note&quot;) ## [1] &quot;LSDs matrix has mean differences (row-col) above the diagonal, LSDs (adjusted by &#39;none&#39; method) below the diagonal&quot; ## ## $`Pairwise p-value` ## C P S ## C 0.0000 -2.3219 -4.5263 ## P 0.0593 0.0000 -2.2043 ## S 0.0040 0.0697 0.0000 ## attr(,&quot;Degree of freedom&quot;) ## [1] 6 ## attr(,&quot;Note&quot;) ## [1] &quot;The matrix has t-value above the diagonal, p-value (adjusted by &#39;none&#39; method) below the diagonal&quot; ## attr(,&quot;Letter-based representation of pairwise comparisons at significant level &#39;0.05&#39;&quot;) ## Treatment Mean Group ## 1 S 11.430 A ## 2 P 9.555 AB ## 3 C 7.580 B ## ## $mean_table ## Surgery Predicted means Standard error Df LL of 95% CI UL of 95% CI ## 1 C 7.580 0.8552404 6 5.487302 9.672698 ## 2 P 9.555 0.8552404 6 7.462302 11.647698 ## 3 S 11.430 0.8552404 6 9.337302 13.522698 "],["split-plot-and-repeated-measures-designs.html", "9 Split-plot and repeated measures designs 9.1 Learning objectives 9.2 Analysis of a split-plot design 9.3 Analysis of a repeated measures design 9.4 Repeated measures designs as split-plots in time", " 9 Split-plot and repeated measures designs 9.1 Learning objectives Describe and discuss split-plot experimental design Describe and discuss repeated measures experiments Write R code to visualise repeated measures data Carry out appropriate analysis in R and draw the appropriate inference 9.2 Analysis of a split-plot design Recall the diabetic and healthy male Wistar rats‚Ä¶ (chapter 6) BUT what about Data available on CANVAS split_plot &lt;- read_csv(&quot;split_plot.csv&quot;) ## recall we need to set factors split_plot$Animal &lt;- factor(split_plot$Animal) split_plot$Sample &lt;- factor(split_plot$Sample) split_plot ## # A tibble: 12 x 5 ## Disease Organ Animal Sample logAUC ## &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 Healthy innerLV 1 1 9.4 ## 2 Healthy outerLV 1 2 8.83 ## 3 Healthy innerLV 2 1 10.3 ## 4 Healthy outerLV 2 2 10.5 ## 5 Healthy innerLV 3 1 9.74 ## 6 Healthy outerLV 3 2 11.0 ## 7 Diabetic innerLV 4 1 7.92 ## 8 Diabetic outerLV 4 2 9.37 ## 9 Diabetic innerLV 5 1 8.69 ## 10 Diabetic outerLV 5 2 11.3 ## 11 Diabetic innerLV 6 1 7.01 ## 12 Diabetic outerLV 6 2 9.29 9.2.1 Using aov() sp_aov &lt;- aov(logAUC ~ Disease*Organ + Error(Animal/Sample), data = split_plot) summary(sp_aov) ## ## Error: Animal ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Disease 1 3.183 3.183 2.187 0.213 ## Residuals 4 5.822 1.456 ## ## Error: Animal:Sample ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Organ 1 4.296 4.296 14.423 0.0191 * ## Disease:Organ 1 2.539 2.539 8.525 0.0433 * ## Residuals 4 1.191 0.298 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 9.2.2 Using lmer() (from lmeTest and lmer4) and predictmeans() library(lmerTest) ## MUST LOAD THIS ## ## Attaching package: &#39;lmerTest&#39; ## The following object is masked from &#39;package:lme4&#39;: ## ## lmer ## The following object is masked from &#39;package:stats&#39;: ## ## step sp_lmer &lt;- lmerTest::lmer(logAUC ~ Disease*Organ + (1|Animal), data = split_plot) ## MUST SPECIFY WHICH PACKAGE anova(sp_lmer,type = 2) ## Type II Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## Disease 0.6513 0.6513 1 4 2.1866 0.21329 ## Organ 4.2960 4.2960 1 4 14.4227 0.01914 * ## Disease:Organ 2.5392 2.5392 1 4 8.5246 0.04326 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 library(lme4) library(predictmeans) sp_lmer4 &lt;- lme4::lmer(logAUC ~ Disease*Organ + (1|Animal), data = split_plot) ## MUST SPECIFY WHICH PACKAGE sp_predmeans &lt;- predictmeans(sp_lmer4,modelterm = &quot;Disease:Organ&quot;, pairwise = TRUE, plot = FALSE) sp_predmeans ## $`Predicted Means` ## Organ innerLV outerLV ## Disease ## Diabetic 7.8733 9.9900 ## Healthy 9.8233 10.1000 ## ## $`Standard Error of Means` ## All means have the same Stder ## 0.54058 ## ## $`Standard Error of Differences` ## Max.SED Min.SED Aveg.SED ## 0.7645006 0.4456206 0.6582073 ## attr(,&quot;For the Same Level of Factor&quot;) ## Disease Organ ## Aveg.SED 0.4456206 0.7645006 ## Min.SED 0.4456206 0.7645006 ## Max.SED 0.4456206 0.7645006 ## ## $LSD ## Max.LSD Min.LSD Aveg.LSD ## 2.12259 1.23724 1.82748 ## attr(,&quot;Significant level&quot;) ## [1] 0.05 ## attr(,&quot;Degree of freedom&quot;) ## [1] 4 ## ## $`Pairwise LSDs` ## Diabetic:innerLV Diabetic:outerLV Healthy:innerLV ## Diabetic:innerLV 0.00000 -2.11667 -1.95000 ## Diabetic:outerLV 1.23724 0.00000 0.16667 ## Healthy:innerLV 2.12259 2.12259 0.00000 ## Healthy:outerLV 2.12259 2.12259 1.23724 ## Healthy:outerLV ## Diabetic:innerLV -2.22667 ## Diabetic:outerLV -0.11000 ## Healthy:innerLV -0.27667 ## Healthy:outerLV 0.00000 ## attr(,&quot;Significant level&quot;) ## [1] 0.05 ## attr(,&quot;Degree of freedom&quot;) ## [1] 4 ## attr(,&quot;Note&quot;) ## [1] &quot;LSDs matrix has mean differences (row-col) above the diagonal, LSDs (adjusted by &#39;none&#39; method) below the diagonal&quot; ## ## $`Pairwise p-value` ## Diabetic:innerLV Diabetic:outerLV Healthy:innerLV ## Diabetic:innerLV 0.0000 -4.7499 -2.5507 ## Diabetic:outerLV 0.0090 0.0000 0.2180 ## Healthy:innerLV 0.0633 0.8381 0.0000 ## Healthy:outerLV 0.0436 0.8925 0.5683 ## Healthy:outerLV ## Diabetic:innerLV -2.9126 ## Diabetic:outerLV -0.1439 ## Healthy:innerLV -0.6209 ## Healthy:outerLV 0.0000 ## attr(,&quot;Degree of freedom&quot;) ## [1] 4 ## attr(,&quot;Note&quot;) ## [1] &quot;The matrix has t-value above the diagonal, p-value (adjusted by &#39;none&#39; method) below the diagonal&quot; ## attr(,&quot;Letter-based representation of pairwise comparisons at significant level &#39;0.05&#39;&quot;) ## Treatment Mean Group ## 1 Healthy:outerLV 10.100000 A ## 2 Diabetic:outerLV 9.990000 A ## 3 Healthy:innerLV 9.823333 AB ## 4 Diabetic:innerLV 7.873333 B ## ## $mean_table ## Disease Organ Predicted means Standard error Df LL of 95% CI UL of 95% CI ## 1 Diabetic innerLV 7.873333 0.5405835 4 6.372433 9.374234 ## 2 Diabetic outerLV 9.990000 0.5405835 4 8.489099 11.490901 ## 3 Healthy innerLV 9.823333 0.5405835 4 8.322433 11.324234 ## 4 Healthy outerLV 10.100000 0.5405835 4 8.599099 11.600901 9.3 Analysis of a repeated measures design We have (balanced) data with the same number of observations on each rat at the same time points. 9.3.1 The data Data available on CANVAS library(tidyverse) liver &lt;- read_csv(&quot;repeated_measures_liver.csv&quot;) ## change time to factor liver$Time &lt;- as.factor(liver$Time) glimpse(liver) ## Rows: 210 ## Columns: 4 ## $ Animal &lt;chr&gt; &quot;Control1&quot;, &quot;Control1&quot;, &quot;Control1&quot;, &quot;Control1&quot;, &quot;Control1&quot;, ‚Ä¶ ## $ Treatment &lt;chr&gt; &quot;Control&quot;, &quot;Control&quot;, &quot;Control&quot;, &quot;Control&quot;, &quot;Control&quot;, &quot;Cont‚Ä¶ ## $ Time &lt;fct&gt; 0, 5, 10, 15, 20, 25, 30, 0, 5, 10, 15, 20, 25, 30, 0, 5, 10‚Ä¶ ## $ Glucose &lt;dbl&gt; 1.599, 1.279, 1.599, 1.579, 1.279, 1.439, 1.179, 0.840, 0.64‚Ä¶ 9.3.2 Visualise plot data ggplot(liver, aes(x = Time, y = Glucose, color = Treatment, group = Animal)) + geom_line(size = 1) + geom_point(shape = 1) + theme_bw() plot group means liver_means &lt;- liver %&gt;% group_by(Time,Treatment) %&gt;% summarise(Glucose = mean(Glucose)) ## `summarise()` has grouped output by &#39;Time&#39;. You can override using the `.groups` argument. ggplot(liver_means, aes(x = Time, y = Glucose, color = Treatment, group = Treatment)) + geom_line(size = 1) + geom_point(shape = 1) + theme_bw() linear? ggplot(liver_means, aes(x = Time, y = Glucose, color = Treatment, group = Treatment)) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_point(shape = 1) + theme_bw() ## `geom_smooth()` using formula &#39;y ~ x&#39; 9.3.3 Using aov() re_aov &lt;- aov(Glucose ~ Treatment*Time + Error(Animal),data = liver) summary(re_aov) ## ## Error: Animal ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Treatment 3 1.90 0.6335 1.407 0.263 ## Residuals 26 11.71 0.4503 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Time 6 0.7973 0.13289 8.732 0.0000000334 *** ## Treatment:Time 18 0.2539 0.01411 0.927 0.547 ## Residuals 156 2.3741 0.01522 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 9.3.4 Using lmer() (from lmerTest and lme4) and predictmeans() library(lmerTest) ## MUST LOAD THIS re_lmer &lt;- lmerTest::lmer(Glucose ~ Treatment*Time + (1|Animal),data = liver) ## MUST SPECIFY WHICH PACKAGE anova(re_lmer,type = 2) ## Type II Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## Treatment 0.06423 0.021409 3 26 1.4068 0.2632 ## Time 0.79731 0.132885 6 156 8.7318 0.00000003345 *** ## Treatment:Time 0.25390 0.014105 18 156 0.9269 0.5474 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 library(lme4) library(predictmeans) re_lmer4 &lt;- lme4::lmer(Glucose ~ Treatment*Time + (1|Animal),data = liver) ## MUST SPECIFY WHICH PACKAGE predictmeans::residplot(re_lmer4) sp_predmeans &lt;- predictmeans(re_lmer4,modelterm = &quot;Time&quot;, pairwise = TRUE) sp_predmeans ## $`Predicted Means` ## Time ## 0 5 10 15 20 25 30 ## 0.6470 0.5885 0.5668 0.5361 0.4977 0.5023 0.4420 ## ## $`Standard Error of Means` ## All means have the same Stder ## 0.0509 ## ## $`Standard Error of Differences` ## Max.SED Min.SED Aveg.SED ## 0.03192336 0.03192336 0.03192336 ## ## $LSD ## Max.LSD Min.LSD Aveg.LSD ## 0.06306 0.06306 0.06306 ## attr(,&quot;Significant level&quot;) ## [1] 0.05 ## attr(,&quot;Degree of freedom&quot;) ## [1] 156 ## ## $`Pairwise LSDs` ## 0 5 10 15 20 25 30 ## 0 0.00000 0.05842 0.08021 0.11086 0.14925 0.14464 0.20498 ## 5 0.06306 0.00000 0.02178 0.05244 0.09083 0.08621 0.14656 ## 10 0.06306 0.06306 0.00000 0.03066 0.06904 0.06443 0.12478 ## 15 0.06306 0.06306 0.06306 0.00000 0.03839 0.03378 0.09412 ## 20 0.06306 0.06306 0.06306 0.06306 0.00000 -0.00461 0.05573 ## 25 0.06306 0.06306 0.06306 0.06306 0.06306 0.00000 0.06034 ## 30 0.06306 0.06306 0.06306 0.06306 0.06306 0.06306 0.00000 ## attr(,&quot;Significant level&quot;) ## [1] 0.05 ## attr(,&quot;Degree of freedom&quot;) ## [1] 156 ## attr(,&quot;Note&quot;) ## [1] &quot;LSDs matrix has mean differences (row-col) above the diagonal, LSDs (adjusted by &#39;none&#39; method) below the diagonal&quot; ## ## $`Pairwise p-value` ## 0 5 10 15 20 25 30 ## 0 0.0000 1.8301 2.5124 3.4727 4.6753 4.5308 6.4211 ## 5 0.0691 0.0000 0.6823 1.6426 2.8451 2.7007 4.5909 ## 10 0.0130 0.4961 0.0000 0.9603 2.1628 2.0184 3.9086 ## 15 0.0007 0.1025 0.3384 0.0000 1.2025 1.0581 2.9483 ## 20 0.0000 0.0050 0.0321 0.2310 0.0000 -0.1445 1.7458 ## 25 0.0000 0.0077 0.0453 0.2917 0.8853 0.0000 1.8903 ## 30 0.0000 0.0000 0.0001 0.0037 0.0828 0.0606 0.0000 ## attr(,&quot;Degree of freedom&quot;) ## [1] 156 ## attr(,&quot;Note&quot;) ## [1] &quot;The matrix has t-value above the diagonal, p-value (adjusted by &#39;none&#39; method) below the diagonal&quot; ## attr(,&quot;Letter-based representation of pairwise comparisons at significant level &#39;0.05&#39;&quot;) ## Treatment Mean Group ## 1 0 0.6469732 A ## 2 5 0.5885491 AB ## 3 10 0.5667679 B ## 4 15 0.5361116 BC ## 5 25 0.5023348 CD ## 6 20 0.4977232 CD ## 7 30 0.4419911 D ## ## $mean_table ## Time Predicted means Standard error Df LL of 95% CI UL of 95% CI ## 1 0 0.6469732 0.05089815 156 0.5464347 0.7475117 ## 2 5 0.5885491 0.05089815 156 0.4880106 0.6890876 ## 3 10 0.5667679 0.05089815 156 0.4662294 0.6673063 ## 4 15 0.5361116 0.05089815 156 0.4355731 0.6366501 ## 5 20 0.4977232 0.05089815 156 0.3971847 0.5982617 ## 6 25 0.5023348 0.05089815 156 0.4017963 0.6028733 ## 7 30 0.4419911 0.05089815 156 0.3414526 0.5425296 9.4 Repeated measures designs as split-plots in time In this set of balanced data we have + Same number of observations on each rat + At the same time points We could analyse as a split-plot design, where + plots = different rats + subplots = different times within a rat Here we would have two error components + Between rats + Between times within rats There are, however, two big differences between Repeated Measures and Split-plot Designs 1. Cannot randomise the levels of time + Measurement 2 hours, comes after that at 1 hour 2. Split plot design block structure + Imposes the same correlation on any two subplots in the the same plot + Implausible for repeated measures designs as observations further apart in time likely to be less correlated than observations close together "]]
