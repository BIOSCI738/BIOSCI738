## Permutation and randomisation tests

A **permutation test** is a statistical significance test where the distribution of the test statistic under the null hypothesis is obtained by calculating **all** possible values of the test statistic under **all** possible rearrangements of the observed data points. This is also known as an **exact** test.

Now, often finding *all* possible combinations is hugely computationally expensive so we harness the power of simulation and carry out a **randomisation test**, which **randomly simulates** (for a given number of repetitions) possible values of the test statistic under the null hypothesis to obtain its approximate distribution.

The basic approach to a randomisation tests is straightforward:

 1. Choose a statistic to measure the effect in question (e.g., differences between group means)
 2. Calculate that test statistic on the observed data. Note this metric can be **anything** you wish
 3. Construct the sampling distribution that this statistic would have if the effect were not present in the population (i.e.,
 the distribution under the *Null* hypothesis, $H_0$): For chosen number of times
    + shuffle the data labels
    + calculate the test statistic for the reshuffled data and retain
 4. Find the location of your observed statistic in the sampling distribution. The location of observed statistic in sampling distribution is informative:
   + if in the main body of the distribution then the observed statistic could easily have occurred by chance
   + if in the tail of the distribution then the observed statistic would rarely occur by chance and there is evidence that something other than chance is operating.
 5. Calculate the proportion of times your reshuffled statistics equal or exceed the observed. This *p-value* is the probability that we observe a statistic at least as “extreme” as the one we observed **State** the strength of evidence against the null on the basis of this **probability**.
 
<div class="alert alert-warning">
  <strong>TASK</strong>  Study [this cheatsheet](https://github.com/BIOSCI738/cowstats/blob/main/img/randomisation.png?raw=true) and link the relevant sections to each step given above.
</div>

### A permutation test: Jackal mandible lengths

Below are data specifying mandible lengths (mm) for golden jackals (*Canis aureus*) of each sex from the British Museum

```{r, echo = FALSE}
## Mandible lengths (mm) for  golden jackals (Canis aureus) of each sex from the British Museum
jackal <- data.frame(mandible_length_mm = c(120, 107, 110, 116, 114, 111, 113, 117, 114, 112,
                                            110, 111, 107, 108, 110, 105, 107, 106, 111, 111),
                     sex = rep(c("Male","Female"), each = 10)) 
kableExtra::kable(jackal, col.names = c("Mandible length (mm)", "Biological sex")) %>%
  kableExtra::kable_classic(full_width = FALSE, html_font = "Cambria")
```

```{r, echo = FALSE}
ggplot(jackal, aes(x = sex, y = mandible_length_mm, pch = sex)) +
  geom_violin() +
  geom_jitter(alpha = 0.5) +
  xlab("Biological Sex") +
  ylab("Mandible length (mm)") +
  ggtitle("Mandible lengths for golden jackals (Canis aureus) ") +
  theme_linedraw() + theme(legend.title = element_blank())
```


**Scientific question**: Are the jaw lengths of jackals the same, on average, in both sexes?

**Null hypothesis**: The average jaw lengths in male jackals the same as for females

**Test statistic**: Difference of sample means


Let us first calculate the observed test statistic:


```{r}
## the data
jackal <- data.frame(mandible_length_mm = c(120, 107, 110, 116, 114, 111, 113, 117, 114, 112,
                                            110, 111, 107, 108, 110, 105, 107, 106, 111, 111),
                     sex = rep(c("Male","Female"), each = 10))
## observed statistic
jackal_mean_diff <- jackal %>%
  group_by(sex) %>%
  summarise(mean = mean(mandible_length_mm)) %>% 
  summarise(diff = diff(mean)) %>%
  as.numeric()
jackal_mean_diff
```

Now we use the command `combn(x, m)` to generate all possible combinations of the elements of `x` taken `m` at a time. For our data we have 20 elements in total with two groups of 10 elements each, therefore, we use `combn(20,10)`. We can use these combinations to generate **all** possible combinations and calculate the proportion of times the test statistic calculated under the null hypothesis is *as least as extreme* as the one observed (the p-value):


```{r}
combinations <- combn(20,10)
## Do the permutations
permtest_combinations <- apply(combinations, 2, function(x)
  mean(jackal$mandible_length_mm[x]) - mean(jackal$mandible_length_mm[-x]))
## Full Permutation test p.value
p_val <- length(permtest_combinations[abs(permtest_combinations) >= jackal_mean_diff]) / choose(20,10)
p_val
```


```{r, echo = FALSE}
ggplot(data.frame(x = permtest_combinations), aes(x = x)) +
  geom_histogram() +
  theme_classic() + ylab("") + xlab("") +
  geom_vline(xintercept = jackal_mean_diff, col = "cyan4", size = 1,alpha = 0.6) +
  annotate(geom = 'text', label = "Observed difference \n between means" , 
           x = 2, y = Inf, hjust = 0, vjust = 1.5, color = "cyan4") +
  ggtitle("Distribution of the test statistic under H0")

```


Rather than considering **all** possible combinations we might rather use 100 random permutations under the null hypothesis. Here, we sample **without** replacement:

```{r}
## set up matrix
random_perm <- apply(matrix(0, nrow = 99, ncol = 1), 1, function(x) sample(20))
random_mean_diff <- apply(random_perm, 2, function(x){
  z <- jackal$mandible_length_mm[x]
  mean(z[jackal$sex == "Male"]) - mean(z[jackal$sex == "Female"])
})
## add the observed 
random_mean_diff <- c(random_mean_diff, jackal_mean_diff)
random_p.value <- length(random_mean_diff[abs(random_mean_diff) >= jackal_mean_diff]) / 100 ## note the abs()
random_p.value
```

How does this compare to the exact (permutation) results above? Try increasing the number of times you randomise? What happens as you increase it?

#### A randomisation test:  Pāua shell length


Remember the Pāua data from the previous module? The dataset contains the following variables

 + `Age` of  P$\overline{\text{a}}$ua in years (calculated from counting rings in the cone) 
 + `Length` of  P$\overline{\text{a}}$ua shell in centimeters
 + `Species` of  P$\overline{\text{a}}$ua: *Haliotis iris* (typically found in NZ) and *Haliotis australis* (less commonly found in NZ) 
 
One question we may want to ask is if on average the shell length differs between Species? 

```{r, echo = TRUE, message = FALSE}
library(tidyverse)
paua <- read_csv("https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/paua.csv")
```

**Scientific question**: Are the shell lengths of shells the same in both species?
**Null hypothesis**: The distribution of shell lengths in *Haliotis iris* the same as in *Haliotis australis*
**Test statistic**: Difference of sample means


```{r, echo = FALSE}
means <- paua %>% group_by(Species) %>% summarise(means = mean(Length))
ggplot(paua,aes(x = Species, y = Length)) + 
  geom_violin() +
  geom_point(alpha = 0.4) +
  ylab("Length (cms)") + xlab("") +
  theme_classic() +
  geom_point(data = means, aes(x = Species, y = means, color = Species), size = 2) +
  geom_hline(data = means, aes(yintercept = means, color = Species), lty = 2, alpha = 0.5) +
  theme(legend.position = "none") +
  geom_text(data = means, aes(x = Species, y = means + 0.3, label = paste0("Species averege = ",round(means,3)), color = Species)) + scale_color_brewer(type = "div", palette = "Dark2")
  
```

But because the data are skewed and we've likely got non-constant variances we may be better off adopting a randomization test (this time using a `for` loop!), rather than a parametric t-test

#### 1. Choose a statistic that measures the effect of interest (in this case the differences between means).


```{r}
## observed differences in means
diff_in_means <- (paua %>% group_by(Species) %>%
                    summarise(mean = mean(Length)) %>% 
                    summarise(diff = diff(mean)))$diff
diff_in_means
```

#### 2. Construct the sampling distribution that this statistic would have if the effect were not present in the population (i.e., the distribution under $H_0$) .

```{r}
## Number of times I want to randomise
nreps <- 999   
## initialize empty array to hold results
randomisation_difference_mean <- numeric(nreps)
set.seed(1234) ## *****Remove this line for actual analyses*****
## This means that each run with produce the same results and
## agree with the printout that I show.

for (i in 1:nreps) {
  ## the observations
  data <- data.frame(value = paua$Length)
  ##  randomise labels
  data$random_labels <-sample(paua$Species, replace = FALSE)
  ## randomised differences in mean
  randomisation_difference_mean[i] <- data %>% 
    group_by(random_labels) %>% summarise(mean = mean(value)) %>% 
    summarise(diff = diff(mean)) %>%
    as.numeric()
}
## results, join randomised stats with the observed value
results <- data.frame(results = c(randomisation_difference_mean, diff_in_means))
```

#### 3. Locate the observed statistic (i.e., from our observed random sample) in the sampling distribution


Let's calculate how many randomised differences in means are *as least as extreme* as the one we observed. Note that we library the absolute value as dealing with a two tailed test.

```{r pval}
n_exceed <- sum(abs(results$results) >= abs(diff_in_means))
n_exceed
## proportion
n_exceed/nreps
```

What do you conclude from this proportion? How does this tie in with the distribution of the test statistic under the null hypothesis shown below?

```{r hist, echo = FALSE}
ggplot(results, aes(x = results)) +
  geom_histogram() +
  theme_classic() + ylab("") + xlab("Differences between randomised group means") +
  geom_vline(xintercept = diff_in_means, col = "cyan4", size = 1,alpha = 0.6) +
  annotate(geom = 'text', label = "Observed difference \n between means" , 
           x = -0.75, y = Inf, hjust = 0, vjust = 1.5, color = "cyan4")

```

**NOTE: We can extend the randomization test to make inference about any sample statistic (not just the mean)**

## Bootstrap resampling

A sampling distribution shows us what would happen if we took very many samples under the same conditions. The bootstrap is a procedure for finding the (approximate) sampling distribution from just one sample.

The original sample represents the **distribution of the population** from which it was drawn. Resamples, taken **with replacement** from the original sample are representative of what we would get from drawing many samples from the population (the distribution of the statistics calculated from each resample is known as the *bootstrap distribution* of the statistic). The bootstrap distribution of a statistic represents that statistic's **sampling distribution**.

<div class="alert alert-warning">
  <strong>TASK</strong>  Study [this cheatsheet](https://github.com/BIOSCI738/cowstats/blob/main/img/bootstrap.png?raw=true) and link the relevant sections to each step given above.
</div>

### Example: constructing bootstrap confidence intervals

**Old faithful** is a gyser located in Yellowstone National Park, Wyoming. Below is a histogram of the duration of 299 consecutive eruptions. Clearly the distribution is bimodal (has two modes)!

```{r,message = FALSE, warning = FALSE}
library(tidyverse)
ggplot(data = MASS::geyser, aes(x = duration)) +
  geom_histogram() +
  xlab("Duration of eruptions (m)") +
  theme_linedraw()

```

**Step 1:** Calculating the observed mean eruption duration time:

```{r}
mean <- MASS::geyser %>%
  summarise(mean = mean(duration))
mean

```

**Step 2:** Construct bootstrap distribution

```{r, message = FALSE, warning = FALSE}
## Number of times I want to bootstrap
nreps <- 1000   
## initialize empty array to hold results
bootstrap_means <- numeric(nreps)
set.seed(1234) ## *****Remove this line for actual analyses*****
## This means that each run with produce the same results and
## agree with the printout that I show.
for (i in 1:nreps) {
  ## bootstrap. note with replacement
  bootstrap_sample <- sample(MASS::geyser$duration, replace = TRUE)
  ##  bootstrapped mean resample
  bootstrap_means[i] <- mean(bootstrap_sample)
}
```

**Step 3:** Collate the sampling distribution


```{r, message = FALSE, warning = FALSE}
## results
results <- data.frame(bootstrap_means = bootstrap_means)
ggplot(data = results, aes(x = bootstrap_means)) +
  geom_histogram() +
  geom_vline(xintercept = as.numeric(mean)) +
  ggtitle("Sampling distribution of the mean") +
  xlab("Bootstrap means")  + ylab("") + theme_classic()
```


**Step 4:** Calculate quantities of interest from the sampling distribution


   1. The **bootstrap estimate of bias** is the difference between the mean of the bootstrap distribution and the value of the statistic in the original sample:

```{r}
bias <- as.numeric(mean) - mean(results$bootstrap_means)
bias

```

  2. The **bootstrap standard error of a statistic** is the standard deviation of its bootstrap distribution:

```{r}
sd(results$bootstrap_means)
## compare to SEM of original data
 MASS::geyser %>%
  summarise(sem = sd(duration)/sqrt(length(duration)))
```

  3. A **Bootstrap $t$ confidence interval.** If, for a sample of size $n$, the bootstrap distribution is approximately Normal and the estimate of bias is small then an approximate $C$ confidence for the parameter corresponding to the statistic is:

$$\text{statistic} \pm t^* \text{SE}_\text{bootstrap}$$ where $t*$ is the critical value of the $t_{n-1}$ distribution with area $C$ between $-t^*$ and $t^*$. 

For $C = 0.95$:

```{r}
as.numeric(mean) + c(-1,1) * qt(0.975,298)*sd(results$bootstrap_means)
```

So our 95% confidence interval is `r round(as.numeric(mean) - qt(0.975,298)*sd(results$bootstrap_means),1)` to `r round(as.numeric(mean) + qt(0.975,298)*sd(results$bootstrap_means),1)`.

  4. A **bootstrap $percentile$ confidence interval**. Use the bootstrap distribution itself to determine the limits of the confidence interval by taking the limits of the sorted, central $C$ bulk of the distribution. For $C = 0.95$:

```{r}
sort(results$bootstrap_means)[c(25,975)]
```



<div class="alert alert-info">

![](https://github.com/BIOSCI738/cowstats/blob/main/img/code_tag.png?raw=true){width=10%}

Resampling procedures, the differences

Resampling methods are any of a variety of methods for doing one of the following

1. **Estimating the precision of sample statistics** (e.g., bootstrapping)
2. **Performing significance tests** (e.g., permutation/exact/randomisation tests)
3. **Validating models** (e.g., bootstrapping, cross validation)

**Permutation vs bootstrap test**

+ The permutation test exploits symmetry under the null hypothesis. 

+ A full permutation test p-value is exact, conditional on data values in the combined sample.

+ A bootstrap estimates the probability mechanism that generated the samples under the null hypothesis.

+ A bootstrap does **not** rely on any special symmetry or assumption or exchangeability.

</div>

