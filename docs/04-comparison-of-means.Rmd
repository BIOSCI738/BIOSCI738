# Introduction to the design and analysis of experiments

## Learning Objectives


   + **Identify** the following
      + experimental unit
      + observational units
   + **List** and **describe** the three main principals of experimental design
       + Randomization
       + Replication
       + Blocking
   + **Calculate** Sums of Squares (between and within groups) given the observations 
   + **Define** and **state** the appropriate degrees of freedom in a one-way ANOVA scenario
   + **Calculate** the F-statistics given the appropriate Sums of Squares and degrees of freedom
   + **Interpret** and **discuss** a given *p-value* in the context of a stated hypothesis test
   + **Explain** between group and within group variation
   + **Describe** a Completely Randomised (experimental) Design 
   + **Carry** out linear regression in `R` with one categorical explanatory variable (one-way ANOVA) and **draw** the appropriate inference
   + **Communicate** statistical concepts and experimental outcomes clearly using language appropriate for both a **scientific** and **non-scientific** audience

   
##  Key phrases


**Experimental unit** Smallest portion of experimental material which is *independently* perturbed
 

**Treatment** The experimental condition *independently* applied to an experimental unit


**Observational unit** The smallest unit on which a response is measured. If one measurement is made on each rat: **Observational unit** = **Experimental unit**. If Multiple measurements are made on each rat: Each experimental unit has >1 observational unit (*pseudo-* or *technical replication*).


## Three key principles:


### **Replication**

+ **Biological replication:** each treatment is *independently* applied to each of several humans, animals or plants
  + To generalize results to population


+ **Technical replication:** two or more samples from the same biological source which are *independently* processed
  + Advantageous if processing steps introduce a lot of variation
  + Increases the precision with which comparisons of relative abundances between treatments are made


+ **Pseudo-replication:** one sample from the same biological source, divided into two or more aliquots which are **independently** measured
  + Advantageous for noisy measuring instruments
  + Increases the **precision** with which comparisons of relative abundances between treatments are made
  
### **Randomization**

+ **Protects against bias**


+ Plan the experiment in such a way that the variations caused by extraneous factors can all be combined under the general heading of "chance".

+ Ensures that each treatment has the same probability of getting good (or bad) units and thus
avoids systematic bias
+ random allocation can cancel out population bias; it ensures that any other possible causes for the experimental results are split equally between groups
+ typically statistical analysis assumes that observations are **independent**. This is almost never strictly true in practice but randomisation means that our estimates will behave as if they were based on independent observations


### **Blocking**

+ Blocking helps **control variability** by making treatment groups more alike. Experimental units are divided into subsets (called blocks) so that units within the same block are more similar than units from different subsets or blocks. 

+ Blocking is a technique for dealing with *nuisance factors*. A *nuisance factor* is a factor that has some effect on the response, but is of no interest (e.g., age class).


## One-Way **An**alysis **o**f **Va**riance (ANOVA)

```{r data-quiet, message = FALSE, echo = FALSE}
library(tidyverse)
rats <- read_csv("../data/crd_rats_data.csv")
rats$Surgery <- as_factor(rats$Surgery)
```

### Between group SS (SSB)

**The idea**: Assess **distances** between treatment (*surgical condition*) means relative to our uncertainty about the actual (*true*) treatment means.


```{r, echo = FALSE}
means <- rats %>% group_by(Surgery) %>% summarise(avg = mean(logAUC))
mean <- mean(rats$logAUC)
means$ends <- mean
ggplot(rats, aes(x = Surgery, y = logAUC)) + 
    geom_violin()  + 
  ylab("logAUC") +
  xlab("Treatment") +
  geom_point(data = means, aes(x = Surgery, y = avg, color = Surgery), size = 2) +
  geom_text(data = means, aes(x = Surgery, y = avg + 0.25, color = Surgery, label = paste0("Treatment mean = ",round(avg,3)))) +
  geom_hline(data = means, aes(yintercept = avg, color = Surgery), alpha = 0.3, lty = 2) +
  geom_hline(yintercept = mean, color = "red", alpha = 0.3) +
  annotate(geom = 'text', label = paste0("Overall average = ",round(mean,3)) , 
           x = -Inf, y = Inf, hjust = 0, vjust = 1.5, color = "red") +
  geom_segment(data = means, aes(x = Surgery, y = avg, xend = Surgery, yend = ends,color = Surgery), size = 1) +
  geom_text(data = means, aes(x = Surgery, y = ends + 0.25, color = Surgery, label = paste0("diff to overall = ",round(avg - ends,3))))
  
  
```


**add up the differences:** `r round(means$avg[1] - mean,3)` + `r round(means$avg[2] - mean,3)` + `r round(means$avg[3] - mean,3)` = `r round(sum(means$avg - mean),3)`. **This is always the case!**

**So adding up the differences:** `r round(means$avg[1] - mean,3)` + `r round(means$avg[2] - mean,3)` + `r round(means$avg[3] - mean,3)` = `r round(sum(means$avg - mean),3)`. **Not a great way to measure distances!**


**Sums of Squares?** 

$`r round(means$avg[1] - mean,3)`^2 + `r round(means$avg[2] - mean,3)`^2 + `r round(means$avg[3] - mean,3)`^2$

**add up the squared differences?** but... there are 4 observations in each group (treatment)


$4\times(`r round(means$avg[1] - mean,3)`)^2 + 4\times(`r round(means$avg[2] - mean,3)`)^2 + 4\times(`r round(means$avg[3] - mean,3)`)^2$

This is the **Between Groups Sums of Squares** or the **Between group SS (SSB)** 


So the Between group SS (SSB) = `r sum(4*((means$avg - mean)^2))`


**Adding up the differences:** `r round(means$avg[1] - mean,3)` + `r round(means$avg[2] - mean,3)` + `r round(means$avg[3] - mean,3)` = `r round(sum(means$avg - mean),3)`. **This is always the case** and that itself gives us information...


**We only need to know two of the values to work out the third!**

So we have only 2 bits of **unique** information; **SSB degrees of freedom** = 2

### Within group SS (SSW)

The **Within group SS (SSW)** arises from the same idea:

To assess distances between treatment (surgical condition) means **relative** to our uncertainty about the actual (true) treatment means.

Procedure:

 + Observation - Treatment mean
 + Square the difference
 + Add them up!

**Within group SS (SSW)** *unexplained variance*

```{r,echo = FALSE}
rats_df <- rats %>% mutate(ov_avg = mean(logAUC)) %>% 
  group_by(Surgery) %>% mutate(tr_avg = mean(logAUC), tr_avg_minus_ov_avg = mean(logAUC) - ov_avg,
                               obvs_minus_tr_avg = logAUC - mean(logAUC))
jit <- ggplot() + 
  geom_jitter(data = rats, aes(x = Surgery, y = logAUC))

rats_df$x_points <- layer_data(jit)$x
rats_df$y_points <- layer_data(jit)$y

ggplot() + 
  ylab("logAUC") +
  xlab("Treatment") +
  geom_point(data = means, aes(x = Surgery, y = avg, color = Surgery), size = 2) +
  geom_text(data = means, aes(x = Surgery, y = avg + 0.25, color = Surgery, 
                              label = paste0("Treatment mean = ",round(avg,3)))) +
  geom_hline(data = means, aes(yintercept = avg, color = Surgery), alpha = 0.3, lty = 2) +
  geom_point(data = rats_df, aes(x = x_points, y = y_points, color = Surgery), alpha = 0.3) +
  geom_segment(data = rats_df, aes(x = x_points, y = y_points, 
                                   xend = x_points, yend = tr_avg,color = Surgery), 
               size = 1, alpha = 0.5) +
    theme(legend.position = "none")
```

### F-statistic

Recall the Between group SS (**SSB**) = `r sum(4*((means$avg - mean)^2))`

So mean **SSB** =  `r sum(4*((means$avg - mean)^2))` / 2


The within group SS (**SSW**) = `r sum(rats_df$obvs_minus_tr_avg^2)`

Here we have $2\times 3$ bits of *unique* information: within groups **degrees of freedom** is 9.

So mean **SSW** = `r round(sum(rats_df$obvs_minus_tr_avg^2),3)`/9


Consider the ratio ${\frac  {{\text{variation due to treatments}}}{{\text{unexplained variance}}}} = {\frac  {{\text{ mean between-group variability}}}{{\text{mean within-group variability}}}}$  $=\frac{\text{mean SSB}}{\text{mean SSW}}$ $=\frac{\text{MSB}}{\text{MSW}}$  = $=\frac{\text{experimental variance}}{\text{error variance}}$ `r (sum(4*((means$avg - mean)^2))/2)/(sum(rats_df$obvs_minus_tr_avg^2)/9)`

This is the **F-statistic**... 

## Analysis of a Completely Randomised Design in `R`: `aov()` and `lm()`

```{r data, message = FALSE, eval = FALSE}
library(tidyverse)
rats <- read_csv("crd_rats_data.csv")
```

```{r means}
rats %>%
  group_by(Surgery) %>%
  summarise(avg = mean(logAUC))
```

### `aov()`

```{r aov}
rats_aov <- aov(logAUC ~ Surgery, data = rats)
```

**Inference**

```{r lm-quiet, echo = FALSE}
rats_lm <- lm(logAUC ~ Surgery, data = rats)
```

Hypothesis: We test the Null hypothesis, $H_0$, population (`Surgery`) means are the same on average verses the alternative hypothesis, $H_1$, that **at least one** differs from the others!

Probability of getting an **F-statistic** at least as extreme as the one we observe (think of the area under the tails of the curve below) **p-value** Pr(>F)= `r round(anova(rats_lm)$"Pr(>F)"[1],4)` tells us we have sufficient evidence to reject $H_0$ at the 1% level of significance


```{r f, echo = FALSE}
stats_df <- data.frame(Fs = summary(rats_aov)[[1]][[4]][[1]])
crit_df <- as.data.frame(apply(stats_df,2,rep,each = 100))
crit_df$Fs <- c(sapply(stats_df$Fs, function(x) seq(x,20, length.out = 100)))
crit_df$y <- df(crit_df$Fs,df1 = 2,df2 = 9) ## corresponding F val
ggplot(stats_df,aes(x = Fs)) +
  geom_vline(aes(xintercept = Fs)) +
    geom_line(data = data.frame(x = seq(0,20,.1),
                              y  = df(seq(0,20,.1),df1 = 2,df2 = 9)),
              aes(x = x,y = y), alpha = 0.7) +
    theme(legend.position = "none") +
    ylab("density") +
    xlab("F-value") +
    geom_area(data = crit_df,mapping = aes(x = Fs,y = y),fill = "blue") +
  geom_line(data = crit_df,mapping = aes(x = Fs,y = y),color = "blue") +
  geom_text(aes(4, 1,label = paste("F-statistic:", format(round(Fs, 2), nsmall = 2))),
              size = 4, hjust = 0, color = "black")  +
  geom_text(aes(4, 0.95,label = paste("p-value:", format(round(pf(Fs,2,9,lower.tail = FALSE),4), 
                                                         nsmall = 2))),
              size = 4, hjust = 0, color = "blue") 

```

### `lm()`

`lm()`

```{r lm}
rats_lm <- lm(logAUC ~ Surgery, data = rats)
```


**Inference**

```{r lmsum}
summary(rats_lm)$coef
```


```{r, echo = FALSE}
means$base <- summary(rats_lm)$coef[1,1]
ggplot(rats, aes(x = Surgery, y = logAUC)) + 
    geom_violin()  + 
  ylab("logAUC") +
  xlab("Treatment") +
  geom_point(data = means, aes(x = Surgery, y = avg, color = Surgery), size = 2) +
  geom_text(data = means, aes(x = Surgery, y = avg + 0.25, color = Surgery, label = paste0("Treatment mean = ",round(avg,3)))) +
  geom_hline(data = means, aes(yintercept = avg, color = Surgery), alpha = 0.3, lty = 2) +
  geom_segment(data = means[2:3,], aes(x = Surgery, y = avg, xend = Surgery, yend = base,color = Surgery), size = 1) +
  geom_text(data = means[2:3,], aes(x = Surgery, y = base - 0.25, color = Surgery, label = paste0("diff to baseline = ",round(avg - base,3)))) +
  geom_hline(data = means[1,], aes(yintercept = avg, color = Surgery)) +
  geom_text(data = means[1,],aes(x = Surgery, y = base - 0.25, color = Surgery, label = paste0("Baseline = ",round(avg,3))))
  
```

Which pairs of means are different?

 + Pair-wise comparisons of means
  	+ Use two-sample t-tests
 	+ We need to calculate our **observed**  t-value where
$\text{t-value} = \frac{\text{Sample Difference}_{ij} - \text{Difference assuming } H_0 \text{ is true}_{ij}}{\text{SE of } \text{Sample Difference}_{ij}}$
 where
   $\text{Sample Difference}_{ij}$ = Difference between pair of sample means
 + Compute the p-value for observed t-value



```{r lmsum2}
summary(rats_lm)$coef
```


(Intercept) = $\text{mean}_C$ = `r summary(rats_lm)$coef[1,1]`

SE of (Intercept) = SE of $\text{mean}_C$ = SEM = `r summary(rats_lm)$coef[1,2]`

$\text{Surgery}_P$ = $\text{mean}_P$ – $\text{mean}_C$ = `r summary(rats_lm)$coef[2,1]`

SE of $\text{Surgery}_P$ = SE of ($\text{mean}_P$ - $\text{mean}_C$ ) = SED = `r summary(rats_lm)$coef[2,2]`


**Hypotheses being tested**

+ The t value and Pr (>|t|) are the t - and p-value for testing the null hypotheses:
	+ Mean abundance is zero for C population
	+ No difference between the population means of P and C
	+ No difference between the population means of S and C

We're interested in 2 and 3, but not necessarily 1!

 Two-sample t -tests for pairwise comparisons of means
 
+ SurgeryP : t value = Estimate ÷ Std.Error =  0.8446; Pr (>|t|) =  0.4202
	

#### Diagnostic plots
```{r,echo = FALSE}
options(warn=-1)
```

```{r qqnorm, warning=FALSE, message=FALSE}
gglm::gglm(rats_lm) # Plot the four main diagnostic plots
```

**Residuals vs Fitted** plot

You are basically looking for no pattern or structure in your residuals (e.g., a "starry" night). You definitely don't want to see is the scatter increasing around the zero line (dashed line) as the fitted values get bigger (e.g., think of a trumpet, a wedge of cheese, or even a slice of pizza) which would indicate unequal variances (heteroscedacity). 

**Normal quantile-quantile (QQ)** plot

This plot shows the sorted residuals versus expected order statistics from a standard normal distribution. Samples should be close to a line; points moving away from 45 degree line at the tails suggest the data are from a skewed distribution.


**Scale-Location** plot ($\sqrt{\text{|standardized residuals vs Fitted|}}$)

Another way to check the homoskedasticity (constant-variance) assumption. We want the  line to be roughly horizontal. If this is the case then the average magnitude of the standardized residuals isn't changing much as a function of the fitted values. We'd also like the spread around the  line not to vary much with the fitted values; then the variability of magnitudes doesn't vary much as a function of the fitted values.

**Residuals vs Leverage** plot (standardized residuals vs Leverage)

This can help detect outliers in a linear regression model. For linear regression model leverage measures how sensitive a fitted value is to a change in the true response. We're looking at how the spread of standardized residuals changes as the leverage. This can also be used to detect heteroskedasticity and non-linearity: the spread of standardized residuals shouldn't change as a function of leverage. In addition, points with high leverage may be influential: that is, deleting them would change the model a lot. 

##  `r emo::ji('scream')` p-values `r emo::ji('scream')`

**[The ASA Statement on p-Values: Context, Process, and Purpose](https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108)**

> "Good statistical practice, as an essential component of good scientific practice, emphasizes principles of good study design and conduct, a variety of numerical and graphical summaries of data, understanding of the phenomenon under study, interpretation of results in context, complete reporting and proper logical and quantitative understanding of what data summaries mean. No single index should substitute for scientific reasoning." `r tufte::quote_footer('--- ASA Statement on p-Values')`


**What is a p-Value?**

Informally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value

+ **p-values** can indicate how incompatible the data are with a specified statistical model

+ p-values **do not** measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone

+ Scientific conclusions and business or policy decisions **should not** be based only on whether a p-value passes a specific threshold

+ Proper inference requires **full** reporting and transparency

+ A p-value, or statistical significance, does **not** measure the size of an effect or the importance of a result

+ By itself, a p-value does **not** provide a good measure of evidence regarding a model or hypothesis


## Terminology and issues


**Type I** error (false positive): declare a difference (i.e., reject $H_0$) when there is no difference (i.e. $H_0$ is true). Risk of the Type I error is determined by the *level of significance* (which we set!) (i.e., $\alpha =\text{ P(Type I error)} = \text{P(false positive)}$.

![](https://raw.githubusercontent.com/allisonhorst/stats-illustrations/master/other-stats-artwork/type_1_errors.png)

**Type II** error (false negative): difference not declared (i.e., $H_0$ not rejected) when there is a difference (i.e., $H_0$ is false). Let $\beta =$ P(do not reject $H_0$ when $H_0$ is false); so, $1-\beta$ = P(reject $H_0$ when $H_0$ is false) = P(a true positive), which is the statistical **power** of the test.

![](https://raw.githubusercontent.com/allisonhorst/stats-illustrations/master/other-stats-artwork/type_2_errors.png) 


**Each** time we carry out a hypothesis test the probability we get a false positive result (type I error) is given by $\alpha$ (the *level of significance* we choose).

When we have **multiple comparisons** to make we should then control the **Type I** error rate across the entire *family* of tests under consideration, i.e., control the Family-Wise Error Rate (FWER); this ensures that the risk of making at least one **Type I** error among the family of comparisons in the experiment is $\alpha$.


|State of Nature  | Don't reject $H_0$ | reject $H_0$ |
|---              |---                |---            |
| $H_0$ is true |  `r emo::ji("check")` | Type I error  |
| $H_0$ is false  | Type II error  | `r emo::ji("check")` |


## Resources

![](https://pbs.twimg.com/media/EeMWb7QWsAIGftR?format=jpg&name=small)

**Looking forward**

| Traditional name    | Model formula  | R code  |
| ------------------- |:--------------:| -------:|
| Simple regression   | $Y \sim X_{continuous}$ | `lm(Y ~ X)` |
| One-way ANOVA       | $Y \sim X_{categorical}$      |   `lm(Y ~ X)` |
| Two-way ANOVA       | $Y \sim X1_{categorical} + X2_{categorical}$| `lm(Y ~ X1 + X2)` |
| ANCOVA              | $Y \sim X1_{continuous} + X2_{categorical}$ |`lm(Y ~ X1 + X2)` |
| Multiple regression | $Y \sim X1_{continuous} + X2_{continuous}$ | `lm(Y ~ X1 + X2)` |
| Factorial ANOVA     | $Y \sim X1_{categorical} * X2_{categorical}$|   `lm(Y ~ X1 * X2)` or `lm(Y ~ X1 + X2 + X1:X2)` |


[Glass, David J. Experimental Design for Biologists. Second ed. 2014. Print.](https://catalogue.library.auckland.ac.nz/primo-explore/fulldisplay?docid=uoa_alma21237737730002091&search_scope=Combined_Local&tab=books&vid=NEWUI&context=L)

[Welham, S. J. Statistical Methods in Biology : Design and Analysis of Experiments and Regression. 2015. Print.](https://catalogue.library.auckland.ac.nz/primo-explore/fulldisplay?docid=uoa_alma21237737830002091&search_scope=Combined_Local&tab=books&vid=NEWUI&context=L)

[Fisher, Ronald Aylmer. The Design of Experiments. 8th ed. Edinburgh: Oliver & Boyd, 1966. Print. O & B Paperbacks.](https://catalogue.library.auckland.ac.nz/primo-explore/fulldisplay?docid=uoa_alma21198532990002091&context=L&vid=NEWUI&lang=en_US&search_scope=Combined_Local&adaptor=Local%20Search%20Engine&isFrbr=true&tab=books&query=any,contains,The_Design_of_Experiments&sortby=date&facet=frbrgroupid,include,627497507&offset=0)
