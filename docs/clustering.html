<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Clustering | Advanced Biological Data Analysis</title>
  <meta name="description" content="Clustering | Advanced Biological Data Analysis" />
  <meta name="generator" content="bookdown 0.23 and GitBook 2.6.7" />

  <meta property="og:title" content="Clustering | Advanced Biological Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Clustering | Advanced Biological Data Analysis" />
  
  
  

<meta name="author" content="University of Auckland" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="module-6.html"/>
<link rel="next" href="tldr.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Nau mai, haere mai. Welcome to BIOSCI 738</a></li>
<li class="chapter" data-level="" data-path="useful-information-to-set-you-up-for-your-semester.html"><a href="useful-information-to-set-you-up-for-your-semester.html"><i class="fa fa-check"></i>Useful information to set you up for your semester</a><ul>
<li class="chapter" data-level="" data-path="useful-information-to-set-you-up-for-your-semester.html"><a href="useful-information-to-set-you-up-for-your-semester.html#course-outline"><i class="fa fa-check"></i>Course outline</a></li>
<li class="chapter" data-level="" data-path="useful-information-to-set-you-up-for-your-semester.html"><a href="useful-information-to-set-you-up-for-your-semester.html#learning-outcomes"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="" data-path="useful-information-to-set-you-up-for-your-semester.html"><a href="useful-information-to-set-you-up-for-your-semester.html#course-summary"><i class="fa fa-check"></i>Course summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="module-1.html"><a href="module-1.html"><i class="fa fa-check"></i>Module 1</a><ul>
<li><a href="r-and-rstudio.html#r-and-rstudio"><code>R</code> and <code>RStudio</code></a><ul>
<li class="chapter" data-level="" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html"><i class="fa fa-check"></i>Installing R and RStudio</a></li>
<li class="chapter" data-level="" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#getting-started"><i class="fa fa-check"></i>Getting started</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="reproducible-research.html"><a href="reproducible-research.html"><i class="fa fa-check"></i>Reproducible research</a><ul>
<li class="chapter" data-level="" data-path="reproducible-research.html"><a href="reproducible-research.html#project-oriented-workflow-good-practice"><i class="fa fa-check"></i>Project-oriented workflow: good practice</a></li>
<li><a href="reproducible-research.html#version-control-with-git-and-github">Version control with <code>git</code> and GitHub</a></li>
</ul></li>
<li><a href="your-data-and-r.html#your-data-and-r">Your data and <code>R</code></a><ul>
<li><a href="your-data-and-r.html#reading-in-data-from-a-.csv-file">Reading in data from a <code>.csv</code> file</a></li>
<li class="chapter" data-level="" data-path="your-data-and-r.html"><a href="your-data-and-r.html"><i class="fa fa-check"></i>Explore your data</a></li>
<li><a href="your-data-and-r.html#the-pipe-operator">The pipe operator <code>%&gt;%</code></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="data-wrangling.html"><a href="data-wrangling.html"><i class="fa fa-check"></i>Data wrangling</a><ul>
<li><a href="data-wrangling.html#introuducing-the-palmer-penguins">Introuducing the <span>Palmer penguins</span></a></li>
<li><a href="data-wrangling.html#common-dataframe-manipulations-in-the-tidyverse-using-dplyr-and-tidyr">Common dataframe manipulations in the <code>tidyverse</code>, using <code>dplyr</code> and <code>tidyr</code></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="data-visualiation-data-viz.html"><a href="data-visualiation-data-viz.html"><i class="fa fa-check"></i>Data visualiation (data viz)</a><ul>
<li class="chapter" data-level="" data-path="data-visualiation-data-viz.html"><a href="data-visualiation-data-viz.html#exploratory-and-explanatory-plots"><i class="fa fa-check"></i>Exploratory and explanatory plots</a></li>
<li class="chapter" data-level="" data-path="data-visualiation-data-viz.html"><a href="data-visualiation-data-viz.html#ten-simple-rules-for-better-figures"><i class="fa fa-check"></i>Ten Simple Rules for Better Figures</a></li>
<li><a href="data-visualiation-data-viz.html#ggplot2"><code>ggplot2</code></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="māori-data-sovereignty-principles.html"><a href="māori-data-sovereignty-principles.html"><i class="fa fa-check"></i>Māori Data Sovereignty principles</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="module-2.html"><a href="module-2.html"><i class="fa fa-check"></i>Module 2</a><ul>
<li class="chapter" data-level="" data-path="key-satistical-concepts.html"><a href="key-satistical-concepts.html"><i class="fa fa-check"></i>Key satistical concepts</a></li>
<li class="chapter" data-level="" data-path="bootstrap-resampling.html"><a href="bootstrap-resampling.html"><i class="fa fa-check"></i>Bootstrap resampling</a><ul>
<li class="chapter" data-level="" data-path="bootstrap-resampling.html"><a href="bootstrap-resampling.html#example-constructing-bootstrap-confidence-intervals"><i class="fa fa-check"></i>Example: constructing bootstrap confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="permutation-tests.html"><a href="permutation-tests.html"><i class="fa fa-check"></i>Permutation tests</a><ul>
<li><a href="permutation-tests.html#significance-testing-using-permutation-randomisation-tests">Significance testing using permutation (<em>randomisation</em>) tests</a></li>
<li class="chapter" data-level="" data-path="permutation-tests.html"><a href="permutation-tests.html#differences"><i class="fa fa-check"></i>Differences</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="p-values.html"><a href="p-values.html"><i class="fa fa-check"></i>😱 p-values 😱</a><ul>
<li class="chapter" data-level="" data-path="p-values.html"><a href="p-values.html#p-values-from-permutation-tests"><i class="fa fa-check"></i>P-values from permutation tests</a></li>
<li class="chapter" data-level="" data-path="p-values.html"><a href="p-values.html#asa-statement-on-p-values"><i class="fa fa-check"></i>ASA Statement on p-Values</a></li>
<li class="chapter" data-level="" data-path="p-values.html"><a href="p-values.html#type-i-and-type-ii-errors"><i class="fa fa-check"></i>Type I and Type II errors</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="one-and-two-sample-tests.html"><a href="one-and-two-sample-tests.html"><i class="fa fa-check"></i>One and two sample tests</a><ul>
<li class="chapter" data-level="" data-path="one-and-two-sample-tests.html"><a href="one-and-two-sample-tests.html#one-sample-t-test"><i class="fa fa-check"></i>One-Sample t-test</a></li>
<li class="chapter" data-level="" data-path="one-and-two-sample-tests.html"><a href="one-and-two-sample-tests.html#differences-between-two-means"><i class="fa fa-check"></i>Differences between two means</a></li>
<li class="chapter" data-level="" data-path="one-and-two-sample-tests.html"><a href="one-and-two-sample-tests.html#one-way-analysis-of-variance-anova"><i class="fa fa-check"></i>One-Way Analysis of Variance (ANOVA)</a></li>
<li class="chapter" data-level="" data-path="one-and-two-sample-tests.html"><a href="one-and-two-sample-tests.html#between-group-ss-ssb"><i class="fa fa-check"></i>Between group SS (SSB)</a></li>
<li class="chapter" data-level="" data-path="one-and-two-sample-tests.html"><a href="one-and-two-sample-tests.html#within-group-ss-ssw"><i class="fa fa-check"></i>Within group SS (SSW)</a></li>
<li class="chapter" data-level="" data-path="one-and-two-sample-tests.html"><a href="one-and-two-sample-tests.html#f-statistic"><i class="fa fa-check"></i>F-statistic</a></li>
<li class="chapter" data-level="" data-path="one-and-two-sample-tests.html"><a href="one-and-two-sample-tests.html#degrees-of-freedom-df"><i class="fa fa-check"></i>Degrees of freedom (DF)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i>Linear regression</a><ul>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#some-mathematical-notation"><i class="fa fa-check"></i>Some mathematical notation</a></li>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#modeling-bill-depth"><i class="fa fa-check"></i>Modeling Bill Depth</a></li>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#single-continuous-variable"><i class="fa fa-check"></i>Single continuous variable</a></li>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#one-factor-and-a-continous-variable"><i class="fa fa-check"></i>One factor and a continous variable</a></li>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#interactions"><i class="fa fa-check"></i>Interactions</a></li>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#other-possible-models"><i class="fa fa-check"></i>Other possible models</a></li>
</ul></li>
<li><a href="model-formula-syntax-in-r.html#model-formula-syntax-in-r">Model formula syntax in <code>R</code></a></li>
<li class="chapter" data-level="" data-path="model-diagnostocs.html"><a href="model-diagnostocs.html"><i class="fa fa-check"></i>Model diagnostocs</a><ul>
<li class="chapter" data-level="" data-path="model-diagnostocs.html"><a href="model-diagnostocs.html#marginal-predictions"><i class="fa fa-check"></i>Marginal predictions</a></li>
<li class="chapter" data-level="" data-path="model-diagnostocs.html"><a href="model-diagnostocs.html#model-selection"><i class="fa fa-check"></i>Model selection</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="point-predictions-and-confidence-intervals.html"><a href="point-predictions-and-confidence-intervals.html"><i class="fa fa-check"></i>Point predictions and confidence intervals</a><ul>
<li class="chapter" data-level="" data-path="point-predictions-and-confidence-intervals.html"><a href="point-predictions-and-confidence-intervals.html#confidence-intervals-for-parameters"><i class="fa fa-check"></i>Confidence intervals for parameters</a></li>
<li class="chapter" data-level="" data-path="point-predictions-and-confidence-intervals.html"><a href="point-predictions-and-confidence-intervals.html#point-prediction"><i class="fa fa-check"></i>Point prediction</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="module-3.html"><a href="module-3.html"><i class="fa fa-check"></i>Module 3</a><ul>
<li class="chapter" data-level="" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html"><i class="fa fa-check"></i>Introduction to the design and analysis of experiments</a><ul>
<li class="chapter" data-level="" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#key-phrases"><i class="fa fa-check"></i>Key phrases</a></li>
<li class="chapter" data-level="" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#the-three-key-principles"><i class="fa fa-check"></i>The three key principles:</a></li>
<li class="chapter" data-level="" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#replication"><i class="fa fa-check"></i>Replication</a></li>
<li class="chapter" data-level="" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#randomization"><i class="fa fa-check"></i>Randomization</a></li>
<li class="chapter" data-level="" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#blocking"><i class="fa fa-check"></i>Blocking</a></li>
<li class="chapter" data-level="" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#setting-up-an-experiment"><i class="fa fa-check"></i>Setting up an experiment</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="some-basic-experimental-designs.html"><a href="some-basic-experimental-designs.html"><i class="fa fa-check"></i>Some basic experimental designs</a><ul>
<li class="chapter" data-level="" data-path="some-basic-experimental-designs.html"><a href="some-basic-experimental-designs.html#completely-randomised-design-crd"><i class="fa fa-check"></i>Completely randomised design (CRD)</a></li>
<li class="chapter" data-level="" data-path="some-basic-experimental-designs.html"><a href="some-basic-experimental-designs.html#randomised-complete-block-design-rcbd"><i class="fa fa-check"></i>Randomised complete block design (RCBD)</a></li>
<li class="chapter" data-level="" data-path="some-basic-experimental-designs.html"><a href="some-basic-experimental-designs.html#factorial-design"><i class="fa fa-check"></i>Factorial design</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="modler-v.-designer.html"><a href="modler-v.-designer.html"><i class="fa fa-check"></i>Modler v. designer</a><ul>
<li class="chapter" data-level="" data-path="modler-v.-designer.html"><a href="modler-v.-designer.html#a-completely-randomised-design-crd"><i class="fa fa-check"></i>A completely randomised design (CRD)</a></li>
<li><a href="modler-v.-designer.html#analysis-of-a-completely-randomised-design-in-r">Analysis of a Completely Randomised Design in <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="factorial-experiments.html"><a href="factorial-experiments.html"><i class="fa fa-check"></i>Factorial experiments</a></li>
<li class="chapter" data-level="" data-path="factorial-design-as-a-crd.html"><a href="factorial-design-as-a-crd.html"><i class="fa fa-check"></i>Factorial design (as a CRD)</a><ul>
<li class="chapter" data-level="" data-path="factorial-design-as-a-crd.html"><a href="factorial-design-as-a-crd.html#equal-replications-balanced-design"><i class="fa fa-check"></i>Equal replications (balanced design)</a></li>
<li class="chapter" data-level="" data-path="factorial-design-as-a-crd.html"><a href="factorial-design-as-a-crd.html#unqual-replications-unbalanced-design"><i class="fa fa-check"></i>Unqual replications (unbalanced design)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html"><i class="fa fa-check"></i>Multiple comparisons</a><ul>
<li class="chapter" data-level="" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#classification-of-multiple-hypothesis-tests"><i class="fa fa-check"></i>Classification of multiple hypothesis tests</a></li>
<li><a href="multiple-comparisons.html#using-the-predictmeans-package">Using the <code>predictmeans</code> package</a></li>
<li class="chapter" data-level="" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#multiple-comparison-procedures"><i class="fa fa-check"></i>Multiple comparison procedures</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="linear-mixed-effect-models-lmms.html"><a href="linear-mixed-effect-models-lmms.html"><i class="fa fa-check"></i>Linear mixed-effect models (LMMs)</a><ul>
<li class="chapter" data-level="" data-path="linear-mixed-effect-models-lmms.html"><a href="linear-mixed-effect-models-lmms.html#fixed-or-random-effects"><i class="fa fa-check"></i>Fixed or Random effects???</a></li>
<li class="chapter" data-level="" data-path="linear-mixed-effect-models-lmms.html"><a href="linear-mixed-effect-models-lmms.html#a-randomised-controlled-block-design-rcbd"><i class="fa fa-check"></i>A Randomised Controlled Block Design (RCBD)</a></li>
<li><a href="linear-mixed-effect-models-lmms.html#analysis-using-lmer-from-lme4">Analysis using <code>lmer()</code> from <code>lme4</code></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="split-plot-and-repeated-measures-designs.html"><a href="split-plot-and-repeated-measures-designs.html"><i class="fa fa-check"></i>Split-plot and repeated measures designs</a><ul>
<li><a href="split-plot-and-repeated-measures-designs.html#using-aov">Using <code>aov()</code></a></li>
<li><a href="split-plot-and-repeated-measures-designs.html#using-lmer-from-lmetest-and-lmer4-and-predictmeans">Using <code>lmer()</code> (from <code>lmeTest</code> and <code>lmer4</code>) and <code>predictmeans()</code></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="analysis-of-a-repeated-measures-design.html"><a href="analysis-of-a-repeated-measures-design.html"><i class="fa fa-check"></i>Analysis of a repeated measures design</a><ul>
<li class="chapter" data-level="" data-path="analysis-of-a-repeated-measures-design.html"><a href="analysis-of-a-repeated-measures-design.html#the-data"><i class="fa fa-check"></i>The data</a></li>
<li class="chapter" data-level="" data-path="analysis-of-a-repeated-measures-design.html"><a href="analysis-of-a-repeated-measures-design.html#visualise"><i class="fa fa-check"></i>Visualise</a></li>
<li><a href="analysis-of-a-repeated-measures-design.html#using-aov-1">Using <code>aov()</code></a></li>
<li><a href="analysis-of-a-repeated-measures-design.html#using-lmer-from-lmertest-and-lme4-and-predictmeans">Using <code>lmer()</code> (from <code>lmerTest</code> and <code>lme4</code>) and <code>predictmeans()</code></a></li>
</ul></li>
<li><a href="repeated-measures-designs-as-split-plots-in-time.html#repeated-measures-designs-as-split-plots-in-time">Repeated measures designs as <em>split-plots in time</em></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="module-4.html"><a href="module-4.html"><i class="fa fa-check"></i>Module 4</a><ul>
<li class="chapter" data-level="" data-path="least-squares-estimation.html"><a href="least-squares-estimation.html"><i class="fa fa-check"></i>Least Squares Estimation</a><ul>
<li class="chapter" data-level="" data-path="least-squares-estimation.html"><a href="least-squares-estimation.html#some-basic-matrix-algebra"><i class="fa fa-check"></i>Some basic matrix algebra</a></li>
<li class="chapter" data-level="" data-path="least-squares-estimation.html"><a href="least-squares-estimation.html#matrix-representation-of-a-crd"><i class="fa fa-check"></i>Matrix representation of a CRD</a></li>
<li class="chapter" data-level="" data-path="least-squares-estimation.html"><a href="least-squares-estimation.html#a-numeric-example"><i class="fa fa-check"></i>A numeric example</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="maximum-likelkihood-estimation.html"><a href="maximum-likelkihood-estimation.html"><i class="fa fa-check"></i>Maximum likelkihood estimation</a><ul>
<li class="chapter" data-level="" data-path="maximum-likelkihood-estimation.html"><a href="maximum-likelkihood-estimation.html#differeniation-rules"><i class="fa fa-check"></i>Differeniation rules</a></li>
<li class="chapter" data-level="" data-path="maximum-likelkihood-estimation.html"><a href="maximum-likelkihood-estimation.html#logarithn-rules"><i class="fa fa-check"></i>Logarithn rules</a></li>
<li class="chapter" data-level="" data-path="maximum-likelkihood-estimation.html"><a href="maximum-likelkihood-estimation.html#maximum-likelihood"><i class="fa fa-check"></i>Maximum likelihood</a></li>
<li class="chapter" data-level="" data-path="maximum-likelkihood-estimation.html"><a href="maximum-likelkihood-estimation.html#the-likelihood"><i class="fa fa-check"></i>The Likelihood</a></li>
<li class="chapter" data-level="" data-path="maximum-likelkihood-estimation.html"><a href="maximum-likelkihood-estimation.html#maximum-likelihood-for-a-crd"><i class="fa fa-check"></i>Maximum likelihood for a CRD</a></li>
<li class="chapter" data-level="" data-path="maximum-likelkihood-estimation.html"><a href="maximum-likelkihood-estimation.html#maximising-the-log-likelihood"><i class="fa fa-check"></i>Maximising the log-likelihood</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="maximum-likelihood-estimation-for-a-poisson-distribution.html"><a href="maximum-likelihood-estimation-for-a-poisson-distribution.html"><i class="fa fa-check"></i>Maximum likelihood estimation for a Poisson distribution</a><ul>
<li class="chapter" data-level="" data-path="maximum-likelihood-estimation-for-a-poisson-distribution.html"><a href="maximum-likelihood-estimation-for-a-poisson-distribution.html#maximising-the-likelihood"><i class="fa fa-check"></i>Maximising the Likelihood</a></li>
<li class="chapter" data-level="" data-path="maximum-likelihood-estimation-for-a-poisson-distribution.html"><a href="maximum-likelihood-estimation-for-a-poisson-distribution.html#maximising-the-log-likelihood-function"><i class="fa fa-check"></i>Maximising the log-likelihood function</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="example-for-a-continuous-random-variable.html"><a href="example-for-a-continuous-random-variable.html"><i class="fa fa-check"></i>Example for a continuous random variable</a></li>
<li class="chapter" data-level="" data-path="introduction-to-bayesian-statistics.html"><a href="introduction-to-bayesian-statistics.html"><i class="fa fa-check"></i>Introduction to Bayesian statistics</a><ul>
<li class="chapter" data-level="" data-path="introduction-to-bayesian-statistics.html"><a href="introduction-to-bayesian-statistics.html#conditional-probability"><i class="fa fa-check"></i>Conditional probability</a></li>
<li class="chapter" data-level="" data-path="introduction-to-bayesian-statistics.html"><a href="introduction-to-bayesian-statistics.html#bayes-rule"><i class="fa fa-check"></i>Bayes’ rule</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="module-5.html"><a href="module-5.html"><i class="fa fa-check"></i>Module 5</a><ul>
<li class="chapter" data-level="" data-path="beyond-linear-models-to-generalised-linear-models-glms.html"><a href="beyond-linear-models-to-generalised-linear-models-glms.html"><i class="fa fa-check"></i>Beyond Linear Models to Generalised Linear Models (GLMs)</a><ul>
<li class="chapter" data-level="" data-path="beyond-linear-models-to-generalised-linear-models-glms.html"><a href="beyond-linear-models-to-generalised-linear-models-glms.html#counting-animals"><i class="fa fa-check"></i>Counting animals…</a></li>
<li class="chapter" data-level="" data-path="beyond-linear-models-to-generalised-linear-models-glms.html"><a href="beyond-linear-models-to-generalised-linear-models-glms.html#other-modelling-approaches-not-examinable"><i class="fa fa-check"></i>Other modelling approaches (not examinable)</a></li>
<li><a href="beyond-linear-models-to-generalised-linear-models-glms.html#model-formula-syntax"><strong>Model formula</strong> syntax</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction-to-generalised-linear-models-glms.html"><a href="introduction-to-generalised-linear-models-glms.html"><i class="fa fa-check"></i>Introduction to generalised linear models (GLMs)</a></li>
<li class="chapter" data-level="" data-path="poisson-regression.html"><a href="poisson-regression.html"><i class="fa fa-check"></i>Poisson regression</a><ul>
<li class="chapter" data-level="" data-path="poisson-regression.html"><a href="poisson-regression.html#an-example-bird-abundance"><i class="fa fa-check"></i>An example: bird abundance</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i>Logistic regression</a><ul>
<li class="chapter" data-level="" data-path="logistic-regression.html"><a href="logistic-regression.html#an-example-lobsters"><i class="fa fa-check"></i>An example: lobsters</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="a-summary-of-glms.html"><a href="a-summary-of-glms.html"><i class="fa fa-check"></i>A summary of GLMs</a></li>
<li class="chapter" data-level="" data-path="introduction-to-generalised-linear-mixed-effects-models-glmmms.html"><a href="introduction-to-generalised-linear-mixed-effects-models-glmmms.html"><i class="fa fa-check"></i>Introduction to generalised linear mixed-effects models (GLMMMs)</a><ul>
<li class="chapter" data-level="" data-path="introduction-to-generalised-linear-mixed-effects-models-glmmms.html"><a href="introduction-to-generalised-linear-mixed-effects-models-glmmms.html#fitting-a-glmm"><i class="fa fa-check"></i>Fitting a GLMM</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="module-6.html"><a href="module-6.html"><i class="fa fa-check"></i>Module 6</a><ul>
<li class="chapter" data-level="" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i>Clustering</a><ul>
<li class="chapter" data-level="" data-path="clustering.html"><a href="clustering.html#partitioning-methods."><i class="fa fa-check"></i>Partitioning methods.</a></li>
<li class="chapter" data-level="" data-path="clustering.html"><a href="clustering.html#hierarchical-methods."><i class="fa fa-check"></i>Hierarchical methods.</a></li>
<li class="chapter" data-level="" data-path="clustering.html"><a href="clustering.html#hierarchical-agglomerative-clustering."><i class="fa fa-check"></i>Hierarchical agglomerative clustering.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="tldr.html"><a href="tldr.html"><i class="fa fa-check"></i>TL;DR</a><ul>
<li class="chapter" data-level="" data-path="tldr.html"><a href="tldr.html#clustering-algorithms"><i class="fa fa-check"></i>Clustering algorithms</a></li>
<li><a href="tldr.html#k-means-using-the-palmerpenguins-data">k-means using the <code>palmerpenguins</code> data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="tldr-k-means-clustering.html"><a href="tldr-k-means-clustering.html"><i class="fa fa-check"></i>TL;DR k-means clustering</a></li>
<li class="chapter" data-level="" data-path="dimension-reduction.html"><a href="dimension-reduction.html"><i class="fa fa-check"></i>Dimension reduction</a><ul>
<li class="chapter" data-level="" data-path="dimension-reduction.html"><a href="dimension-reduction.html#pca"><i class="fa fa-check"></i>PCA</a></li>
<li class="chapter" data-level="" data-path="dimension-reduction.html"><a href="dimension-reduction.html#reality-check-reducing-noise"><i class="fa fa-check"></i>Reality check: reducing noise…</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="multidimensional-scaling-mds.html"><a href="multidimensional-scaling-mds.html"><i class="fa fa-check"></i>Multidimensional Scaling (MDS)</a><ul>
<li class="chapter" data-level="" data-path="multidimensional-scaling-mds.html"><a href="multidimensional-scaling-mds.html#principal-coordinates."><i class="fa fa-check"></i>Principal Coordinates.</a></li>
<li class="chapter" data-level="" data-path="multidimensional-scaling-mds.html"><a href="multidimensional-scaling-mds.html#metric-scaling."><i class="fa fa-check"></i>Metric Scaling.</a></li>
<li class="chapter" data-level="" data-path="multidimensional-scaling-mds.html"><a href="multidimensional-scaling-mds.html#non-metric-scaling."><i class="fa fa-check"></i>Non-metric scaling.</a></li>
<li class="chapter" data-level="" data-path="multidimensional-scaling-mds.html"><a href="multidimensional-scaling-mds.html#which-to-use-metric-or-non-metric"><i class="fa fa-check"></i>Which to use: metric or non-metric?</a></li>
<li><a href="multidimensional-scaling-mds.html#examples-in-r">Examples in <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="non-metric-multidimensional-scaling.html"><a href="non-metric-multidimensional-scaling.html"><i class="fa fa-check"></i>Non-metric Multidimensional Scaling</a><ul>
<li><a href="non-metric-multidimensional-scaling.html#examples-in-r-1">Examples in <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="correspondence-analysis-ca.html"><a href="correspondence-analysis-ca.html"><i class="fa fa-check"></i>Correspondence Analysis (CA)</a></li>
<li class="chapter" data-level="" data-path="mds-summary.html"><a href="mds-summary.html"><i class="fa fa-check"></i>MDS summary</a></li>
<li class="chapter" data-level="" data-path="linear-discriminant-analysis-lda.html"><a href="linear-discriminant-analysis-lda.html"><i class="fa fa-check"></i>Linear Discriminant Analysis (LDA)</a><ul>
<li><a href="linear-discriminant-analysis-lda.html#example-in-r">Example in <code>R</code></a></li>
<li class="chapter" data-level="" data-path="linear-discriminant-analysis-lda.html"><a href="linear-discriminant-analysis-lda.html#prediction"><i class="fa fa-check"></i>Prediction</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Advanced Biological Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="clustering" class="section level2">
<h2>Clustering</h2>
<p>So, it’s all about variation again! And the idea of minimizing it.</p>
<p>Cluster analysis, or classification as it is known in the botanical literature, has the apparently simple
aim of finding clusters in a data cloud of sampling units in the absence of any a priori information
about which point belongs in which cluster. This apparently unambitious aim is unfortunately
fraught with problems.
The major difficulty is that no one seems to agree on precisely what a cluster is. For a very good
reason, the human eye is unexcelled as a pattern recognition device, but we recognise clusters of
points in a variety of different ways. For example, it is extremely difficult to think of a single
definition that would adequately describe all the clusters in fig 7.1, even though they are quite
obvious (I hope). Some workers have stressed the importance of cohesiveness (like fig 7.1a); others
contiguity of points (7.1b); yet others have concentrated on distances such that all or most of the
distances within a cluster are less than those to any point outside the cluster; and finally others have
tried to make the definition so vague that it can include most of the possibilities without the
necessity of actually defining anything. Everitt’s definition (Everitt 1980) seems to come as close to
being useful as any:
“Clusters may be described as continuous regions of (a) space containing a relatively high density
of points, separated from other such regions by regions containing a relatively low density of
points.”
Unfortunately it does not provide a rationale for a single comprehensive technique that can handle
all the data structures shown and satisfy all the requirements of workers. Indeed it is extremely
unlikely that any such method could ever be found, for there lies another problem, workers want the
technique(s) for a number of different purposes:
i) to find groups for classification;
ii) to reduce the number of sampling units in an analysis by using a single representative from each
cluster of similar individuals;
iii) for data exploration and hypothesis generation;
iv) for fitting distribution models and estimating their parameters;
v) dissecting a continuous data cloud into relatively homogeneous zones;
and many more.
The only thing the large number of existing techniques have in common is that unlike canonical
discriminant analysis (section 10) and discriminant function analysis (not covered in this course)
there is no prior information about which sampling unit is in which group. Like the ordination
methods of the earlier chapters, cluster analysis techniques operate on an unpartitioned data matrix
to find, or impose, structure in the data cloud.
One consequence of this variation in definition and use is that cluster analysis as such does not
exist. The title refers to an enormous and extraordinarily diverse family of techniques. For someone
to say that they used cluster analysis is about as informative as their saying they studied an insect.
To cover all the techniques would take a whole (large) book. So for this course I shall content
myself with covering some of the common ones and ones I think are potentially most useful.
Given the diversity of techniques it is very important to choose the technique with a clear idea of
what it is required to do. Like selecting a similarity or distance metric (Section 4), the choice must
be made with care after consideration of the nature of the data, your objectives, and the available
alternatives. However the most important thing to remember when using a clustering technique is:-
you must not believe the result. The pattern you get is at most a plausible way of viewing the data.
By using an appropriate method and by employing validation techniques the plausibility can be
enhanced, but no cluster analysis can be relied on to produce truth. With real data, different methods
will nearly always produce different results. If the structure in the data is fairly obvious then these
answers may not differ much, but if there is any ambiguity in the data then the methods may well
give contradictory results.</p>
<div id="partitioning-methods." class="section level3">
<h3>Partitioning methods.</h3>
<p>Though the hierarchical methods have been historically more important, the partitioning methods
are becoming increasingly popular, and it is easy to see why. The hierarchical methods are restricted
to an often inappropriate nested structure so that at each level (number of clusters) the solution is
constrained by the previous one. In the partitioning or segmentation methods the solution at any
level is independent of the others and can therefore be globally optimal - if you’re lucky.
For a single run of a partitioning method, the desired number of clusters ( k ) is usually fixed - some
techniques do allow some small adjustment in this number during the process. Of course since the
correct number of clusters is usually not known, the program is normally run with different values
of k and the optimum number of clusters chosen (covered later).
There are two major phases to a partitioning method:
i) an initial allocation (usually rather arbitrary) into k preliminary clusters;
ii) reallocation of each point either to the closest centroid, or so as to optimise some property of the
clusters. This is repeated until there is no further improvement, then the program stops.
The initial allocation is usually started by choosing k sampling units to use as “seeds” to
“crystallise” the clusters. There are a number of ways to choose these seeds; it depends on the
program. As we shall see it is a tremendous advantage if you can put in your own set. These seeds
are used as the initial centres of the clusters, points are allocated to the nearest cluster centre, and in
most programs the cluster centroid is adjusted as they are added.
The methods we consider here (there are others) the k -means methods, run through the sampling
units reallocating them to the cluster with the closest centroid; they pass and repass through the data
till no further reallocation of points is possible. Some programs then try swapping pairs of points
between clusters, to further improve the solution, and to protect against local optima.</p>
<div id="k-means-partitioning-methods" class="section level4">
<h4>K-means partitioning methods</h4>
<p>The k -means methods are generally the fastest clustering methods, but they are inclined to be
trapped by local optima and tend to produce equal volume spherical solutions. They are also very
ensitive to starting strategy. Some workers suggest that random starting values should not be used.
Seber reports a study as having located the global optimum only 3 times from 24 random starts!
However their performance in the few Monte Carlo simulation studies that have incorporated them
has been good relative to alternative methods, particularly when the solution from a hierarchical
method was used as the starting configuration. In fact, it has tended to be better than the best
hierarchical methods considered (Ward’s and average linkage).
If the data set is particularly large, a sub-sample of the points could be clustered and the estimated
centroids of the resulting clusters used as seeds for the analysis of the full data set. Some programs
allow you to vary how the distance to the centroid is measured. Some programs normally use the
squared distance which means that it is minimising the trace( W ) where W is the within cluster
variance-covariance matrix pooled over all the clusters, i.e. the total within sample variance. This is
an appealingly statistical thing to optimise.</p>
</div>
</div>
<div id="hierarchical-methods." class="section level3">
<h3>Hierarchical methods.</h3>
<p>These methods assume that the groupings in the data cloud have a hierarchical structure. The
smaller groups form larger groups which form larger groups and so on - a nested classification. If
this assumption is untrue then the techniques can be expected to distort the true structure of the
data.
Most of the commonly used techniques are members of this group. They are widely available, all
the major packages have a selection, and they are relatively easy to use, though often less so to
interpret.
Hierarchical organisation is often difficult to justify for real data sets. Though there may be more
than one level of grouping there may be no reason to assume that they are nested. For example, it
has been shown that the clusterings defined by the optimum sum of squares at various levels of k
may not be nested for all data sets; so a hierarchical method may be unsuitable for any given data
set.
There are two approaches to hierarchical clustering, agglomerative and divisive . Agglomerative
methods start from the individual sampling units forming them into groups and fusing the groups
till there is only one that includes all the points. If we can describe this as working from the bottom
up, then the divisive techniques work from the top down. The groups are formed by splitting the
data set successively until there are as many groups as points.</p>
</div>
<div id="hierarchical-agglomerative-clustering." class="section level3">
<h3>Hierarchical agglomerative clustering.</h3>
<p>All of the commonly used hierarchical methods are agglomerative. Most of them operate in the
same way: first all sampling units that are zero distance apart are fused into clusters. The threshold
for fusion is then raised from zero until two clusters (they may be individual points) are found that
are close enough to fuse. The threshold is raised, fusing the clusters as their distance apart is
reached until all the clusters have been fused into one big one. Thus the close clusters are fused
first, then those further apart, till all have been fused. This process allows the history of the fusions,
the hierarchy, to be displayed as a dendrogram. This is an advantage of the agglomerative methods,
if the data have a nested structure these techniques lead to a useful way of displaying it. Other
advantages are the ready availability of programs and their ability to handle quite large data sets - at
reasonable expense. Unlike the optimisation or k -means methods, most of the agglomerative techniques can use a broad range of similarity or distance measures. This of course means that
considerable care must be taken to choose the appropriate one; different measures often lead to
different results.
Inevitably, given the variety of definitions of a cluster, there are a large number of different
hierarchical agglomerative techniques. They mainly differ in the details of the fusion rule. For most
of them the rule is simply stated: two clusters should be fused if the distance between them has been
reached by the threshold. The problem is to estimate that distance. It can be done in a variety of
ways and will usually affect the results. As we shall see, different types of clusters need different
ways of estimating intercluster distance.
We shall consider the four most commonly used methods.</p>
<div id="single-linkage-nearest-neighbour-clustering." class="section level4">
<h4>Single linkage (nearest neighbour) clustering.</h4>
<p>Single Linkage (<em>nearest neighbour/minimal jump</em>): Computes the distance between clusters as the smallest distance between any two points in the two clusters</p>
<p><img src="img/single_linkage.png" /></p>
<p>The distance between two clusters is the distance between their nearest points (Figure 7.3a).The
simplicity of this method makes it easy to program and extremely efficient. It was one of the most
popular techniques in the early days of clustering; but since then, despite support from the
theoreticians, it has been used less frequently. In general it has not performed well. It identifies
clusters on the basis of isolation, how far apart they are at their closest points. This means that if
there are any intermediate points then single linkage will fuse the groups without leaving any trace
of their separate identities. This is called “chaining”, which leads to characteristic and rather
uninformative dendrograms. It is the chief weakness of the method. Its strength is that if the clusters
are well separated in the data, then single linkage can handle groups of different shapes and sizes,
even long thin straggly ones (e.g. Figure 7.1c) that other methods often cannot recover. It has other
advantages, it will give the same clustering after any monotonic transformation of the distance
measure - that means that it is fairly robust to the choice of measure. It is insensitive to tied
distances - some methods suffer from indeterminacy if there are too many ties; a bit like degenerate
solutions in non-metric MDS, (section 6.3.3.iii) and though the results are seldom as pretty, they
can be just as meaningless.
As a cluster analysis single linkage is usually not very useful (unless the data is of the right type).
Many investigations have found it performs badly with even slightly messy data.</p>
</div>
<div id="complete-linkage-farthest-neighbour-clustering." class="section level4">
<h4>Complete linkage (farthest neighbour) clustering.</h4>
<ul>
<li>Complete Linkage (<em>maximum jump</em>): Calculates the maximum distance betweentwo points from each cluster</li>
</ul>
<p><img src="img/maximum_linkage.png" /></p>
<p>In many respects complete linkage clustering is the opposite of single linkage. Instead of measuring
the distance between two clusters as that between their two nearest members; it uses that between
the two farthest members (Figure 7.3b). In consequence the resulting clusters are compact, spherical
and well defined. Unlike single linkage it can be sensitive to tied distances. There are similarities,
the clustering it gives is also invariant under monotonic transformation of the distances; it is robust
to a certain amount of measurement error and choice of distance. Unfortunately it is sensitive to
even a single change in the rank order of the distances in the dissimilarity matrix (Seber 1984), and
does not cope well with outliers. However, in Monte Carlo simulations, it nearly always performed
better than single linkage; though usually not quite as well as Ward’s or group average.</p>
</div>
<div id="group-average-linkage-upgma" class="section level4">
<h4>Group average linkage (UPGMA)</h4>
<p>This is probably the most popular hierarchical clustering method - for a very good reason - it
usually works well. It could be thought of as an attempt to avoid the extremes of the single and
complete linkage methods. The distance between two clusters is the average of the distances
between the members of the two groups (Figure 7.3c). If the distances are Euclidean this is the
distance between the centroids plus the within group scatter. As a result this method tends to
produce compact spherical clusters.
Like its main rival Ward’s method, average linkage has generally performed well in Monte Carlo
simulations, and its continued popularity is because it consistently, though not inevitably, gives
adequate results. However, Ward’s generally performed better, particularly when there was some
overlap between the groups. When intermediate points and outliers were removed (“trimming” or
“incomplete coverage”), group average’s performance was considerably improved. It performed
poorly with mixtures of multivariate normal distributions probably because of the overlap between
clusters..</p>
</div>
<div id="wards-method-incremental-sums-of-squares-minimum-variance-agglomerative-sums-of-squares." class="section level4">
<h4>Ward’s method (incremental sums of squares, minimum variance, agglomerative sums of squares).</h4>
<p>Ward’s method: where the goal is to minimize the variance within clusters</p>
<p><img src="img/wards_linkage.png" /></p>
<p>Ward’s method is the hierarchical version of the k-means partitioning method. At each fusion it
attempts to minimise the increase in total sum of squared distances within the clusters. This is
equivalent to minimising the sum of squared within cluster deviations from the centroids - i.e.
trace( W ). Since at any one stage it can only fuse those clusters already in existence - it is not
allowed to reallocate points - it can only be stepwise optimal. It cannot find the true minimum
configuration at each level, so it would not be expected to recover natural clusters as well as the
non-hierarchical methods that also minimise trace( W ). A bad start to the agglomeration process can
place the algorithm on a path from which it can never reach the global optimum for a given number
of clusters. Despite this, Ward’s method has performed well in simulations; one of the two best
hierarchical methods overall. Its chief flaw is a tendency to form clusters of equal size, regardless of
the true number. So when the number of points in the clusters are different, group average and
complete link may give better results. Like the complete linkage and group average methods it is
also biased towards forming spherical clusters; though perhaps not as strongly as they are. It may
also be rather sensitive to outliers. However it appears to perform well when there is a lot of
overlap, when many of the other techniques have difficulties. It has been found in simulations that
Ward’s performed best of the hierarchical methods at recovering natural clusters, but that the k -
means and optimising methods were better.</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="module-6.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="tldr.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
