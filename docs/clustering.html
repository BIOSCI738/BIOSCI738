<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Clustering | Advanced Biological Data Analysis</title>
  <meta name="description" content="Clustering | Advanced Biological Data Analysis" />
  <meta name="generator" content="bookdown 0.23 and GitBook 2.6.7" />

  <meta property="og:title" content="Clustering | Advanced Biological Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Clustering | Advanced Biological Data Analysis" />
  
  
  

<meta name="author" content="University of Auckland" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="module-6.html"/>
<link rel="next" href="principal-component-analysis-pca.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Nau mai, haere mai. Welcome to BIOSCI 738</a></li>
<li class="chapter" data-level="" data-path="useful-information-to-set-you-up-for-your-semester.html"><a href="useful-information-to-set-you-up-for-your-semester.html"><i class="fa fa-check"></i>Useful information to set you up for your semester</a><ul>
<li class="chapter" data-level="" data-path="useful-information-to-set-you-up-for-your-semester.html"><a href="useful-information-to-set-you-up-for-your-semester.html#course-outline"><i class="fa fa-check"></i>Course outline</a></li>
<li class="chapter" data-level="" data-path="useful-information-to-set-you-up-for-your-semester.html"><a href="useful-information-to-set-you-up-for-your-semester.html#learning-outcomes"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="" data-path="useful-information-to-set-you-up-for-your-semester.html"><a href="useful-information-to-set-you-up-for-your-semester.html#course-summary"><i class="fa fa-check"></i>Course summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="module-1.html"><a href="module-1.html"><i class="fa fa-check"></i>Module 1</a><ul>
<li><a href="r-and-rstudio.html#r-and-rstudio"><code>R</code> and <code>RStudio</code></a><ul>
<li class="chapter" data-level="" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html"><i class="fa fa-check"></i>Installing R and RStudio</a></li>
<li class="chapter" data-level="" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#getting-started"><i class="fa fa-check"></i>Getting started</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="reproducible-research.html"><a href="reproducible-research.html"><i class="fa fa-check"></i>Reproducible research</a><ul>
<li class="chapter" data-level="" data-path="reproducible-research.html"><a href="reproducible-research.html#project-oriented-workflow-good-practice"><i class="fa fa-check"></i>Project-oriented workflow: good practice</a></li>
<li><a href="reproducible-research.html#version-control-with-git-and-github">Version control with <code>git</code> and GitHub</a></li>
</ul></li>
<li><a href="your-data-and-r.html#your-data-and-r">Your data and <code>R</code></a><ul>
<li><a href="your-data-and-r.html#reading-in-data-from-a-.csv-file">Reading in data from a <code>.csv</code> file</a></li>
<li class="chapter" data-level="" data-path="your-data-and-r.html"><a href="your-data-and-r.html"><i class="fa fa-check"></i>Explore your data</a></li>
<li><a href="your-data-and-r.html#the-pipe-operator">The pipe operator <code>%&gt;%</code></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="data-wrangling.html"><a href="data-wrangling.html"><i class="fa fa-check"></i>Data wrangling</a><ul>
<li><a href="data-wrangling.html#introuducing-the-palmer-penguins">Introuducing the <span>Palmer penguins</span></a></li>
<li><a href="data-wrangling.html#common-dataframe-manipulations-in-the-tidyverse-using-dplyr-and-tidyr">Common dataframe manipulations in the <code>tidyverse</code>, using <code>dplyr</code> and <code>tidyr</code></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="data-visualiation-data-viz.html"><a href="data-visualiation-data-viz.html"><i class="fa fa-check"></i>Data visualiation (data viz)</a><ul>
<li class="chapter" data-level="" data-path="data-visualiation-data-viz.html"><a href="data-visualiation-data-viz.html#exploratory-and-explanatory-plots"><i class="fa fa-check"></i>Exploratory and explanatory plots</a></li>
<li class="chapter" data-level="" data-path="data-visualiation-data-viz.html"><a href="data-visualiation-data-viz.html#ten-simple-rules-for-better-figures"><i class="fa fa-check"></i>Ten Simple Rules for Better Figures</a></li>
<li><a href="data-visualiation-data-viz.html#ggplot2"><code>ggplot2</code></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="mƒÅori-data-sovereignty-principles.html"><a href="mƒÅori-data-sovereignty-principles.html"><i class="fa fa-check"></i>MƒÅori Data Sovereignty principles</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="module-2.html"><a href="module-2.html"><i class="fa fa-check"></i>Module 2</a><ul>
<li class="chapter" data-level="" data-path="key-satistical-concepts.html"><a href="key-satistical-concepts.html"><i class="fa fa-check"></i>Key satistical concepts</a></li>
<li class="chapter" data-level="" data-path="bootstrap-resampling.html"><a href="bootstrap-resampling.html"><i class="fa fa-check"></i>Bootstrap resampling</a><ul>
<li class="chapter" data-level="" data-path="bootstrap-resampling.html"><a href="bootstrap-resampling.html#example-constructing-bootstrap-confidence-intervals"><i class="fa fa-check"></i>Example: constructing bootstrap confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="permutation-and-randomisation-tests.html"><a href="permutation-and-randomisation-tests.html"><i class="fa fa-check"></i>Permutation and randomisation tests</a><ul>
<li class="chapter" data-level="" data-path="permutation-and-randomisation-tests.html"><a href="permutation-and-randomisation-tests.html#a-permutation-test-jackal-mandible-lengths"><i class="fa fa-check"></i>A permutation test: Jackal mandible lengths</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="resampling-procedures-the-differences.html"><a href="resampling-procedures-the-differences.html"><i class="fa fa-check"></i>Resampling procedures, the differences</a></li>
<li class="chapter" data-level="" data-path="hypothesis-testing-key-terms.html"><a href="hypothesis-testing-key-terms.html"><i class="fa fa-check"></i>Hypothesis testing: key terms</a><ul>
<li class="chapter" data-level="" data-path="hypothesis-testing-key-terms.html"><a href="hypothesis-testing-key-terms.html#p-values"><i class="fa fa-check"></i>üò± p-values üò±</a></li>
<li class="chapter" data-level="" data-path="hypothesis-testing-key-terms.html"><a href="hypothesis-testing-key-terms.html#type-i-and-type-ii-errors"><i class="fa fa-check"></i>Type I and Type II errors</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="parametric-hypothesis-tests.html"><a href="parametric-hypothesis-tests.html"><i class="fa fa-check"></i>Parametric hypothesis tests</a><ul>
<li class="chapter" data-level="" data-path="parametric-hypothesis-tests.html"><a href="parametric-hypothesis-tests.html#one-sample-t-test"><i class="fa fa-check"></i>One-Sample t-test</a></li>
<li class="chapter" data-level="" data-path="parametric-hypothesis-tests.html"><a href="parametric-hypothesis-tests.html#differences-between-two-means"><i class="fa fa-check"></i>Differences between two means</a></li>
<li class="chapter" data-level="" data-path="parametric-hypothesis-tests.html"><a href="parametric-hypothesis-tests.html#one-way-analysis-of-variance-anova"><i class="fa fa-check"></i>One-Way Analysis of Variance (ANOVA)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i>Linear regression</a><ul>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#some-mathematical-notation"><i class="fa fa-check"></i>Some mathematical notation</a></li>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#modeling-bill-depth"><i class="fa fa-check"></i>Modeling Bill Depth</a></li>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#single-continuous-variable"><i class="fa fa-check"></i>Single continuous variable</a></li>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#one-factor-and-a-continous-variable"><i class="fa fa-check"></i>One factor and a continous variable</a></li>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#interactions"><i class="fa fa-check"></i>Interactions</a></li>
</ul></li>
<li><a href="model-formula-syntax-in-r.html#model-formula-syntax-in-r">Model formula syntax in <code>R</code></a><ul>
<li class="chapter" data-level="" data-path="model-formula-syntax-in-r.html"><a href="model-formula-syntax-in-r.html"><i class="fa fa-check"></i>Other possible models</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="model-diagnostocs-for-a-linear-model.html"><a href="model-diagnostocs-for-a-linear-model.html"><i class="fa fa-check"></i>Model diagnostocs for a linear model</a><ul>
<li class="chapter" data-level="" data-path="model-diagnostocs-for-a-linear-model.html"><a href="model-diagnostocs-for-a-linear-model.html#marginal-predictions"><i class="fa fa-check"></i>Marginal predictions</a></li>
<li class="chapter" data-level="" data-path="model-diagnostocs-for-a-linear-model.html"><a href="model-diagnostocs-for-a-linear-model.html#model-selection"><i class="fa fa-check"></i>Model selection</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="inference-for-a-linear-model.html"><a href="inference-for-a-linear-model.html"><i class="fa fa-check"></i>Inference for a linear model</a><ul>
<li class="chapter" data-level="" data-path="inference-for-a-linear-model.html"><a href="inference-for-a-linear-model.html#point-prediction"><i class="fa fa-check"></i>Point prediction</a></li>
<li class="chapter" data-level="" data-path="inference-for-a-linear-model.html"><a href="inference-for-a-linear-model.html#confidence-intervals-for-parameters"><i class="fa fa-check"></i>Confidence intervals for parameters</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="module-3.html"><a href="module-3.html"><i class="fa fa-check"></i>Module 3</a><ul>
<li class="chapter" data-level="" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html"><i class="fa fa-check"></i>Introduction to the design and analysis of experiments</a><ul>
<li class="chapter" data-level="" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#key-phrases"><i class="fa fa-check"></i>Key phrases</a></li>
<li class="chapter" data-level="" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#the-three-key-principles"><i class="fa fa-check"></i>The three key principles:</a></li>
<li class="chapter" data-level="" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#replication"><i class="fa fa-check"></i>Replication</a></li>
<li class="chapter" data-level="" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#randomization"><i class="fa fa-check"></i>Randomization</a></li>
<li class="chapter" data-level="" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#blocking"><i class="fa fa-check"></i>Blocking</a></li>
<li class="chapter" data-level="" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#setting-up-an-experiment"><i class="fa fa-check"></i>Setting up an experiment</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="some-basic-experimental-designs.html"><a href="some-basic-experimental-designs.html"><i class="fa fa-check"></i>Some basic experimental designs</a><ul>
<li class="chapter" data-level="" data-path="some-basic-experimental-designs.html"><a href="some-basic-experimental-designs.html#completely-randomised-design-crd"><i class="fa fa-check"></i>Completely randomised design (CRD)</a></li>
<li class="chapter" data-level="" data-path="some-basic-experimental-designs.html"><a href="some-basic-experimental-designs.html#randomised-complete-block-design-rcbd"><i class="fa fa-check"></i>Randomised complete block design (RCBD)</a></li>
<li class="chapter" data-level="" data-path="some-basic-experimental-designs.html"><a href="some-basic-experimental-designs.html#factorial-design"><i class="fa fa-check"></i>Factorial design</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="modlling-experimental-data.html"><a href="modlling-experimental-data.html"><i class="fa fa-check"></i>Modlling experimental data</a><ul>
<li class="chapter" data-level="" data-path="modlling-experimental-data.html"><a href="modlling-experimental-data.html#a-completely-randomised-design-crd-as-a-linear-model"><i class="fa fa-check"></i>A completely randomised design (CRD) as a linear model</a></li>
<li><a href="modlling-experimental-data.html#analysis-of-a-crd-in-r">Analysis of a CRD in <code>R</code></a></li>
<li><a href="modlling-experimental-data.html#using-lm">Using <code>lm()</code></a></li>
<li class="chapter" data-level="" data-path="modlling-experimental-data.html"><a href="modlling-experimental-data.html#a-factorial-experiment-as-a-crd"><i class="fa fa-check"></i>A Factorial experiment (as a CRD)</a></li>
<li class="chapter" data-level="" data-path="modlling-experimental-data.html"><a href="modlling-experimental-data.html#unqual-replications-unbalanced-design"><i class="fa fa-check"></i>Unqual replications (unbalanced design)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html"><i class="fa fa-check"></i>Multiple comparisons</a><ul>
<li class="chapter" data-level="" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#classification-of-multiple-hypothesis-tests"><i class="fa fa-check"></i>Classification of multiple hypothesis tests</a></li>
<li><a href="multiple-comparisons.html#multiple-comparison-procedures-using-predictmeans">Multiple comparison procedures using <code>predictmeans</code></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="linear-mixed-effect-models-lmms.html"><a href="linear-mixed-effect-models-lmms.html"><i class="fa fa-check"></i>Linear mixed-effect models (LMMs)</a><ul>
<li class="chapter" data-level="" data-path="linear-mixed-effect-models-lmms.html"><a href="linear-mixed-effect-models-lmms.html#a-randomised-controlled-block-design-rcbd"><i class="fa fa-check"></i>A Randomised Controlled Block Design (RCBD)</a></li>
<li class="chapter" data-level="" data-path="linear-mixed-effect-models-lmms.html"><a href="linear-mixed-effect-models-lmms.html#a-split-plot-design"><i class="fa fa-check"></i>A Split-plot design</a></li>
<li class="chapter" data-level="" data-path="linear-mixed-effect-models-lmms.html"><a href="linear-mixed-effect-models-lmms.html#a-repeated-measures-design-time-points-as-factors"><i class="fa fa-check"></i>A repeated measures design (time points as factors)</a></li>
<li><a href="linear-mixed-effect-models-lmms.html#animal-as-a-random-effect-in-linear-regression"><code>Animal</code> as a random effect in linear regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="module-4.html"><a href="module-4.html"><i class="fa fa-check"></i>Module 4</a><ul>
<li class="chapter" data-level="" data-path="least-squares-estimation.html"><a href="least-squares-estimation.html"><i class="fa fa-check"></i>Least Squares Estimation</a><ul>
<li class="chapter" data-level="" data-path="least-squares-estimation.html"><a href="least-squares-estimation.html#some-basic-matrix-algebra"><i class="fa fa-check"></i>Some basic matrix algebra</a></li>
<li class="chapter" data-level="" data-path="least-squares-estimation.html"><a href="least-squares-estimation.html#linear-least-squares"><i class="fa fa-check"></i>Linear least squares</a></li>
<li class="chapter" data-level="" data-path="least-squares-estimation.html"><a href="least-squares-estimation.html#matrix-representation-of-a-crd"><i class="fa fa-check"></i>Matrix representation of a CRD</a></li>
<li class="chapter" data-level="" data-path="least-squares-estimation.html"><a href="least-squares-estimation.html#a-numeric-example"><i class="fa fa-check"></i>A numeric example</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="maximum-likelkihood-estimation.html"><a href="maximum-likelkihood-estimation.html"><i class="fa fa-check"></i>Maximum likelkihood estimation</a><ul>
<li class="chapter" data-level="" data-path="maximum-likelkihood-estimation.html"><a href="maximum-likelkihood-estimation.html#differeniation-rules"><i class="fa fa-check"></i>Differeniation rules</a></li>
<li class="chapter" data-level="" data-path="maximum-likelkihood-estimation.html"><a href="maximum-likelkihood-estimation.html#logarithn-rules"><i class="fa fa-check"></i>Logarithn rules</a></li>
<li class="chapter" data-level="" data-path="maximum-likelkihood-estimation.html"><a href="maximum-likelkihood-estimation.html#maximum-likelihood-estimation-for-a-binomial-distribution"><i class="fa fa-check"></i>Maximum likelihood estimation for a Binomial distribution</a></li>
<li class="chapter" data-level="" data-path="maximum-likelkihood-estimation.html"><a href="maximum-likelkihood-estimation.html#maximum-likelihood-estimation-for-a-crd"><i class="fa fa-check"></i>Maximum likelihood estimation for a CRD</a></li>
<li class="chapter" data-level="" data-path="maximum-likelkihood-estimation.html"><a href="maximum-likelkihood-estimation.html#maximising-the-log-likelihood"><i class="fa fa-check"></i>Maximising the log-likelihood</a></li>
<li class="chapter" data-level="" data-path="maximum-likelkihood-estimation.html"><a href="maximum-likelkihood-estimation.html#maximum-likelihood-estimation-for-a-poisson-distribution"><i class="fa fa-check"></i>Maximum likelihood estimation for a Poisson distribution</a></li>
<li class="chapter" data-level="" data-path="maximum-likelkihood-estimation.html"><a href="maximum-likelkihood-estimation.html#maximum-likelihood-estimation-for-a-continuous-random-variable"><i class="fa fa-check"></i>Maximum likelihood estimation for a continuous random variable</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction-to-bayesian-statistics.html"><a href="introduction-to-bayesian-statistics.html"><i class="fa fa-check"></i>Introduction to Bayesian statistics</a><ul>
<li class="chapter" data-level="" data-path="introduction-to-bayesian-statistics.html"><a href="introduction-to-bayesian-statistics.html#conditional-probability"><i class="fa fa-check"></i>Conditional probability</a></li>
<li class="chapter" data-level="" data-path="introduction-to-bayesian-statistics.html"><a href="introduction-to-bayesian-statistics.html#bayes-rule"><i class="fa fa-check"></i>Bayes‚Äô rule</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="module-5.html"><a href="module-5.html"><i class="fa fa-check"></i>Module 5</a><ul>
<li class="chapter" data-level="" data-path="introduction-to-generalised-linear-models-glms.html"><a href="introduction-to-generalised-linear-models-glms.html"><i class="fa fa-check"></i>Introduction to generalised linear models (GLMs)</a></li>
<li class="chapter" data-level="" data-path="poisson-regression.html"><a href="poisson-regression.html"><i class="fa fa-check"></i>Poisson regression</a><ul>
<li class="chapter" data-level="" data-path="poisson-regression.html"><a href="poisson-regression.html#an-example-bird-abundance"><i class="fa fa-check"></i>An example: bird abundance</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i>Logistic regression</a><ul>
<li class="chapter" data-level="" data-path="logistic-regression.html"><a href="logistic-regression.html#an-example-lobsters"><i class="fa fa-check"></i>An example: lobsters</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="a-summary-of-glms.html"><a href="a-summary-of-glms.html"><i class="fa fa-check"></i>A summary of GLMs</a></li>
<li class="chapter" data-level="" data-path="generalised-linear-mixed-effects-models-glmmms.html"><a href="generalised-linear-mixed-effects-models-glmmms.html"><i class="fa fa-check"></i>Generalised linear mixed-effects models (GLMMMs)</a><ul>
<li class="chapter" data-level="" data-path="generalised-linear-mixed-effects-models-glmmms.html"><a href="generalised-linear-mixed-effects-models-glmmms.html#fitting-a-glmm"><i class="fa fa-check"></i>Fitting a GLMM</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="module-6.html"><a href="module-6.html"><i class="fa fa-check"></i>Module 6</a><ul>
<li class="chapter" data-level="" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i>Clustering</a><ul>
<li class="chapter" data-level="" data-path="clustering.html"><a href="clustering.html#divisive-partitioning-methods."><i class="fa fa-check"></i>Divisive (partitioning) methods.</a></li>
<li><a href="clustering.html#k-means-an-example-using-the-palmerpenguins-data">K-means: an example using the <code>palmerpenguins</code> data</a></li>
<li class="chapter" data-level="" data-path="clustering.html"><a href="clustering.html#hierarchical-agglomerative-clustering."><i class="fa fa-check"></i>Hierarchical agglomerative clustering.</a></li>
<li class="chapter" data-level="" data-path="clustering.html"><a href="clustering.html#hierarchical-clustering-an-example"><i class="fa fa-check"></i>Hierarchical clustering: an example</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="principal-component-analysis-pca.html"><a href="principal-component-analysis-pca.html"><i class="fa fa-check"></i>Principal Component Analysis (PCA)</a><ul>
<li><a href="principal-component-analysis-pca.html#examples-in-r">Examples in <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="multidimensional-scaling-mds.html"><a href="multidimensional-scaling-mds.html"><i class="fa fa-check"></i>Multidimensional Scaling (MDS)</a><ul>
<li class="chapter" data-level="" data-path="multidimensional-scaling-mds.html"><a href="multidimensional-scaling-mds.html#metric-scaling"><i class="fa fa-check"></i>Metric Scaling</a></li>
<li class="chapter" data-level="" data-path="multidimensional-scaling-mds.html"><a href="multidimensional-scaling-mds.html#correspondence-analysis-ca"><i class="fa fa-check"></i>Correspondence Analysis (CA)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="non-metric-multidimensional-scaling.html"><a href="non-metric-multidimensional-scaling.html"><i class="fa fa-check"></i>Non-metric Multidimensional Scaling</a><ul>
<li><a href="non-metric-multidimensional-scaling.html#examples-in-r-2">Examples in <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="linear-discriminant-analysis-lda.html"><a href="linear-discriminant-analysis-lda.html"><i class="fa fa-check"></i>Linear Discriminant Analysis (LDA)</a><ul>
<li><a href="linear-discriminant-analysis-lda.html#example-in-r">Example in <code>R</code></a></li>
<li class="chapter" data-level="" data-path="linear-discriminant-analysis-lda.html"><a href="linear-discriminant-analysis-lda.html#prediction"><i class="fa fa-check"></i>Prediction</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Advanced Biological Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="clustering" class="section level2">
<h2>Clustering</h2>
<blockquote>
‚ÄúClusters may be described as continuous regions of (a) space containing a relatively high density
of points, separated from other such regions by regions containing a relatively low density of
points.‚Äù
<footer>
‚Äî Everitt, 1980
</footer>
</blockquote>
<blockquote>
‚ÄúCluster analysis has the apparently simple aim of finding clusters in a data cloud of sampling units in the absence of any a priori information about which point belongs in which cluster. This apparently unambitious aim is unfortunately fraught with problems.‚Äù
<footer>
‚Äî Brian McArdle, STATS302
</footer>
</blockquote>
<p>In brief, cluster analysis involves using measures of (dis)similarity and distances to help us define clusters. We use this to uncover hidden or latent clustering by partitioning the data into tighter sets. There are two main methods for doing this: 1) <em>divisive methods</em> use nonparametric algorithms (such as k-means) to split data into a small number of clusters, and 2) <em>agglomerative methods</em> that cluster cases and/or variables into a hierarchy of sets (e.g., hierarchical clustering). We can use to resampling-based bootstrap methods validate clusters.</p>
<div id="divisive-partitioning-methods." class="section level3">
<h3>Divisive (partitioning) methods.</h3>
<p>For a single run of a partitioning method, the number of clusters ( <span class="math inline">\(k\)</span> ) is typically fixed beforehand. Typically, there are only two steps to a partitioning method:</p>
<ol style="list-style-type: decimal">
<li>an initial allocation (usually rather arbitrary) into <span class="math inline">\(k\)</span> preliminary clusters, and then</li>
<li>reallocation of each point either to the closest centroid, or so as to optimise some property of the
clusters. This is repeated until there is no further improvement.</li>
</ol>
<p>The initial allocation is usually started by choosing <span class="math inline">\(k\)</span> sampling units to use as <em>seeds</em> to
<em>set</em> the clusters. There are a number of ways to choose these seeds. These seeds are used as the initial centres of the clusters, points are allocated to the nearest cluster centre, and in most programs the cluster centroid is adjusted as they are added.</p>
<div id="k-means" class="section level4">
<h4>K-means</h4>
<p>K-means clustering involves defining clusters so that the overall variation within a cluster (known as total within-cluster variation) is minimized. How do we define this variation? Typically, using Euclidean distances; the total within-cluster variation, is in this case, is defined as the sum of squared distances Euclidean distances between observations and the corresponding cluster centroid.</p>
<p>In summary, the k-means procedure is</p>
<ul>
<li>The number of clusters (k) are specified</li>
<li>k objects from the dataset are selected at random and <em>set</em> as the initial cluster centers or means</li>
<li>Each observation is assigned to their closest centroid (<em>based on the Euclidean distance between the object and the centroid</em>)</li>
<li>For each of the k clusters the cluster centroid is then updated based on calculating the new mean values of all the data points in the cluster</li>
<li>Repeat the two previous steps until 1) the cluster assignments stop changing or 2) the maximum number of iterations is reached</li>
</ul>
<p>Identifying the appropriate <span class="math inline">\(k\)</span> is important because too many or too few clusters impedes viewing overall trends. Too many clusters can lead to over-fitting (which limits generalizations) while insufficient clusters limits insights into commonality of groups.</p>
<p>There are assorted methodologies to identify the appropriate <span class="math inline">\(k\)</span>. Tests range from blunt visual inspections to robust algorithms. The optimal number of clusters is ultimately a <strong>subjective decision</strong>.</p>
</div>
</div>
<div id="k-means-an-example-using-the-palmerpenguins-data" class="section level3">
<h3>K-means: an example using the <code>palmerpenguins</code> data</h3>
<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb258-1" title="1"><span class="kw">library</span>(palmerpenguins)</a>
<a class="sourceLine" id="cb258-2" title="2"><span class="co">## getting rid of NAs</span></a>
<a class="sourceLine" id="cb258-3" title="3">penguins_nafree &lt;-<span class="st"> </span>penguins <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">drop_na</span>()</a></code></pre></div>
<div class="sourceCode" id="cb259"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb259-1" title="1"><span class="co">## introducing a new package GGally, please install</span></a>
<a class="sourceLine" id="cb259-2" title="2"><span class="co">## using install.packages(&quot;GGally&quot;)</span></a>
<a class="sourceLine" id="cb259-3" title="3"><span class="kw">library</span>(GGally)</a>
<a class="sourceLine" id="cb259-4" title="4">penguins_nafree <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb259-5" title="5"><span class="st">  </span><span class="kw">select</span>(species, <span class="kw">where</span>(is.numeric)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb259-6" title="6"><span class="st">  </span><span class="kw">ggpairs</span>(<span class="dt">columns =</span> <span class="kw">c</span>(<span class="st">&quot;flipper_length_mm&quot;</span>, <span class="st">&quot;body_mass_g&quot;</span>, </a>
<a class="sourceLine" id="cb259-7" title="7">                     <span class="st">&quot;bill_length_mm&quot;</span>, <span class="st">&quot;bill_depth_mm&quot;</span>)) </a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-164-1.png" width="672" /></p>
<p>We see that a lot of these variables (e.g., <code>flipper_length_mm</code>, <code>body_mass_g</code>, and <code>bill_length_mm</code>) are relatively strongly (positively) related to one another. Could they actually be telling us the same information? Combined we could think of these three variables all telling us a little about <em>bigness</em> of penguin. Is there a way we could reduce these three variables, into say 1, to represent the <em>bigness</em> of a penguin. We may not need <em>all</em> the information (variation) captured by these variables, but could get away with fewer <em>new uncorrelated</em> variables that represent basically the same information (e.g., penguin <em>bigness</em>), thereby, <strong>reducing the dimensionality of the data</strong> (more on this later).</p>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb260-1" title="1"><span class="co">## create a data frame of what we&#39;re interested in</span></a>
<a class="sourceLine" id="cb260-2" title="2">df &lt;-<span class="st"> </span>penguins_nafree <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb260-3" title="3"><span class="st">  </span><span class="kw">select</span>(<span class="kw">where</span>(is.numeric), <span class="op">-</span>year)</a></code></pre></div>
<p>We use the <code>kmeans()</code> function.</p>
<p>The first argument of <code>kmeans()</code> should be the dataset you wish to cluster. Below we use data frame <code>df</code>, the penguin data discussed above. But how many clusters do we choose? Let‚Äôs try 1 to 5‚Ä¶ (i.e., using the <code>centers</code> argument). Setting <code>nstart = 25</code> means that R will try 25 different random starting assignments and then select the best results corresponding to the one with the lowest within cluster variation.</p>
<div class="sourceCode" id="cb261"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb261-1" title="1"><span class="co">## set the seed so we all start off in the same place</span></a>
<a class="sourceLine" id="cb261-2" title="2"><span class="kw">set.seed</span>(<span class="dv">4321</span>)</a>
<a class="sourceLine" id="cb261-3" title="3"><span class="co">## one cluster</span></a>
<a class="sourceLine" id="cb261-4" title="4">k1 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(df, <span class="dt">centers =</span> <span class="dv">1</span>, <span class="dt">nstart =</span> <span class="dv">25</span>)</a>
<a class="sourceLine" id="cb261-5" title="5"><span class="co">## two clusters</span></a>
<a class="sourceLine" id="cb261-6" title="6">k2 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(df, <span class="dt">centers =</span> <span class="dv">2</span>, <span class="dt">nstart =</span> <span class="dv">25</span>)</a>
<a class="sourceLine" id="cb261-7" title="7"><span class="co">## three clusters</span></a>
<a class="sourceLine" id="cb261-8" title="8">k3 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(df, <span class="dt">centers =</span> <span class="dv">3</span>, <span class="dt">nstart =</span> <span class="dv">25</span>)</a>
<a class="sourceLine" id="cb261-9" title="9"><span class="co">## four clusters</span></a>
<a class="sourceLine" id="cb261-10" title="10">k4 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(df, <span class="dt">centers =</span> <span class="dv">4</span>, <span class="dt">nstart =</span> <span class="dv">25</span>)</a>
<a class="sourceLine" id="cb261-11" title="11"><span class="co">## five clusters</span></a>
<a class="sourceLine" id="cb261-12" title="12">k5 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(df, <span class="dt">centers =</span> <span class="dv">5</span>, <span class="dt">nstart =</span> <span class="dv">25</span>)</a></code></pre></div>
<p>The <code>kmeans()</code> function returns a list of components:</p>
<ul>
<li><code>cluster</code>, integers indicating the cluster to which each observation is allocated</li>
<li><code>centers</code>, a matrix of cluster centers/means</li>
<li><code>totss</code>, the total sum of squares</li>
<li><code>withinss</code>, within-cluster sum of squares, one component per cluster</li>
<li><code>tot.withinss</code>, total within-cluster sum of squares</li>
<li><code>betweenss</code>, between-cluster sum of squares</li>
<li><code>size</code>, number of observations in each cluster</li>
</ul>
<div id="choosing-the-number-of-clusters" class="section level4">
<h4>Choosing the number of clusters</h4>
<p>We have an idea there may be 3 clusters, perhaps, but how do we know this is the best fit? Remember it‚Äôs a <strong>subjective choice</strong> and we‚Äôll be looking at a few pointers</p>
<p><strong>Visual inspection</strong> method</p>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb262-1" title="1"><span class="kw">library</span>(factoextra) <span class="co">## a new package for kmeans viz, please install</span></a>
<a class="sourceLine" id="cb262-2" title="2">p1 &lt;-<span class="st"> </span><span class="kw">fviz_cluster</span>(k1, <span class="dt">data =</span> df)</a>
<a class="sourceLine" id="cb262-3" title="3">p2 &lt;-<span class="st"> </span><span class="kw">fviz_cluster</span>(k2, <span class="dt">data =</span> df)</a>
<a class="sourceLine" id="cb262-4" title="4">p3 &lt;-<span class="st"> </span><span class="kw">fviz_cluster</span>(k3, <span class="dt">data =</span> df)</a>
<a class="sourceLine" id="cb262-5" title="5">p4 &lt;-<span class="st"> </span><span class="kw">fviz_cluster</span>(k4, <span class="dt">data =</span> df)</a>
<a class="sourceLine" id="cb262-6" title="6">p5 &lt;-<span class="st"> </span><span class="kw">fviz_cluster</span>(k5, <span class="dt">data =</span> df)</a>
<a class="sourceLine" id="cb262-7" title="7"></a>
<a class="sourceLine" id="cb262-8" title="8"><span class="co">## for arranging plots</span></a>
<a class="sourceLine" id="cb262-9" title="9"><span class="kw">library</span>(patchwork) </a>
<a class="sourceLine" id="cb262-10" title="10">(p1<span class="op">|</span><span class="st"> </span>p2<span class="op">|</span><span class="st"> </span>p3)<span class="op">/</span><span class="st"> </span>(p4 <span class="op">|</span><span class="st"> </span>p5)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-167-1.png" width="672" /></p>
<p>Alternatively, you can use standard pairwise scatter plots to illustrate the clusters compared to the original variables.</p>
<div class="sourceCode" id="cb263"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb263-1" title="1">df <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb263-2" title="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">cluster =</span> k3<span class="op">$</span>cluster,</a>
<a class="sourceLine" id="cb263-3" title="3">         <span class="dt">species =</span> penguins_nafree<span class="op">$</span>species) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb263-4" title="4"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(flipper_length_mm, bill_depth_mm, <span class="dt">color =</span> <span class="kw">factor</span>(cluster), <span class="dt">label =</span> species)) <span class="op">+</span></a>
<a class="sourceLine" id="cb263-5" title="5"><span class="st">  </span><span class="kw">geom_text</span>()</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-168-1.png" width="672" /></p>
<p><strong>Elbow</strong> method</p>
<p>Optimal clusters are at the point in which the knee ‚Äúbends‚Äù or in mathematical terms when the marginal total within sum of squares (<code>tot.withinss</code>) for an additional cluster begins to decrease at a linear rate</p>
<p>This is easier to see via a plot:</p>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb264-1" title="1"><span class="kw">fviz_nbclust</span>(df, kmeans, <span class="dt">method =</span> <span class="st">&quot;wss&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb264-2" title="2"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">subtitle =</span> <span class="st">&quot;Elbow method&quot;</span>)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-169-1.png" width="672" /></p>
<p>There is a pretty obvious inflection (elbow) at 2 clusters, but maybe at 3 too. We can rule out an optimal number of clusters above 3 as there is then only a minimal marginal reduction in total within sum of squares. However, the model is ambiguous on whether 2 or 3 clusters is optimal‚Ä¶</p>
<p><strong>Silhouette</strong> method</p>
<div class="sourceCode" id="cb265"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb265-1" title="1"><span class="co"># Silhouette method</span></a>
<a class="sourceLine" id="cb265-2" title="2"><span class="kw">fviz_nbclust</span>(df, kmeans, <span class="dt">method =</span> <span class="st">&quot;silhouette&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb265-3" title="3"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">subtitle =</span> <span class="st">&quot;Silhouette method&quot;</span>)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-170-1.png" width="672" /></p>
<p><strong>Gap</strong> method</p>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb266-1" title="1"><span class="co"># Gap statistic</span></a>
<a class="sourceLine" id="cb266-2" title="2"><span class="co"># recommended value: nboot = 500 for your analysis (it will take a while)</span></a>
<a class="sourceLine" id="cb266-3" title="3"><span class="kw">set.seed</span>(<span class="dv">123</span>) <span class="co">## remove this</span></a>
<a class="sourceLine" id="cb266-4" title="4"><span class="kw">fviz_nbclust</span>(df, kmeans, <span class="dt">nstart =</span> <span class="dv">25</span>,  <span class="dt">method =</span> <span class="st">&quot;gap_stat&quot;</span>, <span class="dt">nboot =</span> <span class="dv">50</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb266-5" title="5"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">subtitle =</span> <span class="st">&quot;Gap statistic method&quot;</span>)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-171-1.png" width="672" /></p>
<p><strong>Basically it‚Äôs up to you to collate all the suggestions and make and informed decision</strong></p>
<div class="sourceCode" id="cb267"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb267-1" title="1"><span class="co">## Trying all the cluster indecies AHHHHH</span></a>
<a class="sourceLine" id="cb267-2" title="2"><span class="kw">library</span>(NbClust)</a>
<a class="sourceLine" id="cb267-3" title="3">cluster_<span class="dv">30</span>_indexes &lt;-<span class="st"> </span><span class="kw">NbClust</span>(<span class="dt">data =</span> df, <span class="dt">distance =</span> <span class="st">&quot;euclidean&quot;</span>, <span class="dt">min.nc =</span> <span class="dv">2</span>, <span class="dt">max.nc =</span> <span class="dv">9</span>, <span class="dt">method =</span> <span class="st">&quot;complete&quot;</span>, <span class="dt">index =</span><span class="st">&quot;all&quot;</span>)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-172-1.png" width="672" /></p>
<pre><code>## *** : The Hubert index is a graphical method of determining the number of clusters.
##                 In the plot of Hubert index, we seek a significant knee that corresponds to a 
##                 significant increase of the value of the measure i.e the significant peak in Hubert
##                 index second differences plot. 
## </code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-172-2.png" width="672" /></p>
<pre><code>## *** : The D index is a graphical method of determining the number of clusters. 
##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex
##                 second differences plot) that corresponds to a significant increase of the value of
##                 the measure. 
##  
## ******************************************************************* 
## * Among all indices:                                                
## * 5 proposed 2 as the best number of clusters 
## * 6 proposed 3 as the best number of clusters 
## * 1 proposed 4 as the best number of clusters 
## * 4 proposed 5 as the best number of clusters 
## * 1 proposed 8 as the best number of clusters 
## * 3 proposed 9 as the best number of clusters 
## 
##                    ***** Conclusion *****                            
##  
## * According to the majority rule, the best number of clusters is  3 
##  
##  
## *******************************************************************</code></pre>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb270-1" title="1"><span class="kw">fviz_nbclust</span>(cluster_<span class="dv">30</span>_indexes) <span class="op">+</span></a>
<a class="sourceLine" id="cb270-2" title="2"><span class="st">      </span><span class="kw">theme_minimal</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb270-3" title="3"><span class="st">      </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Frequency of Optimal Clusters using 30 indexes in NbClust Package&quot;</span>)</a></code></pre></div>
<pre><code>## Among all indices: 
## ===================
## * 2 proposed  0 as the best number of clusters
## * 1 proposed  1 as the best number of clusters
## * 5 proposed  2 as the best number of clusters
## * 6 proposed  3 as the best number of clusters
## * 1 proposed  4 as the best number of clusters
## * 4 proposed  5 as the best number of clusters
## * 1 proposed  8 as the best number of clusters
## * 3 proposed  9 as the best number of clusters
## * 3 proposed  NA&#39;s as the best number of clusters
## 
## Conclusion
## =========================
## * According to the majority rule, the best number of clusters is  3 .</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-172-3.png" width="672" /></p>
<p>Not obvious, basically still undecided between 2 and 3, but according to the absolute majority rule the ‚Äúbest‚Äù number is 3</p>
</div>
</div>
<div id="hierarchical-agglomerative-clustering." class="section level3">
<h3>Hierarchical agglomerative clustering.</h3>
<p>Most of the hierarchical methods are agglomerative and they operate in the
same way:</p>
<ol style="list-style-type: decimal">
<li>All sampling units that are zero distance (however this is defined) apart are fused into clusters.</li>
<li>The threshold for fusion is then raised from zero until two clusters (they may be individual points) are found that are close enough to fuse.</li>
<li>The threshold is raised, fusing the clusters as their distance apart is reached until all the clusters have been fused into one big one.</li>
</ol>
<p>So, the close clusters are fused first, then those further apart, till all have been fused. This process allows the history of the fusions, the hierarchy, to be displayed as a dendrogram. This is an advantage of the agglomerative methods, if the data have a nested structure these techniques lead to a useful way of displaying it.</p>
<p>Unlike the k-means most of the agglomerative techniques can use a broad range of similarity or <strong>distance measures</strong>. This, however, means that considerable care must be taken to choose the appropriate one; different measures often lead to different results.</p>
<div id="single-linkage-nearest-neighbour-clustering." class="section level4">
<h4>Single linkage (nearest neighbour) clustering.</h4>
<p>Single Linkage (<em>nearest neighbour/minimal jump</em>): Computes the distance between clusters as the smallest distance between any two points in the two clusters.</p>
<p align="center">
<img src="img/single_linkage.png" />
</p>
<p>Single Linkage identifies clusters based on how far apart they are at their closest points. This means that if there are any intermediate points then single linkage will fuse the groups without leaving any trace of their separate identities. This is called <em>chaining</em> and it often leads to uninformative dendrograms.</p>
<p>If, however, the clusters are well separated in the data, then single linkage can handle groups of different shapes and sizes easily. In addition, single linkage will give the same clustering after any monotonic transformation of the distance measure (i.e., it is fairly robust to the choice of measure).</p>
</div>
<div id="complete-linkage-farthest-neighbour-clustering." class="section level4">
<h4>Complete linkage (farthest neighbour) clustering.</h4>
<p>Instead of measuring the distance between two clusters as that between their two nearest members <strong>complete Linkage</strong> (<em>maximum jump</em>) uses that between the two farthest members (i.e., it calculates the maximum distance between two points from each cluster.)</p>
<p align="center">
<img src="img/maximum_linkage.png" />
</p>
<p>The resulting clusters are often compact, spherical and well defined. Complete linkage can, however, be sensitive to tied distances. Although, it too, is robust to a certain amount of measurement error and choice of distance.</p>
</div>
<div id="group-average-linkage-upgma" class="section level4">
<h4>Group average linkage (UPGMA)</h4>
<p>Group average linkage (UPGMA) is probably the most popular hierarchical clustering method! You might like to think of it as an attempt to avoid the extremes of the single and complete linkage methods as the distance between two clusters is the average of the distances between the members of the two groups. As a result this method tends to produce compact spherical clusters.</p>
</div>
<div id="wards-method-incremental-sums-of-squares-minimum-variance-agglomerative-sums-of-squares." class="section level4">
<h4>Ward‚Äôs method (incremental sums of squares, minimum variance, agglomerative sums of squares).</h4>
<p>Ward‚Äôs method is the hierarchical version of the k-means partitioning method. At each fusion it
attempts to minimise the increase in total sum of squared distances within the clusters. This is
equivalent to minimising the sum of squared within cluster deviations from the centroids</p>
<p aligh="center">
<img src="img/wards_linkage.png" />
</p>
<p>Ward‚Äôs method, at any one stage, can only fuse those clusters already in existence (i.e., it is not
allowed to reallocate points). A bad start to the agglomeration process can
place the algorithm on a path from which it can never reach the global optimum for a given number
of clusters. Its chief flaw is a tendency to form clusters of equal size, regardless of the true number. Like the complete linkage and group average methods it is also biased towards forming spherical clusters. Despite this, Ward‚Äôs method performs well in simulations and is often method of choice.</p>
</div>
</div>
<div id="hierarchical-clustering-an-example" class="section level3">
<h3>Hierarchical clustering: an example</h3>
<p><strong>In summary</strong></p>
<ul>
<li>Start with a matrix of distances, (or similarities) between pairs of observations (cases)
<ul>
<li>Choice of distance measure key first step</li>
<li>Algorithm:
<ul>
<li>Initial <span class="math inline">\(n\)</span> singleton clusters</li>
<li>Scan distance matrix for two closest individuals, group them together</li>
<li>Compute distance from cluster of size 2 to remaining <span class="math inline">\(n-1\)</span> singleton clusters</li>
</ul></li>
</ul></li>
</ul>
<table>
<thead>
<tr class="header">
<th>Method</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Single linkage</td>
<td>number of clusters</td>
<td>comb-like trees.</td>
</tr>
<tr class="even">
<td>Complete linkage</td>
<td>compact clusters</td>
<td>one obs. can alter groups</td>
</tr>
<tr class="odd">
<td>Average linkage</td>
<td>similar size and variance</td>
<td>not robust</td>
</tr>
<tr class="even">
<td>Centroid</td>
<td>robust to outliers</td>
<td>smaller number of clusters</td>
</tr>
<tr class="odd">
<td>Ward</td>
<td>minimising an inertia</td>
<td>clusters small if high variability</td>
</tr>
</tbody>
</table>
<div id="ants" class="section level4">
<h4>Ants</h4>
<p>Data were collected on the distribution of ant species at 30 sites across the Auckland region using pitfall traps. Twenty pitfall traps at each site were left open for ten days and the number of individuals captured counted for the four most abundant species: <em>Nylanderia spp</em>, <em>Pheidole rugosula</em>, <em>Tetramorium grassii</em>, and <em>Pachycondyla sp</em>.</p>
<div class="sourceCode" id="cb272"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb272-1" title="1"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb272-2" title="2">ants &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/pitfalls.csv&quot;</span>)</a>
<a class="sourceLine" id="cb272-3" title="3">ants</a></code></pre></div>
<pre><code>## # A tibble: 30 √ó 8
##    Location Habitat Month Site    Nyl   Phe   Tet   Pac
##    &lt;chr&gt;    &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 West     Forest      1 WF1       0     0     0   157
##  2 West     Grass       1 WG1       0     2     7    37
##  3 West     Urban       1 WU1       3     7     0     0
##  4 West     Forest      2 WF2       0     0     0    31
##  5 West     Grass       2 WG2       5     0    25     0
##  6 West     Forest      3 WF3       0     0     0    21
##  7 West     Grass       3 WG3       0     3     2     1
##  8 West     Urban       3 WU3       0     1     0     0
##  9 Central  Forest      1 CF1       0     0     0     1
## 10 Central  Grass       1 CG1       0     3    22     2
## # ‚Ä¶ with 20 more rows</code></pre>
<p>Data are species counts, so we will use Bray Curtis measure:</p>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb274-1" title="1">pitfall.dist &lt;-<span class="st"> </span>vegan<span class="op">::</span><span class="kw">vegdist</span>(ants[,<span class="dv">5</span><span class="op">:</span><span class="dv">8</span>], <span class="dt">method =</span> <span class="st">&quot;bray&quot;</span>, <span class="dt">binary =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb274-2" title="2">factoextra<span class="op">::</span><span class="kw">fviz_dist</span>(pitfall.dist)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-174-1.png" width="672" /></p>
</div>
<div id="single-linkage" class="section level4">
<h4>Single-linkage</h4>
<div class="sourceCode" id="cb275"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb275-1" title="1">single &lt;-<span class="st"> </span>ants[,<span class="dv">5</span><span class="op">:</span><span class="dv">8</span>] <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb275-2" title="2"><span class="st">  </span>vegan<span class="op">::</span><span class="kw">vegdist</span>(., <span class="dt">method =</span> <span class="st">&quot;bray&quot;</span>, <span class="dt">binary =</span> <span class="ot">FALSE</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb275-3" title="3"><span class="st">  </span><span class="kw">hclust</span>(<span class="dt">method =</span> <span class="st">&quot;single&quot;</span>)</a>
<a class="sourceLine" id="cb275-4" title="4"><span class="kw">plot</span>(single, <span class="dt">labels =</span> ants<span class="op">$</span>Site)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-175-1.png" width="672" /></p>
</div>
<div id="maximum-linkage" class="section level4">
<h4>Maximum linkage</h4>
<div class="sourceCode" id="cb276"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb276-1" title="1">complete &lt;-<span class="st"> </span>ants[,<span class="dv">5</span><span class="op">:</span><span class="dv">8</span>] <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb276-2" title="2"><span class="st">  </span>vegan<span class="op">::</span><span class="kw">vegdist</span>(., <span class="dt">method =</span> <span class="st">&quot;bray&quot;</span>, <span class="dt">binary =</span> <span class="ot">FALSE</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb276-3" title="3"><span class="st">  </span><span class="kw">hclust</span>(<span class="dt">method =</span> <span class="st">&quot;complete&quot;</span>)</a>
<a class="sourceLine" id="cb276-4" title="4"><span class="kw">plot</span>(complete, <span class="dt">labels =</span> ants<span class="op">$</span>Site)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-176-1.png" width="672" /></p>
</div>
<div id="average-linkage-upgma" class="section level4">
<h4>Average linkage (UPGMA)</h4>
<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb277-1" title="1">average &lt;-<span class="st"> </span>ants[,<span class="dv">5</span><span class="op">:</span><span class="dv">8</span>] <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb277-2" title="2"><span class="st">  </span>vegan<span class="op">::</span><span class="kw">vegdist</span>(., <span class="dt">method =</span> <span class="st">&quot;bray&quot;</span>, <span class="dt">binary =</span> <span class="ot">FALSE</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb277-3" title="3"><span class="st">  </span><span class="kw">hclust</span>(<span class="dt">method =</span> <span class="st">&quot;average&quot;</span>)</a>
<a class="sourceLine" id="cb277-4" title="4"><span class="kw">plot</span>(average, <span class="dt">labels =</span> ants<span class="op">$</span>Site)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-177-1.png" width="672" /></p>
</div>
<div id="wards-method" class="section level4">
<h4>Ward‚Äôs method</h4>
<div class="sourceCode" id="cb278"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb278-1" title="1">ward &lt;-<span class="st"> </span>ants[,<span class="dv">5</span><span class="op">:</span><span class="dv">8</span>] <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb278-2" title="2"><span class="st">  </span>vegan<span class="op">::</span><span class="kw">vegdist</span>(., <span class="dt">method =</span> <span class="st">&quot;bray&quot;</span>, <span class="dt">binary =</span> <span class="ot">FALSE</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb278-3" title="3"><span class="st">  </span><span class="kw">hclust</span>(<span class="dt">method =</span> <span class="st">&quot;ward.D&quot;</span>)</a>
<a class="sourceLine" id="cb278-4" title="4"><span class="kw">plot</span>(ward, <span class="dt">labels =</span> ants<span class="op">$</span>Site)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-178-1.png" width="672" /></p>
</div>
<div id="what-are-dendrograms-good-for-suggesting-clusters-for-further-study" class="section level4">
<h4>WHAT ARE DENDROGRAMS GOOD FOR? <em>Suggesting clusters for further study‚Ä¶</em></h4>
<p>Using the function <code>cutree()</code> to split into clusters and plot:</p>
<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb279-1" title="1">ants<span class="op">$</span>clust4 &lt;-<span class="st"> </span><span class="kw">cutree</span>(ward, <span class="dt">k =</span> <span class="dv">4</span>)</a>
<a class="sourceLine" id="cb279-2" title="2"><span class="kw">library</span>(ape)   <span class="co">## install</span></a>
<a class="sourceLine" id="cb279-3" title="3">pitfall.phylo &lt;-<span class="st"> </span><span class="kw">as.phylo</span>(ward)</a>
<a class="sourceLine" id="cb279-4" title="4">pitfall.phylo<span class="op">$</span>tip.label &lt;-<span class="st"> </span>ants<span class="op">$</span>Site</a>
<a class="sourceLine" id="cb279-5" title="5"><span class="co">## Set colours </span></a>
<a class="sourceLine" id="cb279-6" title="6">colours  &lt;-<span class="st">  </span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;green&quot;</span>,<span class="st">&quot;black&quot;</span>)</a>
<a class="sourceLine" id="cb279-7" title="7"><span class="kw">plot</span>(pitfall.phylo, <span class="dt">cex =</span> <span class="fl">0.6</span>, <span class="dt">tip.color =</span> colours[ants<span class="op">$</span>clust4], <span class="dt">label.offset =</span> <span class="fl">0.05</span>) </a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-179-1.png" width="672" /></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="module-6.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="principal-component-analysis-pca.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
