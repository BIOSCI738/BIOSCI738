<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Maximum likelihood estimation | Advanced Biological Data Analysis</title>
  <meta name="description" content="Maximum likelihood estimation | Advanced Biological Data Analysis" />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="Maximum likelihood estimation | Advanced Biological Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Maximum likelihood estimation | Advanced Biological Data Analysis" />
  
  
  

<meta name="author" content="University of Auckland" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="least-squares-estimation.html"/>
<link rel="next" href="introduction-to-bayesian-statistics.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Nau mai, haere mai. Welcome to BIOSCI 738</a></li>
<li class="chapter" data-level="" data-path="useful-information-to-set-you-up-for-your-semester.html"><a href="useful-information-to-set-you-up-for-your-semester.html"><i class="fa fa-check"></i>Useful information to set you up for your semester</a>
<ul>
<li class="chapter" data-level="" data-path="useful-information-to-set-you-up-for-your-semester.html"><a href="useful-information-to-set-you-up-for-your-semester.html#course-outline"><i class="fa fa-check"></i>Course outline</a></li>
<li class="chapter" data-level="" data-path="useful-information-to-set-you-up-for-your-semester.html"><a href="useful-information-to-set-you-up-for-your-semester.html#learning-outcomes"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="" data-path="useful-information-to-set-you-up-for-your-semester.html"><a href="useful-information-to-set-you-up-for-your-semester.html#course-summary"><i class="fa fa-check"></i>Course summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="module-1.html"><a href="module-1.html"><i class="fa fa-check"></i>Module 1</a>
<ul>
<li class="chapter" data-level="" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html"><i class="fa fa-check"></i><code>R</code> and <code>RStudio</code></a>
<ul>
<li class="chapter" data-level="" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#installing-r-and-rstudio"><i class="fa fa-check"></i>Installing R and RStudio</a></li>
<li class="chapter" data-level="" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#getting-started"><i class="fa fa-check"></i>Getting started</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="reproducible-research.html"><a href="reproducible-research.html"><i class="fa fa-check"></i>Reproducible research</a>
<ul>
<li class="chapter" data-level="" data-path="reproducible-research.html"><a href="reproducible-research.html#project-oriented-workflow-good-practice"><i class="fa fa-check"></i>Project-oriented workflow: good practice</a></li>
<li class="chapter" data-level="" data-path="reproducible-research.html"><a href="reproducible-research.html#version-control-with-git-and-github"><i class="fa fa-check"></i>Version control with <code>git</code> and GitHub</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="your-data-and-r.html"><a href="your-data-and-r.html"><i class="fa fa-check"></i>Your data and <code>R</code></a>
<ul>
<li class="chapter" data-level="" data-path="your-data-and-r.html"><a href="your-data-and-r.html#reading-in-data-from-a-.csv-file"><i class="fa fa-check"></i>Reading in data from a <code>.csv</code> file</a></li>
<li class="chapter" data-level="" data-path="your-data-and-r.html"><a href="your-data-and-r.html#explore-your-data"><i class="fa fa-check"></i>Explore your data</a></li>
<li class="chapter" data-level="" data-path="your-data-and-r.html"><a href="your-data-and-r.html#the-pipe-operator"><i class="fa fa-check"></i>The pipe operator <code>%&gt;%</code></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="data-wrangling.html"><a href="data-wrangling.html"><i class="fa fa-check"></i>Data wrangling</a>
<ul>
<li class="chapter" data-level="" data-path="data-wrangling.html"><a href="data-wrangling.html#introuducing-the-palmer-penguins"><i class="fa fa-check"></i>Introuducing the <span>Palmer penguins</span></a></li>
<li class="chapter" data-level="" data-path="data-wrangling.html"><a href="data-wrangling.html#common-dataframe-manipulations-in-the-tidyverse-using-dplyr-and-tidyr"><i class="fa fa-check"></i>Common dataframe manipulations in the <code>tidyverse</code>, using <code>dplyr</code> and <code>tidyr</code></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="data-visualiation-data-viz.html"><a href="data-visualiation-data-viz.html"><i class="fa fa-check"></i>Data visualiation (data viz)</a>
<ul>
<li class="chapter" data-level="" data-path="data-visualiation-data-viz.html"><a href="data-visualiation-data-viz.html#exploratory-plots"><i class="fa fa-check"></i>Exploratory plots</a></li>
<li class="chapter" data-level="" data-path="data-visualiation-data-viz.html"><a href="data-visualiation-data-viz.html#explanatory-plots"><i class="fa fa-check"></i>Explanatory plots</a></li>
<li class="chapter" data-level="" data-path="data-visualiation-data-viz.html"><a href="data-visualiation-data-viz.html#ten-simple-rules-for-better-figures"><i class="fa fa-check"></i>Ten Simple Rules for Better Figures</a></li>
<li class="chapter" data-level="" data-path="data-visualiation-data-viz.html"><a href="data-visualiation-data-viz.html#ggplot2"><i class="fa fa-check"></i><code>ggplot2</code></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="māori-data-sovereignty-principles.html"><a href="māori-data-sovereignty-principles.html"><i class="fa fa-check"></i>Māori Data Sovereignty principles</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="module-2.html"><a href="module-2.html"><i class="fa fa-check"></i>Module 2</a>
<ul>
<li class="chapter" data-level="" data-path="key-satistical-concepts.html"><a href="key-satistical-concepts.html"><i class="fa fa-check"></i>Key satistical concepts</a></li>
<li class="chapter" data-level="" data-path="bootstrap-resampling.html"><a href="bootstrap-resampling.html"><i class="fa fa-check"></i>Bootstrap resampling</a>
<ul>
<li class="chapter" data-level="" data-path="bootstrap-resampling.html"><a href="bootstrap-resampling.html#example-constructing-bootstrap-confidence-intervals"><i class="fa fa-check"></i>Example: constructing bootstrap confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="permutation-and-randomisation-tests.html"><a href="permutation-and-randomisation-tests.html"><i class="fa fa-check"></i>Permutation and randomisation tests</a>
<ul>
<li class="chapter" data-level="" data-path="permutation-and-randomisation-tests.html"><a href="permutation-and-randomisation-tests.html#a-permutation-test-jackal-mandible-lengths"><i class="fa fa-check"></i>A permutation test: Jackal mandible lengths</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="resampling-procedures-the-differences.html"><a href="resampling-procedures-the-differences.html"><i class="fa fa-check"></i>Resampling procedures, the differences</a></li>
<li class="chapter" data-level="" data-path="hypothesis-testing-key-terms.html"><a href="hypothesis-testing-key-terms.html"><i class="fa fa-check"></i>Hypothesis testing: key terms</a>
<ul>
<li class="chapter" data-level="" data-path="hypothesis-testing-key-terms.html"><a href="hypothesis-testing-key-terms.html#p-values"><i class="fa fa-check"></i>😱 p-values 😱</a></li>
<li class="chapter" data-level="" data-path="hypothesis-testing-key-terms.html"><a href="hypothesis-testing-key-terms.html#type-i-and-type-ii-errors"><i class="fa fa-check"></i>Type I and Type II errors</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="parametric-hypothesis-tests.html"><a href="parametric-hypothesis-tests.html"><i class="fa fa-check"></i>Parametric hypothesis tests</a>
<ul>
<li class="chapter" data-level="" data-path="parametric-hypothesis-tests.html"><a href="parametric-hypothesis-tests.html#one-sample-t-test"><i class="fa fa-check"></i>One-Sample t-test</a></li>
<li class="chapter" data-level="" data-path="parametric-hypothesis-tests.html"><a href="parametric-hypothesis-tests.html#differences-between-two-means"><i class="fa fa-check"></i>Differences between two means</a></li>
<li class="chapter" data-level="" data-path="parametric-hypothesis-tests.html"><a href="parametric-hypothesis-tests.html#one-way-analysis-of-variance-anova"><i class="fa fa-check"></i>One-Way Analysis of Variance (ANOVA)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i>Linear regression</a>
<ul>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#some-mathematical-notation"><i class="fa fa-check"></i>Some mathematical notation</a></li>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#modeling-bill-depth"><i class="fa fa-check"></i>Modeling Bill Depth</a></li>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#single-continuous-variable"><i class="fa fa-check"></i>Single continuous variable</a></li>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#one-factor-and-a-continuous-variable"><i class="fa fa-check"></i>One factor and a continuous variable</a></li>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#interactions"><i class="fa fa-check"></i>Interactions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="model-formula-syntax-in-r.html"><a href="model-formula-syntax-in-r.html"><i class="fa fa-check"></i>Model formula syntax in <code>R</code></a>
<ul>
<li class="chapter" data-level="" data-path="model-formula-syntax-in-r.html"><a href="model-formula-syntax-in-r.html#other-possible-models"><i class="fa fa-check"></i>Other possible models</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="model-diagnostics-for-a-linear-model.html"><a href="model-diagnostics-for-a-linear-model.html"><i class="fa fa-check"></i>Model diagnostics for a linear model</a>
<ul>
<li class="chapter" data-level="" data-path="model-diagnostics-for-a-linear-model.html"><a href="model-diagnostics-for-a-linear-model.html#marginal-predictions"><i class="fa fa-check"></i>Marginal predictions</a></li>
<li class="chapter" data-level="" data-path="model-diagnostics-for-a-linear-model.html"><a href="model-diagnostics-for-a-linear-model.html#model-selection"><i class="fa fa-check"></i>Model selection</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="inference-for-a-linear-model.html"><a href="inference-for-a-linear-model.html"><i class="fa fa-check"></i>Inference for a linear model</a>
<ul>
<li class="chapter" data-level="" data-path="inference-for-a-linear-model.html"><a href="inference-for-a-linear-model.html#point-prediction"><i class="fa fa-check"></i>Point prediction</a></li>
<li class="chapter" data-level="" data-path="inference-for-a-linear-model.html"><a href="inference-for-a-linear-model.html#confidence-intervals-for-parameters"><i class="fa fa-check"></i>Confidence intervals for parameters</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="module-3.html"><a href="module-3.html"><i class="fa fa-check"></i>Module 3</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html"><i class="fa fa-check"></i>Introduction to the design and analysis of experiments</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#key-phrases"><i class="fa fa-check"></i>Key phrases</a></li>
<li class="chapter" data-level="" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#the-three-key-principles"><i class="fa fa-check"></i>The three key principles:</a></li>
<li class="chapter" data-level="" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#replication"><i class="fa fa-check"></i>Replication</a></li>
<li class="chapter" data-level="" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#randomization"><i class="fa fa-check"></i>Randomization</a></li>
<li class="chapter" data-level="" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#blocking"><i class="fa fa-check"></i>Blocking</a></li>
<li class="chapter" data-level="" data-path="introduction-to-the-design-and-analysis-of-experiments.html"><a href="introduction-to-the-design-and-analysis-of-experiments.html#setting-up-an-experiment"><i class="fa fa-check"></i>Setting up an experiment</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="some-basic-experimental-designs.html"><a href="some-basic-experimental-designs.html"><i class="fa fa-check"></i>Some basic experimental designs</a>
<ul>
<li class="chapter" data-level="" data-path="some-basic-experimental-designs.html"><a href="some-basic-experimental-designs.html#completely-randomised-design-crd"><i class="fa fa-check"></i>Completely randomised design (CRD)</a></li>
<li class="chapter" data-level="" data-path="some-basic-experimental-designs.html"><a href="some-basic-experimental-designs.html#randomised-complete-block-design-rcbd"><i class="fa fa-check"></i>Randomised complete block design (RCBD)</a></li>
<li class="chapter" data-level="" data-path="some-basic-experimental-designs.html"><a href="some-basic-experimental-designs.html#factorial-design"><i class="fa fa-check"></i>Factorial design</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="modelling-experimental-data.html"><a href="modelling-experimental-data.html"><i class="fa fa-check"></i>Modelling experimental data</a>
<ul>
<li class="chapter" data-level="" data-path="modelling-experimental-data.html"><a href="modelling-experimental-data.html#a-completely-randomised-design-crd-as-a-linear-model"><i class="fa fa-check"></i>A completely randomised design (CRD) as a linear model</a></li>
<li class="chapter" data-level="" data-path="modelling-experimental-data.html"><a href="modelling-experimental-data.html#analysis-of-a-crd-in-r"><i class="fa fa-check"></i>Analysis of a CRD in <code>R</code></a></li>
<li class="chapter" data-level="" data-path="modelling-experimental-data.html"><a href="modelling-experimental-data.html#a-factorial-experiment-as-a-crd"><i class="fa fa-check"></i>A Factorial experiment (as a CRD)</a></li>
<li class="chapter" data-level="" data-path="modelling-experimental-data.html"><a href="modelling-experimental-data.html#unqual-replications-unbalanced-design"><i class="fa fa-check"></i>Unqual replications (unbalanced design)</a></li>
<li class="chapter" data-level="" data-path="modelling-experimental-data.html"><a href="modelling-experimental-data.html#sums-of-squares-ss"><i class="fa fa-check"></i>Sums of squares (<span class="math inline">\(SS\)</span>)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html"><i class="fa fa-check"></i>Multiple comparisons</a>
<ul>
<li class="chapter" data-level="" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#adjustments-for-multiple-testing"><i class="fa fa-check"></i>Adjustments for multiple testing</a></li>
<li class="chapter" data-level="" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#classification-of-multiple-hypothesis-tests"><i class="fa fa-check"></i>Classification of multiple hypothesis tests</a></li>
<li class="chapter" data-level="" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html#multiple-comparison-procedures"><i class="fa fa-check"></i>Multiple comparison procedures</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="linear-mixed-effect-models-lmms.html"><a href="linear-mixed-effect-models-lmms.html"><i class="fa fa-check"></i>Linear mixed-effect models (LMMs)</a>
<ul>
<li class="chapter" data-level="" data-path="linear-mixed-effect-models-lmms.html"><a href="linear-mixed-effect-models-lmms.html#a-randomised-controlled-block-design-rcbd"><i class="fa fa-check"></i>A Randomised Controlled Block Design (RCBD)</a></li>
<li class="chapter" data-level="" data-path="linear-mixed-effect-models-lmms.html"><a href="linear-mixed-effect-models-lmms.html#a-split-plot-design"><i class="fa fa-check"></i>A Split-plot design</a></li>
<li class="chapter" data-level="" data-path="linear-mixed-effect-models-lmms.html"><a href="linear-mixed-effect-models-lmms.html#a-repeated-measures-design"><i class="fa fa-check"></i>A repeated measures design</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="module-4.html"><a href="module-4.html"><i class="fa fa-check"></i>Module 4</a>
<ul>
<li class="chapter" data-level="" data-path="least-squares-estimation.html"><a href="least-squares-estimation.html"><i class="fa fa-check"></i>Least Squares Estimation</a>
<ul>
<li class="chapter" data-level="" data-path="least-squares-estimation.html"><a href="least-squares-estimation.html#some-basic-matrix-algebra"><i class="fa fa-check"></i>Some basic matrix algebra</a></li>
<li class="chapter" data-level="" data-path="least-squares-estimation.html"><a href="least-squares-estimation.html#linear-least-squares"><i class="fa fa-check"></i>Linear least squares</a></li>
<li class="chapter" data-level="" data-path="least-squares-estimation.html"><a href="least-squares-estimation.html#matrix-representation-of-a-crd"><i class="fa fa-check"></i>Matrix representation of a CRD</a></li>
<li class="chapter" data-level="" data-path="least-squares-estimation.html"><a href="least-squares-estimation.html#a-numeric-example"><i class="fa fa-check"></i>A numeric example</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html"><i class="fa fa-check"></i>Maximum likelihood estimation</a>
<ul>
<li class="chapter" data-level="" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html#differeniation-rules"><i class="fa fa-check"></i>Differeniation rules</a></li>
<li class="chapter" data-level="" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html#logarithm-rules"><i class="fa fa-check"></i>Logarithm rules</a></li>
<li class="chapter" data-level="" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html#maximum-likelihood-estimation-for-a-binomial-distribution"><i class="fa fa-check"></i>Maximum likelihood estimation for a Binomial distribution</a></li>
<li class="chapter" data-level="" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html#maximum-likelihood-estimation-for-a-crd"><i class="fa fa-check"></i>Maximum likelihood estimation for a CRD</a></li>
<li class="chapter" data-level="" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html#maximising-the-log-likelihood"><i class="fa fa-check"></i>Maximising the log-likelihood</a></li>
<li class="chapter" data-level="" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html#maximum-likelihood-estimation-for-a-poisson-distribution"><i class="fa fa-check"></i>Maximum likelihood estimation for a Poisson distribution</a></li>
<li class="chapter" data-level="" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html#maximum-likelihood-estimation-for-a-continuous-random-variable"><i class="fa fa-check"></i>Maximum likelihood estimation for a continuous random variable</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction-to-bayesian-statistics.html"><a href="introduction-to-bayesian-statistics.html"><i class="fa fa-check"></i>Introduction to Bayesian statistics</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-bayesian-statistics.html"><a href="introduction-to-bayesian-statistics.html#conditional-probability"><i class="fa fa-check"></i>Conditional probability</a></li>
<li class="chapter" data-level="" data-path="introduction-to-bayesian-statistics.html"><a href="introduction-to-bayesian-statistics.html#bayes-rule"><i class="fa fa-check"></i>Bayes’ rule</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="module-5.html"><a href="module-5.html"><i class="fa fa-check"></i>Module 5</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-generalised-linear-models-glms.html"><a href="introduction-to-generalised-linear-models-glms.html"><i class="fa fa-check"></i>Introduction to generalised linear models (GLMs)</a></li>
<li class="chapter" data-level="" data-path="poisson-regression.html"><a href="poisson-regression.html"><i class="fa fa-check"></i>Poisson regression</a>
<ul>
<li class="chapter" data-level="" data-path="poisson-regression.html"><a href="poisson-regression.html#an-example-bird-abundance"><i class="fa fa-check"></i>An example: bird abundance</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i>Logistic regression</a>
<ul>
<li class="chapter" data-level="" data-path="logistic-regression.html"><a href="logistic-regression.html#an-example-lobsters"><i class="fa fa-check"></i>An example: lobsters</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="a-summary-of-glms.html"><a href="a-summary-of-glms.html"><i class="fa fa-check"></i>A summary of GLMs</a></li>
<li class="chapter" data-level="" data-path="generalised-linear-mixed-effects-models-glmmms.html"><a href="generalised-linear-mixed-effects-models-glmmms.html"><i class="fa fa-check"></i>Generalised linear mixed-effects models (GLMMMs)</a>
<ul>
<li class="chapter" data-level="" data-path="generalised-linear-mixed-effects-models-glmmms.html"><a href="generalised-linear-mixed-effects-models-glmmms.html#fitting-a-glmm"><i class="fa fa-check"></i>Fitting a GLMM</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="module-6.html"><a href="module-6.html"><i class="fa fa-check"></i>Module 6</a>
<ul>
<li class="chapter" data-level="" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i>Clustering</a>
<ul>
<li class="chapter" data-level="" data-path="clustering.html"><a href="clustering.html#divisive-partitioning-methods."><i class="fa fa-check"></i>Divisive (partitioning) methods.</a></li>
<li class="chapter" data-level="" data-path="clustering.html"><a href="clustering.html#k-means-an-example-using-the-palmerpenguins-data"><i class="fa fa-check"></i>K-means: an example using the <code>palmerpenguins</code> data</a></li>
<li class="chapter" data-level="" data-path="clustering.html"><a href="clustering.html#hierarchical-agglomerative-clustering."><i class="fa fa-check"></i>Hierarchical agglomerative clustering.</a></li>
<li class="chapter" data-level="" data-path="clustering.html"><a href="clustering.html#hierarchical-clustering-an-example"><i class="fa fa-check"></i>Hierarchical clustering: an example</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="principal-component-analysis-pca.html"><a href="principal-component-analysis-pca.html"><i class="fa fa-check"></i>Principal Component Analysis (PCA)</a>
<ul>
<li class="chapter" data-level="" data-path="principal-component-analysis-pca.html"><a href="principal-component-analysis-pca.html#examples-in-r"><i class="fa fa-check"></i>Examples in <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="multidimensional-scaling-mds.html"><a href="multidimensional-scaling-mds.html"><i class="fa fa-check"></i>Multidimensional Scaling (MDS)</a>
<ul>
<li class="chapter" data-level="" data-path="multidimensional-scaling-mds.html"><a href="multidimensional-scaling-mds.html#metric-scaling"><i class="fa fa-check"></i>Metric Scaling</a></li>
<li class="chapter" data-level="" data-path="multidimensional-scaling-mds.html"><a href="multidimensional-scaling-mds.html#correspondence-analysis-ca"><i class="fa fa-check"></i>Correspondence Analysis (CA)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="non-metric-multidimensional-scaling.html"><a href="non-metric-multidimensional-scaling.html"><i class="fa fa-check"></i>Non-metric Multidimensional Scaling</a>
<ul>
<li class="chapter" data-level="" data-path="non-metric-multidimensional-scaling.html"><a href="non-metric-multidimensional-scaling.html#examples-in-r-2"><i class="fa fa-check"></i>Examples in <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="linear-discriminant-analysis-lda.html"><a href="linear-discriminant-analysis-lda.html"><i class="fa fa-check"></i>Linear Discriminant Analysis (LDA)</a>
<ul>
<li class="chapter" data-level="" data-path="linear-discriminant-analysis-lda.html"><a href="linear-discriminant-analysis-lda.html#example-in-r"><i class="fa fa-check"></i>Example in <code>R</code></a></li>
<li class="chapter" data-level="" data-path="linear-discriminant-analysis-lda.html"><a href="linear-discriminant-analysis-lda.html#prediction"><i class="fa fa-check"></i>Prediction</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Advanced Biological Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="maximum-likelihood-estimation" class="section level2 hasAnchor">
<h2>Maximum likelihood estimation<a href="maximum-likelihood-estimation.html#maximum-likelihood-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Under the assumptions of a linear model then maximum likelihood estimation is equivalent to least squares. However, (as we’ll see in a later module) we often need to be more flexible!</p>
<div id="differeniation-rules" class="section level3 hasAnchor">
<h3>Differeniation rules<a href="maximum-likelihood-estimation.html#differeniation-rules" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This section is a recap only, if you need a more in-depth overview of differentiation then use the extra materials provided at the start of this module.</p>
<p><strong>The constant factor rule</strong></p>
<p><span class="math display">\[(af(x))&#39;=af&#39;(x)\]</span>
<strong>The sum rule</strong></p>
<p><span class="math display">\[(f(x)+g(x))&#39;=f&#39;(x)+g&#39;(x)\]</span>
<strong>The subtraction rule</strong></p>
<p><span class="math display">\[(f(x) - g(x))&#39;=f&#39;(x) - g&#39;(x)\]</span></p>
<p><strong>The product rule</strong></p>
<p>For the functions <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span>, the derivative of the function <span class="math inline">\(h(x) = f(x)g(x)\)</span> with respect to <span class="math inline">\(x\)</span> is</p>
<p><span class="math display">\[h&#39;(x) = (fg)&#39;(x) = f&#39;(x)g(x) + f(x)g&#39;(x)\]</span></p>
<p><strong>The chain rule</strong></p>
<p>The derivative of the function <span class="math inline">\(h(x)=f(g(x))h(x)=f(g(x))\)</span> is</p>
<p><span class="math display">\[h&#39;(x)=f&#39;(g(x)\cdot g&#39;(x)\]</span></p>
<p>In summary, for any functions <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> and any real numbers <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, the derivative of the function
<span class="math inline">\(h(x) = a f(x)+bg(x)\)</span> with respect to <span class="math inline">\(x\)</span> is <span class="math inline">\(h&#39;(x)=af&#39;(x) + bg&#39;(x).\)</span></p>
</div>
<div id="logarithm-rules" class="section level3 hasAnchor">
<h3>Logarithm rules<a href="maximum-likelihood-estimation.html#logarithm-rules" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This section is a recap only, if you need a more in-depth overview of log rules then use the extra materials provided at the start of this module.</p>
<ul>
<li><strong>Product rule:</strong> <span class="math inline">\(\text{log}(xy) = \text{log}(x) + \text{log}(y)\)</span></li>
<li><strong>Quotient rule:</strong> <span class="math inline">\(\text{log}(x/y)=\text{log}(x)−\text{log}(y)\)</span></li>
<li><strong>Log of power:</strong> <span class="math inline">\(\text{log}(x^y)=y\text{log}(x)\)</span></li>
<li><strong>Log of e:</strong> <span class="math inline">\(\text{log}(e)=1\)</span></li>
<li><strong>Log of one:</strong> <span class="math inline">\(\text{log}(1)=0\)</span></li>
<li><strong>Log reciprocal:</strong> <span class="math inline">\(\text{log}(1/x)=−\text{log}(x)\)</span></li>
<li><strong>Differentiating a log:</strong> <span class="math inline">\(\frac{\delta \text{log}(y)}{\delta x} = \frac{1}{y}\frac{\delta y}{\delta x}.\)</span></li>
</ul>
</div>
<div id="maximum-likelihood-estimation-for-a-binomial-distribution" class="section level3 hasAnchor">
<h3>Maximum likelihood estimation for a Binomial distribution<a href="maximum-likelihood-estimation.html#maximum-likelihood-estimation-for-a-binomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The process of using observations to suggest a value for a parameter is called estimation. The value suggested is called the <strong>estimate</strong> of the parameter.</p>
<p>Let us consider data from the published article <a href="https://www.sciencedirect.com/science/article/pii/S0022098115000039">Influence of predator identity on the strength of predator avoidance responses in lobsters.</a>.</p>
<p>The authors were interested in how a juvenile lobster’s size was related to its vulnerability to predation. In total, 159 juvenile lobsters were collected from their natural habitat in the Gulf of Maine, USA, and the length of each lobster’s carapace (upper shell) was measured to the nearest 3 mm. The lobsters were then tethered to the ocean floor for 24 hours. Any missing lobsters were assumed to have been consumed by a predator, while the surviving lobsters were released.</p>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb282-1"><a href="maximum-likelihood-estimation.html#cb282-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb282-2"><a href="maximum-likelihood-estimation.html#cb282-2"></a>data &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/STATS-UOA/databunker/master/data/lobster.csv&quot;</span>)</span>
<span id="cb282-3"><a href="maximum-likelihood-estimation.html#cb282-3"></a>data <span class="op">%&gt;%</span></span>
<span id="cb282-4"><a href="maximum-likelihood-estimation.html#cb282-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">survived =</span> <span class="kw">ifelse</span>(survived <span class="op">==</span><span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;consumed&quot;</span>, <span class="st">&quot;alive&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb282-5"><a href="maximum-likelihood-estimation.html#cb282-5"></a><span class="st">  </span><span class="kw">group_by</span>(survived) <span class="op">%&gt;%</span></span>
<span id="cb282-6"><a href="maximum-likelihood-estimation.html#cb282-6"></a><span class="st">  </span><span class="kw">tally</span>() <span class="op">%&gt;%</span></span>
<span id="cb282-7"><a href="maximum-likelihood-estimation.html#cb282-7"></a><span class="st">  </span><span class="kw">pivot_wider</span>(<span class="dt">names_from =</span> <span class="kw">c</span>(survived), <span class="dt">values_from =</span> n)</span></code></pre></div>
<pre><code>## # A tibble: 1 × 2
##   alive consumed
##   &lt;int&gt;    &lt;int&gt;
## 1    79       80</code></pre>
<p>So, 79 of the small juvenile lobsters survived predation from a total of 159. We are interested in the probability of survival, <span class="math inline">\(\theta\)</span>, for the general population. The obvious estimate is simply to take the ratio, <span class="math inline">\(\frac{\text{number surviving juveniles}}{\text{total number juveniles}} = \frac{79}{159} = 0.4968553\)</span>!!</p>
<p>There are, however, many situations where our common sense fails us. For example, what would we do if we had a regression-model situation as in Module 2 and would like to specify an alternative form for <span class="math inline">\(\theta\)</span>, such as</p>
<p><span class="math display">\[\theta = \alpha + \beta \times (\text{lobster species}).\]</span></p>
<p>How would we estimate the unknown intercept <span class="math inline">\(\alpha\)</span> and slope(s) <span class="math inline">\(\beta\)</span>, assuming we had information on lobster species etc. For this we need a general framework for estimation that can be applied to any situation. The most useful and general method of obtaining parameter estimates is the method of <strong>maximum likelihood estimation</strong>. I mean the title of this section was a bit of a giveaway!</p>
<div id="the-likelihood" class="section level4 hasAnchor">
<h4>The Likelihood<a href="maximum-likelihood-estimation.html#the-likelihood" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The likelihood function, <span class="math inline">\(L(\theta ; x)\)</span> is a function of the parameter(s) <span class="math inline">\(\theta\)</span> for fixed data <span class="math inline">\(x\)</span> and it gives the probability of a fixed observation <span class="math inline">\(x\)</span> for every possible value of the parameter(s) <span class="math inline">\(\theta\)</span>, <span class="math inline">\(P(X = x)\)</span>.</p>
<p>From above, letting <span class="math inline">\(S\)</span> be the number of lobsters alive after the 24 hours we can assume a Binomial distribution:</p>
<p><span class="math display">\[L(\theta ; s) = P(S = s) = {n \choose s} \theta^s (1 - \theta)^{n-s}.\]</span></p>
<p>To obtain the maximum likelihood estimate (i.e., the <em>best guess</em> for <span class="math inline">\(\theta\)</span> given the observed data) we need to differentiate the likelihood. That is, find it’s rate of change given data <span class="math inline">\(x\)</span>, <span class="math inline">\(\frac{\delta L}{\delta \theta}\)</span> . We are interested in the <em>best guess</em> for <span class="math inline">\(\theta\)</span>; this occurs at the point when the rate of change of our likelihood is zero (i.e., <span class="math inline">\(\frac{\delta L}{\delta \theta} = 0\)</span>)</p>
<p>Using the product rule we differentiate <span class="math inline">\(L(\theta ; s)\)</span>:</p>
<p><span class="math display">\[ \begin{array}{cl}
\frac{\delta L}{\delta \theta} &amp;= {n \choose s}\left( s\theta^{s-1} (1 - \theta)^{n-s} + \theta^s (n-s) (1 - \theta)^{n-s-1}(-1)  \right)\\
&amp;=  (1 - \theta)^{n-s-1}\theta^{s-1}\{s(1-\theta) - (n-s)\theta\} \\
&amp;=  (1 - \theta)^{n-s-1}\theta^{s-1}\{s - s\theta - n\theta + s\theta \}\\
&amp;= (1 - \theta)^{n-s-1}\theta^{s-1}\{s - n\theta \}.
\end{array}\]</span></p>
<p>Setting <span class="math inline">\(\frac{\delta L}{\delta \theta} = 0\)</span> and solve for <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[\frac{\delta L}{\delta \theta} = (1 - \theta)^{n-s-1}\theta^{s-1}\{s - n\theta \} = 0.\]</span>
There are, technically, three possible solutions to this:</p>
<ol style="list-style-type: decimal">
<li>when <span class="math inline">\(\theta^{s-1} = 0 \rightarrow \theta = 0\)</span>,</li>
<li>when <span class="math inline">\(s - n\theta = 0 \rightarrow\theta = \frac{s}{n}\)</span>, or</li>
<li>when <span class="math inline">\((1 - \theta)^{n-s-1} \rightarrow\theta = 1\)</span>.</li>
</ol>
<p>Now, remember <span class="math inline">\(\theta\)</span> is a probability and we are after a maximum likelihood estimate based on the data, so based on the above our <em>best guess</em> for <span class="math inline">\(\theta\)</span> is <span class="math display">\[\hat{\theta} = \frac{s}{n}.\]</span> Using this for the lobster data we get <span class="math inline">\(\hat{\theta} = \frac{79}{159}.\)</span> Surprise surprise!!</p>
<p><strong>Using <code>R</code></strong></p>
<div class="sourceCode" id="cb284"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb284-1"><a href="maximum-likelihood-estimation.html#cb284-1"></a><span class="co">## define the likelihood as a function of the parameter(s)</span></span>
<span id="cb284-2"><a href="maximum-likelihood-estimation.html#cb284-2"></a><span class="co">## Luckily the Binomial likelihood is already defined in R</span></span>
<span id="cb284-3"><a href="maximum-likelihood-estimation.html#cb284-3"></a><span class="co">## as dbinom()</span></span>
<span id="cb284-4"><a href="maximum-likelihood-estimation.html#cb284-4"></a>likelihood &lt;-<span class="st"> </span><span class="cf">function</span>(theta) <span class="kw">dbinom</span>(<span class="dt">x =</span> <span class="dv">79</span>, <span class="dt">size =</span> <span class="dv">159</span>, <span class="dt">prob =</span> theta)</span>
<span id="cb284-5"><a href="maximum-likelihood-estimation.html#cb284-5"></a><span class="co">## Use the optimise function to optimise!!</span></span>
<span id="cb284-6"><a href="maximum-likelihood-estimation.html#cb284-6"></a><span class="co">## the second argument specifies the plausible</span></span>
<span id="cb284-7"><a href="maximum-likelihood-estimation.html#cb284-7"></a><span class="co">## interval for the parameter</span></span>
<span id="cb284-8"><a href="maximum-likelihood-estimation.html#cb284-8"></a><span class="co">## Note for a number of parameters &gt; 1</span></span>
<span id="cb284-9"><a href="maximum-likelihood-estimation.html#cb284-9"></a><span class="co">## the optim() function is used</span></span>
<span id="cb284-10"><a href="maximum-likelihood-estimation.html#cb284-10"></a><span class="kw">optimise</span>(likelihood, <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">maximum =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<pre><code>## $maximum
## [1] 0.4968541
## 
## $objective
## [1] 0.06317819</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-169-1.png" width="672" /></p>
</div>
</div>
<div id="maximum-likelihood-estimation-for-a-crd" class="section level3 hasAnchor">
<h3>Maximum likelihood estimation for a CRD<a href="maximum-likelihood-estimation.html#maximum-likelihood-estimation-for-a-crd" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall the following CRD equation</p>
<p><span class="math display">\[Y_{ik} = \mu_k + \epsilon_{ik}\]</span></p>
<p>where <span class="math inline">\(Y_{ik}\)</span> is the response for the <span class="math inline">\(k^{th}\)</span> experimental unit (<span class="math inline">\(k = 1, ..., r_i\)</span>, where <span class="math inline">\(r_i\)</span> is the number of experimental replications in the <span class="math inline">\(i^{th}\)</span> level of the treatment factor) subjected to the <span class="math inline">\(i^{th}\)</span> level of the treatment factor (<span class="math inline">\(i = 1, ..., t\)</span>,). Here <span class="math inline">\(\mu_i\)</span> are the different (cell) means for each level of the treatment factor.</p>
<p>Under the assumptions of a the CRD (i.e., <span class="math inline">\(\epsilon_{ik} \sim N(0, \sigma^2)\)</span>) then (for equal number of replicates) the estimates of the cell means (<span class="math inline">\(\mu_k\)</span>) are found by minimising the error of the sum of squares <span class="math display">\[SS_{\epsilon} = \Sigma_{i=1}^t \Sigma_{k=1}^{r_i}(y_{ik}-\mu_i)^2.\]</span> Taking the partial derivatives of <span class="math inline">\(SS_{\epsilon}\)</span> with respect to each cell mean, setting to zero, and solving each equation with give us our estimates: <span class="math display">\[\frac{\delta SS_{\epsilon}}{\delta \mu_i} = -2 \Sigma_{i=1}^t \Sigma_{k=1}^{r_i}(y_{ik}-\mu_i) = 0.\]</span> This works out as <span class="math inline">\(\hat{\mu_i} = \overline{y_i.}\)</span></p>
<p>So in our mask example <span class="math inline">\(\hat{\mu}_{\text{Type 1 }} = 5.7 ,\; \hat{\mu}_{\text{Type 2 }} = 7.125 \; , \&amp;\; \hat{\mu}_{\text{Type 3 }} = 8.8 .\)</span> Compare these estimates to those we obtained via least squares estimation in the previously.</p>
</div>
<div id="maximising-the-log-likelihood" class="section level3 hasAnchor">
<h3>Maximising the log-likelihood<a href="maximum-likelihood-estimation.html#maximising-the-log-likelihood" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In many situations differentiating the likelihood is tricky. Even the simple binomial example above was a bit finicky. So. we often deal with the log-likelihood (the log of the likelihood). Why?</p>
<ol style="list-style-type: decimal">
<li>The logarithmic function <span class="math inline">\(L \mapsto \text{log}(L)\)</span> is increasing, so the functions <span class="math inline">\(L(\theta)\)</span> and <span class="math inline">\(\text{log}(L(\theta))\)</span> will have the same maximum, <span class="math inline">\(\hat{\theta}\)</span>.</li>
<li>When there are observations <span class="math inline">\(x_1, \ldots, x_n\)</span>, the likelihood <span class="math inline">\(L\)</span> is a product as <span class="math inline">\(\text{log}(a b) = \text{log}(a) + \text{log}(b)\)</span>, the log-likelihood converts the product into a sum. It is often easier to differentiate a sum than a product, so the log-likelihood is easier to maximize while still giving the same MLE.</li>
<li>If we need to use a computer to calculate and maximize the likelihood, there will often be numerical problems with computing the likelihood product, whereas the log-likelihood sum can be accurately calculated.</li>
</ol>
<p>Let’s consider the binomial example above. We had <span class="math inline">\(L(\theta ; s) = {n \choose s} \theta^s (1 - \theta)^{n-s}.\)</span></p>
<p>Therefore, <span class="math display">\[\begin{array}{cl}
\text{log}(L(\theta ; s)) &amp;=  \text{log}({n \choose s}) + \text{log}(\theta^s) + \text{log}((1 - \theta)^{n-s})\\
 &amp;=  \text{log}({n \choose s}) + s\text{log}(\theta) + (n-s)\text{log}(1 - \theta).
\end{array}\]</span></p>
<p>Differentiating this: <span class="math display">\[\begin{array}{cl}
\frac{\delta \text{log}(L(\theta ; s))}{\delta \theta} &amp;=  0 + \frac{s}{\theta} \times 1 + \frac{n-s}{1-\theta}\times (-1)\\
 &amp;= \frac{s}{\theta} - \frac{n-s}{1-\theta} \\
 \end{array}\]</span></p>
<p>Setting this to zero we get <span class="math inline">\(\frac{s}{\theta} = \frac{n-s}{1-\theta} \rightarrow s(1-\theta) = \theta(n-s) \rightarrow s - s\theta = \theta n - s\theta \rightarrow s + (s\theta - s\theta) = \theta n \rightarrow \theta = \frac{s}{n}.\)</span> Therefore, as above <span class="math display">\[\hat{\theta} = \frac{s}{n}.\]</span></p>
<p><img src="_main_files/figure-html/unnamed-chunk-171-1.png" width="672" /></p>
</div>
<div id="maximum-likelihood-estimation-for-a-poisson-distribution" class="section level3 hasAnchor">
<h3>Maximum likelihood estimation for a Poisson distribution<a href="maximum-likelihood-estimation.html#maximum-likelihood-estimation-for-a-poisson-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Poisson process counts the number of events occurring in a fixed time or space, when events occur independently and at a constant average rate, <span class="math inline">\(\lambda\)</span>.</p>
<p>For <span class="math inline">\(X \sim \text{Poisson}(\lambda)\)</span>,</p>
<p><span class="math display">\[f_X(x) = P(X=x)=\frac{\lambda^x}{x!}e^{-\lambda}\]</span></p>
<p>for <span class="math inline">\(x = 0,1,2,\dots\)</span></p>
<div id="maximising-the-likelihood" class="section level4 hasAnchor">
<h4>Maximising the Likelihood<a href="maximum-likelihood-estimation.html#maximising-the-likelihood" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Suppose that <span class="math inline">\(x_1, \ldots, x_n\)</span> are iid observations from a Poisson distribution with unknown parameter <span class="math inline">\(\lambda\)</span>:</p>
<p><span class="math display">\[ L(\lambda\,; x_1, \ldots, x_n) = K e^{-n\lambda} \, \lambda^{n \overline{x}} \,,\]</span></p>
<p>where <span class="math inline">\(\overline{x} = \frac{1}{n} \sum_{i=1}^n x_i\)</span>, and <span class="math inline">\(K=\prod_{i=1}^n \frac1{x_i\,!}\)</span> is a constant that doesn’t depend on <span class="math inline">\(\lambda\)</span>.</p>
<p>We differentiate <span class="math inline">\(L(\lambda\,; x_1, \ldots, x_n)\)</span> and set to 0 to find the MLE:</p>
<p><span class="math display">\[\begin{array}{rl}
0 &amp;= \frac{\delta}{\delta\lambda} L(\lambda\,; x_1, \ldots, x_n) \\
&amp;= K \left( -n e^{-n\lambda} \, \lambda^{n \overline{x}}   +  n\overline{x}   e^{-n\lambda} \, \lambda^{n \overline{x} - 1} \right)\\
&amp;=  K e^{-n\lambda} \lambda^{n \overline{x} - 1} \left(- n\lambda + n\overline{x} \right)
\end{array}\]</span></p>
<p><span class="math inline">\(\rightarrow \lambda=\infty, \lambda=0, \text{or } \lambda = \overline{x}.\)</span></p>
<p>If we know that <span class="math inline">\(L(\lambda\,; x_1, \ldots, x_n)\)</span> reaches a unique maximum in <span class="math inline">\(0 &lt; \lambda &lt; \infty\)</span> then the MLE is <span class="math inline">\(\overline{x}\)</span>.</p>
<p>So the maximum likelihood estimator is <span class="math display">\[ \hat{\lambda} = \overline{x}  = \frac{x_1 + \ldots + x_n}{n}
\;.\]</span></p>
</div>
<div id="maximising-the-log-likelihood-function" class="section level4 hasAnchor">
<h4>Maximising the log-likelihood function<a href="maximum-likelihood-estimation.html#maximising-the-log-likelihood-function" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>As above,</p>
<p><span class="math display">\[L(\lambda\,; x_1, \ldots, x_n)  = \prod_{i=1}^n \frac{\lambda^{x_i}}{x_i\,!} e^{-\lambda}.\]</span>
Therefore,</p>
<p><span class="math display">\[\begin{array}{rl}\text{log}(L(\lambda\,; x_1, \ldots, x_n)) &amp;= \sum_{i=1}^n \text{log}(\frac{\lambda^{x_i}}{x_i\,!} e^{-\lambda})\\
&amp;= \sum_{i=1}^n  \text{log}(\frac{1}{x_i\,!}) +  \text{log}(\lambda^{x_i}) + \text{log}(e^{-\lambda}) \\
&amp;= \sum_{i=1}^n \text{log}(\frac{1}{x_i\,!}) +  x_i \text{log}(\lambda) + (-\lambda) \\
&amp;= K&#39; + \text{log}(\lambda) \sum_{i=1}^n x_i - n \lambda   \quad \mbox{ where $K&#39;$ is a constant} \\
&amp;= K&#39; + \text{log}(\lambda) n\overline{x} - n \lambda. \end{array}\]</span></p>
<p>Differentiate and set to 0 for the MLE:</p>
<p><span class="math display">\[\begin{array}{rcl}
0 &amp;=&amp; \frac{\delta}{\delta\lambda} \text{log} (L(\lambda\,; x_1, \ldots, x_n) ) \\
 &amp;=&amp; \frac{\delta}{\delta\lambda} \left (K&#39; + \text{log}(\lambda) n\overline{x} - n \lambda  \right )\\
 &amp;=&amp; \frac{n\overline{x}}{\lambda} - n \\
\end{array}\]</span>
assuming a unique maximum in <span class="math inline">\(0 &lt; \lambda &lt; \infty\)</span> the MLE is <span class="math inline">\(\hat{\lambda} = \overline{x}\)</span> as before.</p>
</div>
</div>
<div id="maximum-likelihood-estimation-for-a-continuous-random-variable" class="section level3 hasAnchor">
<h3>Maximum likelihood estimation for a continuous random variable<a href="maximum-likelihood-estimation.html#maximum-likelihood-estimation-for-a-continuous-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let <span class="math inline">\(X\)</span> be a continuous random variable with probability density function
<span class="math display">\[f_X(x) = 
 \left\{ \begin{array}{cl}
\frac{2(s-x)}{s^2} &amp; \mbox{for } 0 &lt; x  &lt; s\,, \\
0 &amp; \mbox{otherwise}\,. \end{array} \right.\]</span></p>
<p>Here, <span class="math inline">\(s\)</span> is a parameter to be estimated, where <span class="math inline">\(s\)</span> is the maximum value of <span class="math inline">\(X\)</span> and <span class="math inline">\(s&gt;0\)</span>.</p>
<p>Assuming a single observation <span class="math inline">\(X=x\)</span> the likelihood function is</p>
<p><span class="math display">\[ L(s\,;x) = \frac{2(s-x)}{s^2}\]</span> for <span class="math inline">\(\;x &lt; s &lt; \infty.\)</span></p>
<p>Differentiating this</p>
<p><span class="math display">\[\begin{array}{rl}
 \frac{dL}{ds} &amp;= 2 \left(-2 s^{-3}(s-x) + s^{-2}\right ) \\
&amp;= 2s^{-3} (-2(s-x) + s) \\
&amp;= \frac{2}{s^3} (2x-s).
\end{array}\]</span></p>
<p>At the MLE, <span class="math display">\[ \frac{\delta L}{\delta s} = 0 \implies s=\infty \quad\mbox{ or }\quad s = 2x.\]</span></p>
<p>Realistically <span class="math inline">\(s=\infty\)</span> is not the maximum (see graph below) so <span class="math inline">\(s = 2x.\)</span> Therefore maximum likelihood <strong>estimator</strong> is <span class="math display">\[\hat{s} = 2X.\]</span></p>
<p><strong>Using <code>R</code> to get the MLE</strong></p>
<p>Let’s say we observe <span class="math inline">\(X = 3\)</span>, then to find the MLE using <code>R</code> we use</p>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb286-1"><a href="maximum-likelihood-estimation.html#cb286-1"></a><span class="co">## define the likelihood as a function of the parameter(s)</span></span>
<span id="cb286-2"><a href="maximum-likelihood-estimation.html#cb286-2"></a>likelihood &lt;-<span class="st"> </span><span class="cf">function</span>(s) (<span class="dv">2</span><span class="op">*</span>(s <span class="op">-</span><span class="st"> </span><span class="dv">3</span>))<span class="op">/</span>s<span class="op">^</span><span class="dv">2</span></span>
<span id="cb286-3"><a href="maximum-likelihood-estimation.html#cb286-3"></a><span class="co">## Use the optimise function to optimise</span></span>
<span id="cb286-4"><a href="maximum-likelihood-estimation.html#cb286-4"></a><span class="co">## the second argument specifies the plausible</span></span>
<span id="cb286-5"><a href="maximum-likelihood-estimation.html#cb286-5"></a><span class="co">## interval for the parameter</span></span>
<span id="cb286-6"><a href="maximum-likelihood-estimation.html#cb286-6"></a><span class="co">## Note for a number of parameters &gt; 1</span></span>
<span id="cb286-7"><a href="maximum-likelihood-estimation.html#cb286-7"></a><span class="co">## the optim() function is used</span></span>
<span id="cb286-8"><a href="maximum-likelihood-estimation.html#cb286-8"></a><span class="kw">optimise</span>(likelihood, <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">8</span>), <span class="dt">maximum =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<pre><code>## $maximum
## [1] 6.000018
## 
## $objective
## [1] 0.1666667</code></pre>
<p>How does this to compare to the exact estimator, <span class="math inline">\(\hat{s} = 2X\)</span>, we found using calculus above? Consider, too, the plot below showing <span class="math inline">\(L(s; X = 3)\)</span>.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-173-1.png" width="672" /></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="least-squares-estimation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="introduction-to-bayesian-statistics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
