# A completely randomomised design

## Learning Objectives

   + **Describe** a Completely Randomised (experimental) Design 
   + **Carry** out linear regression in `R` with one categorical explanatory variable (one-way ANOVA) and **draw** the appropriate inference
   + **Communicate** statistical concepts and experimental outcomes clearly using language appropriate for both a **scientific** and **non-scientific** audience

## Analysis of a Completely Randomised Design in `R`: `aov()` and `lm()`

```{r data, message = FALSE, eval = FALSE}
library(tidyverse)
rats <- read_csv("crd_rats_data.csv")
```

```{r means}
rats %>%
  group_by(Surgery) %>%
  summarise(avg = mean(logAUC))
```

### `aov()`

```{r aov}
rats_aov <- aov(logAUC ~ Surgery, data = rats)
summary(rats_aov)
```

**Inference**

```{r lm-quiet, echo = FALSE}
rats_lm <- lm(logAUC ~ Surgery, data = rats)
```

Hypothesis: We test the Null hypothesis, $H_0$, population (`Surgery`) means are the same on average verses the alternative hypothesis, $H_1$, that **at least one** differs from the others!

Probability of getting an **F-statistic** at least as extreme as the one we observe (think of the area under the tails of the curve below) **p-value** Pr(>F)= `r round(anova(rats_lm)$"Pr(>F)"[1],4)` tells us we have sufficient evidence to reject $H_0$ at the 1% level of significance


```{r f, echo = FALSE}
stats_df <- data.frame(Fs = summary(rats_aov)[[1]][[4]][[1]])
crit_df <- as.data.frame(apply(stats_df,2,rep,each = 100))
crit_df$Fs <- c(sapply(stats_df$Fs, function(x) seq(x,20, length.out = 100)))
crit_df$y <- df(crit_df$Fs,df1 = 2,df2 = 9) ## corresponding F val
ggplot(stats_df,aes(x = Fs)) +
  geom_vline(aes(xintercept = Fs)) +
    geom_line(data = data.frame(x = seq(0,20,.1),
                              y  = df(seq(0,20,.1),df1 = 2,df2 = 9)),
              aes(x = x,y = y), alpha = 0.7) +
    theme(legend.position = "none") +
    ylab("density") +
    xlab("F-value") +
    geom_area(data = crit_df,mapping = aes(x = Fs,y = y),fill = "blue") +
  geom_line(data = crit_df,mapping = aes(x = Fs,y = y),color = "blue") +
  geom_text(aes(4, 1,label = paste("F-statistic:", format(round(Fs, 2), nsmall = 2))),
              size = 4, hjust = 0, color = "black")  +
  geom_text(aes(4, 0.95,label = paste("p-value:", format(round(pf(Fs,2,9,lower.tail = FALSE),4), 
                                                         nsmall = 2))),
              size = 4, hjust = 0, color = "blue") 

```

### `lm()`

`lm()`

```{r lm}
rats_lm <- lm(logAUC ~ Surgery, data = rats)
```


**Inference**

```{r lmsum}
summary(rats_lm)$coef
```


```{r, echo = FALSE}
means$base <- summary(rats_lm)$coef[1,1]
ggplot(rats, aes(x = Surgery, y = logAUC)) + 
    geom_violin()  + 
  ylab("logAUC") +
  xlab("Treatment") +
  geom_point(data = means, aes(x = Surgery, y = avg, color = Surgery), size = 2) +
  geom_text(data = means, aes(x = Surgery, y = avg + 0.25, color = Surgery, label = paste0("Treatment mean = ",round(avg,3)))) +
  geom_hline(data = means, aes(yintercept = avg, color = Surgery), alpha = 0.3, lty = 2) +
  geom_segment(data = means[2:3,], aes(x = Surgery, y = avg, xend = Surgery, yend = base,color = Surgery), size = 1) +
  geom_text(data = means[2:3,], aes(x = Surgery, y = base - 0.25, color = Surgery, label = paste0("diff to baseline = ",round(avg - base,3)))) +
  geom_hline(data = means[1,], aes(yintercept = avg, color = Surgery)) +
  geom_text(data = means[1,],aes(x = Surgery, y = base - 0.25, color = Surgery, label = paste0("Baseline = ",round(avg,3))))
  
```

Which pairs of means are different?

 + Pair-wise comparisons of means
  	+ Use two-sample t-tests
 	+ We need to calculate our **observed**  t-value where
$\text{t-value} = \frac{\text{Sample Difference}_{ij} - \text{Difference assuming } H_0 \text{ is true}_{ij}}{\text{SE of } \text{Sample Difference}_{ij}}$
 where
   $\text{Sample Difference}_{ij}$ = Difference between pair of sample means
 + Compute the p-value for observed t-value


(Intercept) = $\text{mean}_C$ = `r summary(rats_lm)$coef[1,1]`

SE of (Intercept) = SE of $\text{mean}_C$ = SEM = `r summary(rats_lm)$coef[1,2]`

$\text{Surgery}_P$ = $\text{mean}_P$ – $\text{mean}_C$ = `r summary(rats_lm)$coef[2,1]`

SE of $\text{Surgery}_P$ = SE of ($\text{mean}_P$ - $\text{mean}_C$ ) = SED = `r summary(rats_lm)$coef[2,2]`


**Hypotheses being tested**

+ The t value and Pr (>|t|) are the t - and p-value for testing the null hypotheses:
	+ Mean abundance is zero for C population
	+ No difference between the population means of P and C
	+ No difference between the population means of S and C

We're interested in 2 and 3, but not necessarily 1!

 Two-sample t -tests for pairwise comparisons of means
 
+ SurgeryP : t value = Estimate ÷ Std.Error =  0.8446; Pr (>|t|) =  0.4202

**F-test:**

```{r lmsum2}
anova(rats_lm)
```
	
**The same as `aov()`** in fact `aov()` is calling `lm()` in the background.

#### Diagnostic plots

Carrying out any linear regression we have some **key assumptions**

+ **Independence** 
+ There is a **linear relationship** between the response and the explanatory variables
+ The residuals have **constant variance**
+ The **residuals** are normally distributed

```{r,echo = FALSE}
options(warn=-1)
```

```{r qqnorm, warning=FALSE, message=FALSE}
gglm::gglm(rats_lm) # Plot the four main diagnostic plots
```

**Residuals vs Fitted** plot

You are basically looking for no pattern or structure in your residuals (e.g., a "starry" night). You definitely don't want to see is the scatter increasing around the zero line (dashed line) as the fitted values get bigger (e.g., think of a trumpet, a wedge of cheese, or even a slice of pizza) which would indicate unequal variances (heteroscedacity). 

**Normal quantile-quantile (QQ)** plot

This plot shows the sorted residuals versus expected order statistics from a standard normal distribution. Samples should be close to a line; points moving away from 45 degree line at the tails suggest the data are from a skewed distribution.


**Scale-Location** plot ($\sqrt{\text{|standardized residuals vs Fitted|}}$)

Another way to check the homoskedasticity (constant-variance) assumption. We want the  line to be roughly horizontal. If this is the case then the average magnitude of the standardized residuals isn't changing much as a function of the fitted values. We'd also like the spread around the  line not to vary much with the fitted values; then the variability of magnitudes doesn't vary much as a function of the fitted values.

**Residuals vs Leverage** plot (standardized residuals vs Leverage)

This can help detect outliers in a linear regression model. For linear regression model leverage measures how sensitive a fitted value is to a change in the true response. We're looking at how the spread of standardized residuals changes as the leverage. This can also be used to detect heteroskedasticity and non-linearity: the spread of standardized residuals shouldn't change as a function of leverage. In addition, points with high leverage may be influential: that is, deleting them would change the model a lot. 


