# Multiple comparisons and more designs

## Learning objectives

   + *Discuss* and **critique** methods for controlling errors in hypothesis testing, for example Fisher’s LSD and the Bonferroni Correction
   + **Detail** and **draw** inference form multiple comparison procedures such as Tukey’s HSD and Dunnett's test
   + **Describe** the family wise error rate (FWER) and false discover rate (FDR) in the context of multiple comparisons
   + **Describe** a Randomized Complete Block Design (RCBD)
   + **Carry** out analysis of a RCBD in `R` using `lm()`, `aov()`, and `lmer()` and **discuss** and **compare** the three
   + **Describe** and **discuss** factorial experiments with both equal and unequal replication
   + **Carry** out linear regression in `R` with two categorical explanatory variables and an interaction (two-way ANOVA with interaction) and **draw** the appropriate inference
   + **Calculate** the marginal means for a balanced and unbalanced design
   + **Communicate** statistical concepts and experimental outcomes clearly using language appropriate for both a **scientific** and **non-scientific** audience
   
## Adjustments for multiple testing

Recall that **each** time we carry out a hypothesis test the probability we get a false positive result (type I error) is given by $\alpha$ (the *level of significance* we choose).

When we have **multiple comparisons** to make we should then control the **Type I** error rate across the entire *family* of tests under consideration, i.e., control the Family-Wise Error Rate (FWER); this ensures that the risk of making at least one **Type I** error among the family of comparisons in the experiment is $\alpha$.


|State of Nature  | Don't reject $H_0$ | reject $H_0$ |
|---              |---                |---            |
| $H_0$ is true |  `r emo::ji("check")` | Type I error  |
| $H_0$ is false  | Type II error  | `r emo::ji("check")` |

The **familywise error rate (FWER)** is the risk of making at least one **Type I** error among the family of comparisons in the experiment. Now let's consider carrying out $m$ independent t-tests and let for any single test, let Pr(commit a Type 1 error) $= \alpha_c$ be the **per comparison error rate (PCER)**. So for a single test the probability a correct decision is made is $1 - \alpha_c$. Therefore for $m$ **independent** t-tests the probability of committing no Type I errors is $(1 - \alpha_c)^m$ and the probability of committing at least one Type I error is $1 -(1 - \alpha_c)^m = \alpha_F$ which is the upper limit of the FWER.

### Fisher’s, Least Significant Difference, LSD

Carry out post-hoc tests only if the ANOVA F-test is *significant*. If so declare significant $100\alpha\%$ any pairwise difference > LSD. This does **not** control the FWER.

### Bonferroni correction

### Multiple comparison procedures

#### Tukey’s Honest Significant Difference (HSD)

#### Dunnett’s test


##  The `predictmeans` package

```{r,echo = FALSE}
options(warn=-1)
```


```{r, message = FALSE, echo = FALSE}
library(tidyverse)
rats <- read_csv("../data/crd_rats_data.csv")
rats$Surgery <- as_factor(rats$Surgery)
```


```{r lm}
rats_lm <- lm(logAUC ~ Surgery, data = rats)
```

```{r predmeans, message=FALSE, warnings = FALSE}
# Load predictmeans (assumes already installed)
library(predictmeans)
pred_means <- predictmeans(rats_lm , modelterm = "Surgery",pairwise = TRUE, adj = "none", plot = FALSE)
pred_means$`Predicted Means`
```

```{r}
pred_means$`Standard Error of Means`
pred_means$LSD
pred_means$`Pairwise p-value`
```

```{r,echo = TRUE}
predictmeans(rats_lm , modelterm = "Surgery",pairwise = TRUE, adj = "none", plot = TRUE)
```