## Least Squares Estimation


### Some basic matrix algebra

This section is a recap only, if you need a more in-depth overiew of matrix algebra then use the extra materials provided at the start of this module.


Matrices are commonly written in box brackets or parentheses and are typically denoted by upper case bold letters (e.g., **A**) with elements represented by the corresponding lower case indexed letters:

$$\mathbf {A} = \begin{bmatrix}a_{11}&a_{12}&\cdots &a_{1n}\\a_{21}&a_{22}&\cdots &a_{2n}\\\vdots &\vdots &\ddots &\vdots \\a_{m1}&a_{m2}&\cdots &a_{mn}\end{bmatrix}$$

The entry in the $i^{th}$ row and $j^{th}$ column of a matrix **A** is often referred to as the $(i,j)^{th}$ entry of the matrix, and most commonly denoted as $a_{i,j}$, or $a_{ij}$. 

**Matrix addition**	

The sum **A** + **B** of two m-by-n matrices **A** and **B**:
 $$\mathbf {A} + \mathbf {B} = \begin{bmatrix}a_{11}&a_{12}&\cdots &a_{1n}\\a_{21}&a_{22}&\cdots &a_{2n}\\\vdots &\vdots &\ddots &\vdots \\a_{m1}&a_{m2}&\cdots &a_{mn}\end{bmatrix} +  \begin{bmatrix}b_{11}&b_{12}&\cdots &b_{1n}\\b_{21}&b_{22}&\cdots &b_{2n}\\\vdots &\vdots &\ddots &\vdots \\b_{m1}&b_{m2}&\cdots &b_{mn}\end{bmatrix} =  \begin{bmatrix}a_{11} +b_{11}&a_{12}+b_{12}&\cdots &a_{1n}+b_{1n}\\a_{21}+b_{21}&a_{22}+b_{22}&\cdots &a_{2n}+b_{2n}\\\vdots &\vdots &\ddots &\vdots \\a_{m1}+b_{m1}&a_{m2}+b_{m2}&\cdots &a_{mn}+b_{mn}\end{bmatrix}$$

**Scalar multiplication**

The product c**A** of a number c and a matrix **A**:

$$c\times \mathbf {A} = \begin{bmatrix}c\times a_{11}&c\times a_{12}&\cdots &c\times a_{1n}\\c\times a_{21}&c\times a_{22}&\cdots &c\times a_{2n}\\\vdots &\vdots &\ddots &\vdots \\c\times a_{m1}&c\times a_{m2}&\cdots &c\times a_{mn}\end{bmatrix}$$

**Matrix transposition**

The transpose of an m-by-n matrix **A** is the n-by-m matrix **A**^T:

$$\mathbf {A}^\text{T} = \begin{bmatrix}a_{11}&a_{12}&\cdots &a_{1n}\\a_{21}&a_{22}&\cdots &a_{2n}\\\vdots &\vdots &\ddots &\vdots \\a_{m1}&a_{m2}&\cdots &a_{mn}\end{bmatrix}^\text{T} = \begin{bmatrix}a_{11}&a_{21}&\cdots &a_{m1}\\a_{12}&a_{22}&\cdots &a_{m2}\\\vdots &\vdots &\ddots &\vdots \\a_{1n}&a_{2n}&\cdots &a_{mn}\end{bmatrix}$$

**Matrix multiplication**

Multiplication of two matrices is defined if and only if ($iif$) the number of columns of the left matrix is the same as the number of rows of the right matrix. If **A** is an m-by-n matrix and **B** is an n-by-p matrix, then their matrix product **AB** is the m-by-p matrix whose entries are given by dot product of the corresponding row of **A** and the corresponding column of **B**:

$$[\mathbf {AB} ]_{i,j}=a_{i,1}b_{1,j}+a_{i,2}b_{2,j}+\cdots +a_{i,n}b_{n,j}=\sum _{r=1}^{n}a_{i,r}b_{r,j}$$

**Note** Matrix multiplication satisfies the rules (AB)C = A(BC) (associativity), and (A + B)C = AC + BC as well as C(A + B) = CA + CB (left and right distributivity), whenever the size of the matrices is such that the various products are defined.[13] The product AB may be defined without BA being defined, namely if A and B are m-by-n and n-by-k matrices, respectively, and m ≠ k. Even if both products are defined, they generally need not be equal, that is: AB ≠ BA.


### Matrix representation of a CRD

Let's consider the CRD outlined in the previous module, we can write the effects model using matrix representation:

$$\boldsymbol{y} = \boldsymbol{X\beta} + \boldsymbol{\epsilon}$$

where 

$\boldsymbol{y} = \begin{bmatrix}
 y_{11} \\ y_{12} \\ y_{13} \\ y_{14} \\ y_{21} \\ y_{22} \\ y_{23} \\ y_{24} \\ y_{31} \\ y_{32} \\ y_{33} \\ y_{34} 
\end{bmatrix}$, $\boldsymbol{X} = \begin{bmatrix}
 1 & 1 & 0 & 0 \\ 1 & 1 & 0 & 0  \\ 1 & 1 & 0 & 0  \\ 1 & 1 & 0 & 0  \\ 1 & 0 & 1 & 0 \\ 1 & 0 & 1 & 0 \\ 1 & 0 & 1 & 0 \\ 1 & 0 & 1 & 0 \\  1 & 0 & 0 & 1 \\ 1 & 0 & 0 & 1 \\ 1 & 0 & 0 & 1 \\ 1 & 0 & 0 & 1 
\end{bmatrix}$, $\boldsymbol{\beta} = \begin{bmatrix}
 \alpha \\ \tau_1 \\ \tau_2 \\ \tau_3
\end{bmatrix}$, and $\boldsymbol{\epsilon} = \begin{bmatrix}
 \epsilon_{11} \\ \epsilon_{12} \\ \epsilon_{13} \\ \epsilon_{14} \\ \epsilon_{21} \\ \epsilon_{22} \\ \epsilon_{23} \\ \epsilon_{24} \\ \epsilon_{31} \\ \epsilon_{32} \\ \epsilon_{33} \\ \epsilon_{34} 
\end{bmatrix}.$

where $\boldsymbol{\epsilon} \sim \text{MVN}(\boldsymbol{0}, \boldsymbol{\sigma^2 I})$. The least squares estimators of $\boldsymbol{\beta}$ are the solutions to the $$\boldsymbol{X^{'}X\beta}=\boldsymbol{X^{'}y}$$. 

Recall that for a factor variable we take the one level (the first factor) into the baseline (i.e., the standard) and hence then the coefficients we estimate are compared to it (i.e., the differences in the mean). This is to ensure that the matrix $\boldsymbol{X}$ is full rank. So 

$$\boldsymbol{X} = \begin{bmatrix}
 1  & 0 & 0 \\ 1  & 0 & 0  \\ 1  & 0 & 0  \\ 1  & 0 & 0  \\ 1  & 1 & 0 \\ 1 & 1 & 0 \\ 1  & 1 & 0 \\ 1  & 1 & 0 \\  1  & 0 & 1 \\ 1  & 0 & 1 \\ 1  & 0 & 1 \\ 1  & 0 & 1 
\end{bmatrix}$$

Now, with three factor levels the least squares estimators of $\boldsymbol{\beta}$ are (note the hat to denote the estimates)

$$(\boldsymbol{X^{'}X})^{-1}\boldsymbol{X^{'}y} = \boldsymbol{\hat{\beta}}= \begin{bmatrix}
 \hat{\alpha} - \hat{\tau_1} \\ \hat{\tau_2} - \hat{\tau_1} \\ \hat{\tau_3} - \hat{\tau_1}
\end{bmatrix} $$

### A numeric example

#### By hand

Using the mask data from the previous chapter we have

```{r, results='asis',echo = FALSE}
set.seed(5469)
type <- factor(rep(paste("Type", 1:3), each = 4))
df <- data.frame(Mask = type, Droplets = round(c(rnorm(4, 5), rnorm(4, 7), rnorm(4, 9)), 1))
kableExtra::kable(df)
```

```{r, eval = TRUE, echo = FALSE}
write_mat <- function(x) {
  begin <- "\\begin{bmatrix}"
  end <- "\\end{bmatrix}"
  X <-
    apply(x, 1, function(x) {
      paste(
        paste(x, collapse = "&"),
        "\\\\"
      )
    })
  paste(c(begin, X, end), collapse = "")
}
x <- matrix(c(rep(1, 12), rep(c(0, 1, 0), each = 4), rep(0, 8), rep(1, 4)), ncol = 3)
y <- df$Droplets
```

so 

$\boldsymbol{X^{'}X} = `r write_mat(t(x)%*%x)`$, $\boldsymbol{X^{'}y} = `r write_mat(t(x)%*%y)`$, and $(\boldsymbol{X^{'}X})^{-1} = `r write_mat(solve(t(x)%*%x))`.$

Therefore, 

$$\boldsymbol{\hat{\beta}} = (\boldsymbol{X^{'}X})^{-1}\boldsymbol{X^{'}y} = \begin{bmatrix}
 \hat{\alpha} - \hat{\tau_1} \\ \hat{\tau_2} - \hat{\tau_1} \\ \hat{\tau_3} - \hat{\tau_1}
\end{bmatrix} = `r write_mat(solve(t(x)%*%x))` \times `r write_mat(t(x)%*%y)` = `r write_mat((solve(t(x)%*%x)) %*% (t(x)%*%y))`$$

#### Using `lm` in `R`

```{r}
mod <- lm(Droplets ~ Mask, data = df)
summary(mod)
```
