## Linear mixed-effect models (LMMs)

**Recall,** blocking helps control variability by making treatment groups more alike. Experimental units are divided into subsets (called blocks) so that units within the same block are more similar than units from different subsets or blocks. Blocking is a technique for dealing with *nuisance factors*. A nuisance factor is a factor that has some effect on the response, but is of no interest (e.g., age class).

**Fixed effects** are terms (parameters) in a statistical model which are fixed, or non-random, quantities (e.g., *treatment* group's mean response). For the same *treatment*, we expect this quantity to be the same from experiment to experiment.

**Random effects** are terms (parameters) in a statistical model which are considered as random quantities or variables (e.g., block id). Specifically, terms whose levels are a representative sample from a population, and where the variance of the population is of interest should be allocated as random. Setting a block as a random effect allows us to infer variation between blocks as well as the variation between experimental units within blocks.

**Key idea:** Partition known sources of variation which are unimportant to key scientific question(s) to improve precision of comparisons between treatment means.


### A Randomised Controlled Block Design (RCBD)

The `rcbd` data:

```{r, eval = FALSE}
glimpse(rcbd)
```

To use `predictmeans` later on we have to ensure that the relevant variables are coded as factors:

```{r}
rcbd <- rcbd %>%
  mutate(Run = as.factor(Run)) %>%
  mutate(Surgery = as.factor(Surgery))
```

#### `Run` as a fixed effect

```{r}
lm <- lm(logAUC8 ~ Run + Surgery, data = rcbd)
summary(lm)
```


#### `Run` as a random effect

<div class="alert alert-info">

![](https://github.com/BIOSCI738/cowstats/blob/main/img/code_tag.png?raw=true){width=10%}

**Note** Both the `lmerTest` and `lme4` packages export a function called `lmer`, which fits linear mixed effect models. For all estimation purposes etc. they essentially do the same think. However, the structure of the output differs and this MATTERS when we later come to use other funtions on the outputs. Therefore, I take care below to specify which version of the `lmer` function I use by explicitly calling it from either `lmerTest` or `lme4` using the `::` syntax.

</div>

There are, confusingly, two ways of fitting the same model. For inference we require both!

Option 1 uses the `lmer` function from the `lme4` package:

```{r}
lmer4_mod <- lme4::lmer(logAUC8 ~ Surgery + (1|Run), data = rcbd)
summary(lmer4_mod)
```

Option 2 uses the `lmer` function from the `lmerTest` package:

```{r}
lmerTest_mod <- lmerTest::lmer(logAUC8 ~ Surgery + (1|Run), data = rcbd)
summary(lmerTest_mod)
```


As you can see they give the same output! Why bother, you might ask?! This will become apparent later on.

**Inference** about the random effects

We have two variance components
  
  + Between Groups (Runs) $\hat{\sigma^2}_{\text{Run}}$ = 1.479
  + Within Runs (between observations) $\hat{\sigma_2}$ = 1.447
  
Note that `aov()` presents the same information, but in a different way:

```{r}
summary(aov(logAUC8 ~ Surgery + Error(Run), data = rcbd))
```

 + Within Runs (Residuals) $\hat{\sigma}_2$ = 1.447 (*same as `lmer`*)
 + Between Run variance = $\hat{\sigma}^2$ +  $3\:\hat{\sigma}^2_{\text{Run}}$ so $\hat{\sigma}^2_{\text{Run}} = \frac{5.883 - \hat{\sigma}^2 }{3} = \frac{5.883 - 1.447}{3} = 1.479$
 
**Inference** about the fixed effects

Specifying `Run` as random effect changes our estimated baseline (i.e., `Intercept` coefficient) as now and effect due to `Run` is attributed to the structural component of the model.

We can interpret the fixed effects of a LMM as we might for a linear model (now the `Intercept` estimate changes depending on `Run`).

```{r}
coefficients(lmer4_mod)
coefficients(lmerTest_mod)
```

What about an ANOVA table? **NOTE** that specifying the type of $SS$ (e.g., Type I, II, or III) in an `anova` call **only** works with a model fitted using `lmerTest::lmer`:

```{r}
anova(lmerTest_mod, type = 1)
anova(lmerTest_mod, type = 2)
```

Now, as we only have a single fixed effect in our model (`Surgery`) the ANOVA Type I and Type II tables above are equivalent! 

**Inference using `predictmeans()`** (Note: output will be the same for the `lmerTest` model.)

```{r}
pm <- predictmeans(lmer4_mod, modelterm = "Surgery", pairwise = TRUE, plot = FALSE)
```

Using the elements of the `predictmeans` object (as in the last section) we can extract the pairwise comparison information:

```{r, message = FALSE}
pm$`Pairwise LSDs`
print(pm$`Pairwise p-value`)
```

Gives us 1) pairwise differences (upper diagonal) and the LSD values (lower diagonal), and 2) the pairwise comparison statistic (the $t$-statistic in this case) on the upper diagonal and the associated p-value's on the lower diagonal.

So is there a pairwise difference in means? Let's organise the information in a table. Below we read a specifically designed function to do this from the given URL.

```{r}
comparisons(pm)
```

Have a look at the CIs and p-values. What do you conclude?

We could also plot the pairwise comparisons using `emmeans`

```{r}
em <- emmeans(lmer4_mod, specs =  "Surgery")
plot(em, pairwise = TRUE) + theme_bw()
```


### A Split-plot design


```{r}
glimpse(split_plot)
```

To use `predictmeans` later on we have to ensure that the relevant variables are coded as factors:

```{r}
split_plot <- split_plot %>%
  mutate(Animal = as.factor(Animal)) %>%
  mutate(Disease = as.factor(Disease))%>%
  mutate(Organ = as.factor(Organ))
```

#### `Animal` as a random effect

**Using `aov()`**

```{r}
sp_aov <- aov(logAUC ~ Disease*Organ + Error(Animal), data = split_plot)
summary(sp_aov)
```

**Using `lmer()` (from `lmeTest` and `lmer4`)**

Recall that specifying the type of $SS$ (e.g., Type I, II, or III) in an `anova` call **only** works with a model fitted using `lmerTest::lmer`. As we now have specified an interaction model then the type of $SS$ calculated will have an effect on inference for unbalanced designs.

```{r}
sp_lmer <- lmerTest::lmer(logAUC ~ Disease*Organ + (1|Animal), 
                          data = split_plot) 
anova(sp_lmer,type = 1)
anova(sp_lmer,type = 2)
```

Note here, though, our design is **balanced** hence the ordering of terms in our model does not make a difference (no need to specify Type II $SS$)


**Pairwise comparison of time means**

When a design has blocking, to get summary stats using `predictmeans` you should fit the model using `lme4::lmer()`:

Calling `predictmeans` alone produces plots:

```{r, results='hide'}
lmer <- lme4::lmer(logAUC ~ Disease*Organ + (1|Animal), 
                          data = split_plot)
predmeans <- predictmeans::predictmeans(model = lmer ,modelterm = "Disease:Organ", 
                                        pairwise = TRUE)
```

```{r}
predmeans$predictmeansPlot
```

Recall from previous sections that the LSD vale is essentially the *buffer* around the point estimate (the radius of the CI if you like), beyond the limit of which we might believe there to be a *significant* difference (a bit lax with terminology there!). 

**How** would be interpret the above plot? I would conclude that there is a big difference between Diabetic and Healthy for InnerLV wall, but no difference for the outerLV wall. We can construct the pairwise comparison table:

```{r}
comparisons(predmeans)
```

What do the CI coverages indicate?

We can also use `emmeans` to plot the pairwise comparisons

```{r}
em <- emmeans(lmer, ~Disease*Organ)
plot(em, pairwise = TRUE) + theme_bw()
```

From all the output above, what do you conclude about the interaction effect, if any! 

### A repeated measures design 


```{r, message = FALSE}
liver <- liver %>%
  mutate(Time = as.factor(Time)) %>%
  mutate(Treatment = as.factor(Treatment)) 
glimpse(liver)
```

Below these data are plotted.

```{r, echo = FALSE}
ggplot(liver, aes(x = Time, y = Glucose, color = Treatment, group = Animal)) +
  geom_line(size = 1) + 
  geom_point(shape = 1) +
  theme_linedraw() +
  ggtitle("Repeated measurments across time")
liver_means <- liver %>% group_by(Time,Treatment) %>%
  summarise(Glucose = mean(Glucose))
ggplot(liver_means, aes(x = Time, y = Glucose, color = Treatment, group = Treatment)) +
  geom_line(size = 1) + 
  geom_point(shape = 1) +
  theme_linedraw() +
  ggtitle("Repeated measurments across time, group means")
```


#### `Animal` as a random effect 

**Using `aov()`**

```{r, message = FALSE}
re_aov <- aov(Glucose ~ Treatment*Time + Error(Animal),data = liver)
summary(re_aov)
```

**Using `lmer()` (from `lmerTest` and `lme4`)**


Recall that specifying the type of $SS$ (e.g., Type I, II, or III) in an `anova` call **only** works with a model fitted using `lmerTest::lmer`. As we now have specified an interaction model then the type of $SS$ calculated will have an effect on inference for unbalanced designs.


```{r}
re_lmer <- lmerTest::lmer(Glucose ~ Treatment*Time + (1|Animal), data = liver)
anova(re_lmer,type = 2)
```

**Pairwise comparison of time means**

As above, when a design has blocking, to get summary stats using `predictmeans` you should fit the model using `lme4::lmer()`:


```{r,}
re_lmer4 <- lme4::lmer(Glucose ~ Treatment*Time + (1|Animal),data = liver) 
predmeans <- predictmeans(model = re_lmer4 ,modelterm = "Time",
                                        pairwise = TRUE, plot = TRUE)
```

<div class="alert alert-warning">
  <strong>TASK</strong> What inference do you draw? You may also find the outputs from the code below useful.
  

```{r, eval = FALSE}
comparisons(predmeans)
em <- emmeans(re_lmer4, ~Treatment*Time)
plot(em, pairwise = TRUE) + theme_bw() + facet_wrap(~Treatment)
```

</div>

