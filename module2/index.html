<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>BIOSCI738</title>
    <meta charset="utf-8" />
    <meta name="author" content="Charlotte M. Jones-Todd" />
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <link href="libs/countdown/countdown.css" rel="stylesheet" />
    <script src="libs/countdown/countdown.js"></script>
    <script src="libs/kePrint/kePrint.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# BIOSCI738
## <img src="https://cpb-ap-se2.wpmucdn.com/blogs.auckland.ac.nz/dist/d/79/files/2015/10/uoa-v-reverse1.png" style="width:5.0%" /> Universty of Auckland
### Charlotte M. Jones-Todd
### <a href="https://stats-uoa.github.io/BIOSCI738/">bluepages</a>
### Module 2: Experimental Design and Statistical Inference

---

class: inverse, center, middle



&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 20px;
    padding: 1em 4em 1em 4em;
}
&lt;/style&gt;



# Experimental Design


---
class: inverse
## Learning Objectives


   + **Identify** the following
      + experimental unit
      + observational units
   + **List** and **describe** the three main principals of experimental design
       + Randomization
       + Replication
       + Blocking
   + **Discuss** the advantages and disadvantages of different designs
   + **Critique** experimental designs
   + **Communicate** statistical concepts and experimental outcomes clearly using language appropriate for both a **scientific** and **non-scientific** audience
   
---

# The language of design

.pull-left[![](img/cox.png)
[Gertrude Mary Cox](https://en.wikipedia.org/wiki/Gertrude_Mary_Cox)]

.pull-right[![](img/cochran.png)

[William Gemmell Cochran](https://en.wikipedia.org/wiki/William_Gemmell_Cochran)]

[Cochran, William G.; Cox, Gertrude M. (1950). Experimental Designs. New York: Wiley.](https://www.wiley.com/en-us/Experimental+Designs%2C+2nd+Edition-p-9780471545675)

---

## The language of design

#### **Scientific objective:** The biological question (**AVOID BEING VAGUE**)
  + Should include details such as what is being *compared*, what is being *measured* and *how*, and under what *experimental conditions* etc.

--
### Compare three surgical conditionings of a biofluid...

--

### **Too vague!**

--

 + Which biofluid and collected from what?
 + How will they be compared?
    + What will be measured?
    + How will it be measured?
    + Under which experimental conditions?
    + By whom?
    
???

+ Organism? Male Wistar Rat
+ Cell, tissue, biofluid? Lymph
+ What? Global proteomic profile
+ Proteins + Abundances
+ How? 2Dâ€“LCâ€“MS/MS

---
##  Key phrases


 **Experimental unit** Smallest portion of experimental material which is *independently* perturbed
 
--

**Treatment** The experimental condition *independently* applied to an experimental unit

--

**Observational unit** The smallest unit on which a response is measured. If one measurement is made on each rat: **Observational unit** = **Experimental unit**. If Multiple measurements are made on each rat: Each experimental unit has &gt;1 observational unit (*pseudo-* or *technical replication*).

---
class: center, middle

## Experimental or Observational unit?

<div class="countdown" id="timer_602b5054" style="top:0;right:0;font-size:48px;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">05</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

### [https://b.socrative.com/login/student/](https://b.socrative.com/login/student/)
### Room Name: BIOSCIUOA


---
class: center, middle

# Three key principles:


## **Replication**
## **Randomization**
## **Blocking**


---
### Fisher... 


.pull-left[![](https://upload.wikimedia.org/wikipedia/commons/a/aa/Youngronaldfisher2.JPG)

*a genius who almost single-handedly created the foundations for modern statistical science*]

--

.pull-right[
### You should also know

[![](img/fisher.png)](https://www.newstatesman.com/international/science-tech/2020/07/ra-fisher-and-science-hatred)

[RA Fisher and the science of hatred](https://statmodeling.stat.columbia.edu/2020/08/01/ra-fisher-and-the-science-of-hatred/)

[\#Topple The Racists](https://www.bbc.com/news/uk-england-cambridgeshire-53023823)]


---

class: inverse

# 1st major principle: Replication

![](https://media.makeameme.org/created/precision-ypqmw8.jpg)

---

class: inverse

# 1st major principle: Replication

+ **Biological replication:** each treatment is *independently* applied to each of several humans, animals or plants
  + To generalize results to population

--

+ **Technical replication:** two or more samples from the same biological source which are *independently* processed
  + Advantageous if processing steps introduce a lot of variation
  + Increases the precision with which comparisons of relative abundances between treatments are made
  
--

+ **Pseudo-replication:** one sample from the same biological source, divided into two or more aliquots which are **independently** measured
  + Advantageous for noisy measuring instruments
  + Increases the **precision** with which comparisons of relative abundances between treatments are made
  
---

class: inverse

# 2nd major principle: Randomisation

![](https://media.makeameme.org/created/your-bias-is.jpg)

---

class: inverse

# 2nd major principle: Randomisation

+ **Protects against bias**


+ Plan the experiment in such a way that the variations caused by extraneous factors can all be combined under the general heading of "chance".

+ Ensures that each treatment has the same probability of getting good (or bad) units and thus
avoids systematic bias
+ random allocation can cancel out population bias; it ensures that any other possible causes for the experimental results are split equally between groups
+ typically statistical analysis assumes that observations are **independent**. This is almost never strictly true in practice but randomisation means that our estimates will behave as if they were based on independent observations

---

class: inverse

# 3rd major principle: Blocking

![](https://www.memecreator.org/static/images/memes/4524459.jpg)
---

class: inverse

# 3rd major principle: Blocking


Blocking helps **control variability** by making treatment groups more alike. Experimental units are divided into subsets (called blocks) so that units within the same block are more similar than units from different subsets or blocks. 

Blocking is a technique for dealing with *nuisance factors*.

A *nuisance factor* is a factor that has some effect on the response, but is of no interest (e.g., age class).



---

# Other resources


[Glass, David J. Experimental Design for Biologists. Second ed. 2014. Print.](https://catalogue.library.auckland.ac.nz/primo-explore/fulldisplay?docid=uoa_alma21237737730002091&amp;search_scope=Combined_Local&amp;tab=books&amp;vid=NEWUI&amp;context=L)

[Welham, S. J. Statistical Methods in Biology : Design and Analysis of Experiments and Regression. 2015. Print.](https://catalogue.library.auckland.ac.nz/primo-explore/fulldisplay?docid=uoa_alma21237737830002091&amp;search_scope=Combined_Local&amp;tab=books&amp;vid=NEWUI&amp;context=L)

[Fisher, Ronald Aylmer. The Design of Experiments. 8th ed. Edinburgh: Oliver &amp; Boyd, 1966. Print. O &amp; B Paperbacks.](https://catalogue.library.auckland.ac.nz/primo-explore/fulldisplay?docid=uoa_alma21198532990002091&amp;context=L&amp;vid=NEWUI&amp;lang=en_US&amp;search_scope=Combined_Local&amp;adaptor=Local%20Search%20Engine&amp;isFrbr=true&amp;tab=books&amp;query=any,contains,The_Design_of_Experiments&amp;sortby=date&amp;facet=frbrgroupid,include,627497507&amp;offset=0)


---
class: inverse, center, middle

# One-Way **An**alysis **o**f **Va**riance (ANOVA)
## Sums of Squares (SS)
## Mean Sums of suares (MSS)
## Degrees of freedom

---
class: inverse
## Learning Objectives

+ **Calculate** Sums of Squares (between and within groups) given the observations 
+ **Define** and **state** the appropriate degrees of freedom in a one-way ANOVA scenario
+ **Calculate** the F-statistics given the appropriate Sums of Squares and degrees of freedom
+ **Interpret** and **discuss** a given *p-value* in the context of a stated hypothesis test

---

## The data



&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Surgery &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Rat &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; logAUC &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; C &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.49 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; C &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.20 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; C &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.08 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; C &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.07 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; P &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 10.24 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; P &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7.72 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; P &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.34 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; P &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.50 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; S &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 11.31 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; S &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 12.69 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; S &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 11.37 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; S &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 10.82 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

**The idea**: Assess **distances** between treatment (*surgical condition*) means relative to our uncertainty about the actual (*true*) treatment means.

![](/home/charlotte/Git/BIOSCI738/module2/index_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;

---
 **The idea**: Assess **distances** between treatment (*surgical condition*) means relative to our uncertainty about the actual (*true*) treatment means.

![](/home/charlotte/Git/BIOSCI738/module2/index_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;

--

**add up the differences:** -1.192 + -0.703 + 1.895 = 0. **This is always the case!**

---
**So adding up the differences:** -1.192 + -0.703 + 1.895 = 0. **Not a great way to measure distances!**

--

**Sums of Squares?** 

--

`\(-1.192^2 + -0.703^2 + 1.895^2\)`

**add up the squared differences?** but... there are 4 observations in each group (treatment)

--

`\(4\times(-1.192)^2 + 4\times(-0.703)^2 + 4\times(1.895)^2\)`

This is the **Between Groups Sums of Squares** or the **Between group SS (SSB)** 

--

So the Between group SS (SSB) = 22.02635

---

**Adding up the differences:** -1.192 + -0.703 + 1.895 = 0. **This is always the case** and that itself gives us information...

--

**We only need to know two of the values to work out the third!**

So we have only 2 bits of **unique** information; **SSB degrees of freedom** = 2



---
Now:  Each observation (*logAUC*) can be *decomposed* in the same way

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Surgery &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Rat &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; logAUC &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; ov_avg &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; tr_avg &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; tr_avg_minus_ov_avg &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; obvs_minus_tr_avg &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; C &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.49 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6525 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.4600 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.1925 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0300 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; C &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.20 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6525 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.4600 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.1925 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.2600 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; C &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.08 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6525 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.4600 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.1925 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.6200 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; C &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.07 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6525 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.4600 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.1925 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.3900 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; P &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 10.24 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6525 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.9500 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.7025 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.2900 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; P &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7.72 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6525 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.9500 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.7025 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.2300 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; P &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.34 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6525 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.9500 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.7025 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.3900 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; P &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.50 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6525 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.9500 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.7025 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.4500 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; S &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 11.31 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6525 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 11.5475 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.8950 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.2375 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; S &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 12.69 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6525 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 11.5475 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.8950 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.1425 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; S &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 11.37 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6525 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 11.5475 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.8950 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.1775 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; S &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 10.82 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6525 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 11.5475 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.8950 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.7275 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


---

`logAUC` = `ov_avg` + `tr_avg_minus_ov_avg` + `obvs_minus_tr_avg` `\(\rightarrow\)` **observation** = **overall average** + **treatment average minus overall average** + **observation minus treatment average**

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; logAUC &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; equals &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; ov_avg &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; add &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; tr_avg_minus_ov_avg &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; plus &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; obvs_minus_tr_avg &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 8.49 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; = &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6525 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.1925 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0300 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 8.20 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; = &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6525 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.1925 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.2600 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 9.08 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; = &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6525 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.1925 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.6200 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 8.07 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; = &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6525 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.1925 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.3900 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 10.24 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; = &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6525 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.7025 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.2900 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 7.72 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; = &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6525 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.7025 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.2300 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 9.34 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; = &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6525 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.7025 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.3900 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 8.50 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; = &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6525 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.7025 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.4500 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 11.31 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; = &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6525 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.8950 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.2375 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 12.69 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; = &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6525 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.8950 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.1425 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 11.37 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; = &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6525 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.8950 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.1775 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 10.82 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; = &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.6525 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.8950 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; + &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.7275 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---
Remember the **Between group SS (SSB)** *variation due to treatments*

--

The **Within group SS (SSW)** arises from the same idea:

To assess distances between treatment (surgical condition) means **relative** to our uncertainty about the actual (true) treatment means.

--

Procedure:

 + Observation - Treatment mean
 + Square the difference
 + Add them up!
 
---
**Within group SS (SSW)** *unexplained variance*

![](/home/charlotte/Git/BIOSCI738/module2/index_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;

---

Recall the Between group SS (**SSB**) = 22.02635

So mean **SSB** =  22.02635 / 2

--

The within group SS (**SSW**) = 6.059075

Here we have `\(2\times 3\)` bits of *unique* information: within groups **degrees of freedom** is 9.

So mean **SSW** = 6.059/9

--

Consider the ratio `\({\frac  {{\text{variation due to treatments}}}{{\text{unexplained variance}}}} = {\frac  {{\text{ mean between-group variability}}}{{\text{mean within-group variability}}}}\)`  `\(=\frac{\text{mean SSB}}{\text{mean SSW}}\)` `\(=\frac{\text{MSB}}{\text{MSW}}\)`  = `\(=\frac{\text{experimental variance}}{\text{error variance}}\)` 16.3586975

--

This is the **F-statistic**... more to come!


---
class: inverse, center, middle

# Analysis of a Completely Randomised Design in `R`
## `aov()` and `lm()`

---
class: inverse
## Learning Objectives

+ **Explain** between group and within group variation
+ **Describe** a Completely Randomised (experimental) Design 
+ **Carry** out linear regression in `R` with one categorical explanatory variable (one-way ANOVA) and **draw** the appropriate inference
+ **Communicate** statistical concepts and experimental outcomes clearly using language appropriate for both a **scientific** and **non-scientific** audience


---

## One-way ANOVA



```r
library(tidyverse)
rats &lt;- read_csv("crd_rats_data.csv")
```


```r
rats %&gt;%
  group_by(Surgery) %&gt;%
  summarise(avg = mean(logAUC))
```

```
## # A tibble: 3 x 2
##   Surgery   avg
## * &lt;fct&gt;   &lt;dbl&gt;
## 1 C        8.46
## 2 P        8.95
## 3 S       11.5
```



---

## One-way ANOVA `aov()`



```r
rats_aov &lt;- aov(logAUC ~ Surgery, data = rats)
rats_aov
```

```
## Call:
##    aov(formula = logAUC ~ Surgery, data = rats)
## 
## Terms:
##                   Surgery Residuals
## Sum of Squares  22.026350  6.059075
## Deg. of Freedom         2         9
## 
## Residual standard error: 0.8205063
## Estimated effects may be unbalanced
```

.footnote[recognize the **Sums of Squares** values?]

---



## Inference

Hypothesis: We test the Null hypothesis, `\(H_0\)`, population (`Surgery`) means are the same on average verses the alternative hypothesis, `\(H_1\)`, that **at least one** differs from the others!

Probability of getting an **F-statistic** at least as extreme as the one we observe (think of the area under the tails of the curve below) **p-value** Pr(&gt;F)= 0.001 tells us we have sufficient evidence to reject `\(H_0\)` at the 1% level of significance


---

## Inference


![](/home/charlotte/Git/BIOSCI738/module2/index_files/figure-html/f-1.png)&lt;!-- --&gt;
---

# ðŸ˜± p-values ðŸ˜±

### [The ASA Statement on p-Values: Context, Process, and Purpose](https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108)

**Q:** Why do so many colleges and grad schools teach *p-val*=0.05?


**A:** Because that's still what the scientific community and journal editors use.


**Q:** Why do so many people still use *p-val*=0.05?


**A:** Because that's what they were taught in college or grad school. 

---

# ðŸ˜± p-values ðŸ˜±

**What is a p-Value?**

Informally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value


---
# ðŸ˜± p-values ðŸ˜±

**p-values** can indicate how incompatible the data are with a specified statistical model

--

p-values **do not** measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone

--

Scientific conclusions and business or policy decisions **should not** be based only on whether a p-value passes a specific threshold

--

Proper inference requires **full** reporting and transparency

--

A p-value, or statistical significance, does **not** measure the size of an effect or the importance of a result

--

By itself, a p-value does **not** provide a good measure of evidence regarding a model or hypothesis

---
# ðŸ˜± p-values ðŸ˜±

&gt; "Good statistical practice, as an essential component of good scientific practice, 
&gt; emphasizes principles of good study design and conduct, a variety of numerical 
&gt; and graphical summaries of data, understanding of the phenomenon under study, 
&gt; interpretation of results in context, complete reporting and proper
&gt; logical and quantitative understanding of what data summaries mean. 
&gt; No single index should substitute for scientific reasoning." &lt;footer&gt;--- ASA Statement on p-Values&lt;/footer&gt;


---
class: inverse

The **ANOVA** enables us to:

 + Test only  the global null hypothesis of *no difference between any of the treatment groups' means*
	
 + Estimate the population variance. (We'll come back to why this is important)

--

But...


A primary goal of our experiment

  + *Biologically speaking:* To identify which pairs  of treatments are biologically different from one another
  
  + *Statistically speaking:* To identify which pairs of treatment means  are statistically  different from one another 


---

## Everything is a regression...

![](https://pbs.twimg.com/media/EeMWb7QWsAIGftR?format=jpg&amp;name=small)

---

## Looking forward

| Traditional name    | Model formula  | R code  |
| ------------------- |:--------------:| -------:|
| Simple regression   | `\(Y \sim X_{continuous}\)` | `lm(Y ~ X)` |
| One-way ANOVA       | `\(Y \sim X_{categorical}\)`      |   `lm(Y ~ X)` |
| Two-way ANOVA       | `\(Y \sim X1_{categorical} + X2_{categorical}\)`| `lm(Y ~ X1 + X2)` |
| ANCOVA              | `\(Y \sim X1_{continuous} + X2_{categorical}\)` |`lm(Y ~ X1 + X2)` |
| Multiple regression | `\(Y \sim X1_{continuous} + X2_{continuous}\)` | `lm(Y ~ X1 + X2)` |
| Factorial ANOVA     | `\(Y \sim X1_{categorical} * X2_{categorical}\)`|   `lm(Y ~ X1 * X2)` or `lm(Y ~ X1 + X2 + X1:X2)` |


---

## One-way ANOVA `lm()`


```r
rats_lm &lt;- lm(logAUC ~ Surgery, data = rats)
anova(rats_lm)
```

```
## Analysis of Variance Table
## 
## Response: logAUC
##           Df  Sum Sq Mean Sq F value   Pr(&gt;F)   
## Surgery    2 22.0263 11.0132  16.359 0.001006 **
## Residuals  9  6.0591  0.6732                    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

.footnote[recognize the **Sums of Squares** values?]

---


## Inference `lm()`


```r
summary(rats_lm)$coef
```

```
##             Estimate Std. Error    t value     Pr(&gt;|t|)
## (Intercept)   8.4600  0.4102531 20.6214144 6.930903e-09
## SurgeryP      0.4900  0.5801856  0.8445574 4.202408e-01
## SurgeryS      3.0875  0.5801856  5.3215734 4.799872e-04
```

.footnote[Taking **treatment** `SurgeryC` as the **baseline**...]
---
class: center, middle


![](/home/charlotte/Git/BIOSCI738/module2/index_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;
---
## Inference `lm()`

Which pairs of means are different?
 + Pair-wise comparisons of means
  	+ Use two-sample t-tests
 	+ We need to calculate our **observed**  t-value where
`\(\text{t-value} = \frac{\text{Sample Difference}_{ij} - \text{Difference assuming } H_0 \text{ is true}_{ij}}{\text{SE of } \text{Sample Difference}_{ij}}\)`
 where
   `\(\text{Sample Difference}_{ij}\)` = Difference between pair of sample means
 + Compute the p-value for observed t-value


---

## Inference `lm()`


```r
summary(rats_lm)$coef
```

```
##             Estimate Std. Error    t value     Pr(&gt;|t|)
## (Intercept)   8.4600  0.4102531 20.6214144 6.930903e-09
## SurgeryP      0.4900  0.5801856  0.8445574 4.202408e-01
## SurgeryS      3.0875  0.5801856  5.3215734 4.799872e-04
```


(Intercept) = `\(\text{mean}_C\)` = 8.46

SE of (Intercept) = SE of `\(\text{mean}_C\)` = SEM = 8.46

`\(\text{Surgery}_P\)` = `\(\text{mean}_P\)` â€“ `\(\text{mean}_C\)` = 0.49

SE of `\(\text{Surgery}_P\)` = SE of ($\text{mean}_P$ - `\(\text{mean}_C\)` ) = SED = 0.5801856

---

## Hypotheses being tested

+ The t value and Pr (&gt;|t|) are the t - and p-value for testing the null hypotheses:
	+ Mean abundance is zero for C population
	+ No difference between the population means of P and C
	+ No difference between the population means of S and C

Weâ€™re interested in 2 and 3, but not necessarily 1!

 Two-sample t -tests for pairwise comparisons of means
 
+ SurgeryP : t value = Estimate Ã· Std.Error =  0.8446; Pr (&gt;|t|) =  0.4202
	
.footnote[Cannot get t - or p -value of mean P â€“ mean S directly from table]

---
### Estimation of `\(\sigma^2\)`

```
## Analysis of Variance Table
## 
## Response: logAUC
##           Df  Sum Sq Mean Sq F value   Pr(&gt;F)   
## Surgery    2 22.0263 11.0132  16.359 0.001006 **
## Residuals  9  6.0591  0.6732                    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```
Some notation:

+ `\(\sigma^2\)` denotes the **population** variance
+ `\(s^2\)` denotes the **sample** variance, i.e. is estimated from dataset
+ A hat (^) above a maths symbol means an *estimated value*
+ So, what is the value of `\(\sigma^2\)`?

<div class="countdown" id="timer_602b4e54" style="top:0;right:0;font-size:48px;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">00</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">30</span></code>
</div>
	
--
0.6732
 
---
#### Standard Error of the Mean, SEM, 
#### Error of the Difference between two means, SED

What is the difference?

+ From Analysis of Variance Table:
	+ **SEM** = `\(\sqrt{\frac{\hat{\sigma^2}}{n}} = \sqrt{\frac{0.673}{4}} = 0.4103\)` ($n$ is the group replication)
	+ **SED** = `\(\sqrt{\frac{2\times\hat{\sigma^2}}{n}} = \sqrt{\frac{2 \times0.673}{4}} = 0.5801\)` (only true for equi-replicated groups)

&lt;table class="table" style="font-size: 18px; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Group &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Mean &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Standard Errors &lt;/th&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; SEM &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; SED &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; C &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 8.460 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.4103 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.5801 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; P &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 8.950 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.4103 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.5801 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; S &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 11.547 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.4103 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.5801 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---
class: inverse, middle

## Thereâ€™s an easier way!!

### Use package `predictmeans`
		

---
###  Analysis of a CRD Using the `predictmeans` package




```r
# Load predictmeans (assumes already installed)
library(predictmeans)
pred_means &lt;- predictmeans(rats_lm , modelterm = "Surgery",pairwise = TRUE, adj = "none", plot = FALSE)
pred_means$`Predicted Means`
```

```
## Surgery
##       C       P       S 
##  8.4600  8.9500 11.5475
```

---
###  Analysis of a CRD Using the `predictmeans` package

**compare** 


```r
rats_lm
```

```
## 
## Call:
## lm(formula = logAUC ~ Surgery, data = rats)
## 
## Coefficients:
## (Intercept)     SurgeryP     SurgeryS  
##       8.460        0.490        3.087
```


`\(\text{Surgery_P}\)` = P mean â€“ C mean 		0.490 = 8.9500 â€“ 8.4600
`\(\text{Surgery_S}\)` = S mean â€“ C mean 		3.087 = 11.5475 â€“ 8.4600

What about the difference between `\(\text{Surgery_S}\)` and `\(\text{Surgery_P}\)`?

S mean â€“ P mean       = 11.5475 â€“ 8.9500  =   2.5975
`\(\text{Surgery_S}\)` â€“ `\(\text{Surgery_P}\)` =   3.087 â€“ 0.490    =   2.597


---
###  Analysis of a CRD Using the `predictmeans` package


```r
pred_means$`Standard Error of Means`
```

```
## All means have the same Stder 
##                       0.41025
```

```r
pred_means$LSD
```

```
##  Max.LSD  Min.LSD Aveg.LSD 
##  1.31247  1.31247  1.31247 
## attr(,"Significant level")
## [1] 0.05
## attr(,"Degree of freedom")
## [1] 9
```

.footnote[All **SED**s the same, - each level of Treatment factor, Surgery,  has the same replication]

---
###  Analysis of a CRD Using the `predictmeans` package


```r
pred_means$`Pairwise LSDs`
```

```
##         C        P       S
## C 0.00000 -0.49000 -3.0875
## P 1.31247  0.00000 -2.5975
## S 1.31247  1.31247  0.0000
## attr(,"Significant level")
## [1] 0.05
## attr(,"Degree of freedom")
## [1] 9
## attr(,"Note")
## [1] "LSDs matrix has mean differences (row-col) above the diagonal, LSDs (adjusted by 'none' method) below the diagonal"
```

.footnote[LSDs matrix has **mean** differences (row-col) **above** the diagonal, LSDs below the diagonal]

---
###  Analysis of a CRD Using the `predictmeans` package


```r
pred_means$`Pairwise p-value`
```

```
##        C       P       S
## C 0.0000 -0.8446 -5.3216
## P 0.4202  0.0000 -4.4770
## S 0.0005  0.0015  0.0000
## attr(,"Degree of freedom")
## [1] 9
## attr(,"Note")
## [1] "The matrix has t-value above the diagonal, p-value (adjusted by 'none' method) below the diagonal"
## attr(,"Letter-based representation of pairwise comparisons at significant level '0.05'")
##   Treatment    Mean Group
## 1         S 11.5475    A 
## 2         P  8.9500     B
## 3         C  8.4600     B
```

.footnote[The matrix has t-value above the diagonal, p-value (adjusted by â€˜noneâ€™ method) below the diagonal]


---

### How would you summarise this info?


<div class="countdown" id="timer_602b4f75" style="top:0;right:0;font-size:48px;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">01</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

--

.panelset[.panel[.panel-name[Table?]
&lt;table class="table" style="font-size: 18px; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Group &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Mean &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; SE &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; t &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; p &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; C &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 8.460 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.225 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -0.845 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.4202 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; P &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 8.950 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.542 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -5.322 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.0005 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; S &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 11.547 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.400 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -4.477 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.0015 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
]
.panel[.panel-name[Please no...]
![](/home/charlotte/Git/BIOSCI738/module2/index_files/figure-html/dynamite-1.png)&lt;!-- --&gt;

]
.panel[.panel-name[Mean +/- 2SEs]
![](/home/charlotte/Git/BIOSCI738/module2/index_files/figure-html/d2 SEs-1.png)&lt;!-- --&gt;
]
.panel[.panel-name[Mean `\(\pm\)` 2SEs]

![](/home/charlotte/Git/BIOSCI738/module2/index_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;
]
]
---

## Why error bars that are `\(\pm\)` twice the SE?

This is approx. the 95% confidence interval for the population mean

The exact 95% CI is given by
  + Mean `\(\pm\)` `\(t_{df,1 - \alpha/2}\)` `\(\times\)` SEM
 ( df = degrees of freedom; `\(\alpha\)` = level of significance)

Each mean has its own confidence interval whose width depends on the SEM for that mean

When the df are large (e.g. 30 or greater) and `\(\alpha\)` = 0.05 `\(t_{df,1 - \alpha/2}\)` = `\(t_{large,0.975}\)` `\(\approx\)` 2

Hence, the 95% confidence interval for the population mean is approximately Mean Â± 2 `\(\pm\)` SEM
	
---

## 95% CI for the Control group mean

`\(t_{df,1 - \alpha/2}\)` = `\(t_{9,1 - 0.05/2}\)` = 2.262

(In `R` `\(t_{9,1 - 0.05/2}\)` = `qt (0.975, 9)` )
LSD = `\(t_{9,1 - 0.05/2} \times\)` SED = 2.262  `\(\times\)`  0.41025

95% CI: 8.460 `\(\pm\)` 0.928 = ( 7.532, 9.388)


&lt;table class="table" style="font-size: 18px; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Group &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Mean &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; SE &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; t &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; p &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; C &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 8.460 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.225 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -0.845 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.4202 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; P &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 8.950 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.542 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -5.322 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.0005 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; S &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 11.547 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.400 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -4.477 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.0015 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

### The Least significant difference, LSD


+ The LSD = `\(t_{df,1 - \alpha/2}\)` `\(\times\)`  SED
	
+ The SED is the **S**tandard **E**rror of the **D**ifference (of a pair of means)

+ SED = `\(\sqrt{SEM_1^2 + SEM_2^2}\)`
		
+ When the two means have the same replication SED = `\(\sqrt{2} \times\)`  SEM

+  When a pair of means differ by **more** than  the LSD, we say:

*Their difference is statistically significant at the 100 `\(\alpha\)` % level*

---

### Which means are significantly different?


```r
predictmeans(rats_lm , modelterm = "Surgery",pairwise = TRUE, adj = "none", plot = TRUE)
```

```
## $`Predicted Means`
## Surgery
##       C       P       S 
##  8.4600  8.9500 11.5475 
## 
## $`Standard Error of Means`
## All means have the same Stder 
##                       0.41025 
## 
## $`Standard Error of Differences`
##   Max.SED   Min.SED  Aveg.SED 
## 0.5801856 0.5801856 0.5801856 
## 
## $LSD
##  Max.LSD  Min.LSD Aveg.LSD 
##  1.31247  1.31247  1.31247 
## attr(,"Significant level")
## [1] 0.05
## attr(,"Degree of freedom")
## [1] 9
## 
## $`Pairwise LSDs`
##         C        P       S
## C 0.00000 -0.49000 -3.0875
## P 1.31247  0.00000 -2.5975
## S 1.31247  1.31247  0.0000
## attr(,"Significant level")
## [1] 0.05
## attr(,"Degree of freedom")
## [1] 9
## attr(,"Note")
## [1] "LSDs matrix has mean differences (row-col) above the diagonal, LSDs (adjusted by 'none' method) below the diagonal"
## 
## $`Pairwise p-value`
##        C       P       S
## C 0.0000 -0.8446 -5.3216
## P 0.4202  0.0000 -4.4770
## S 0.0005  0.0015  0.0000
## attr(,"Degree of freedom")
## [1] 9
## attr(,"Note")
## [1] "The matrix has t-value above the diagonal, p-value (adjusted by 'none' method) below the diagonal"
## attr(,"Letter-based representation of pairwise comparisons at significant level '0.05'")
##   Treatment    Mean Group
## 1         S 11.5475    A 
## 2         P  8.9500     B
## 3         C  8.4600     B
## 
## $mean_table
##   Surgery Predicted means Standard error Df LL of 95% CI UL of 95% CI
## 1       C          8.4600      0.4102531  9     7.531943     9.388057
## 2       P          8.9500      0.4102531  9     8.021943     9.878057
## 3       S         11.5475      0.4102531  9    10.619443    12.475557
```
---
class: inverse, middle, center

## Model diagnostics and data transformations

---
#  Key assumptions of Analysis of Variance 

Observations are **independent**
 
+ Check experiment description
+ How were data collected?
	
All observations have the **same variance**

+ Spread of the observations does not depend on the Treatment Mean
	
All observations are (approximately) normally distributed.

---

#### Raw AUC data



```
## `stat_bindot()` using `bins = 30`. Pick better value with `binwidth`.
```

![](/home/charlotte/Git/BIOSCI738/module2/index_files/figure-html/raw-1.png)&lt;!-- --&gt;

--

.footnote[&lt;span style="color:red"&gt;Shows variability in AUC increases with mean AUC&lt;/span&gt;]
---

#### Log AUC data



```
## `stat_bindot()` using `bins = 30`. Pick better value with `binwidth`.
```

![](/home/charlotte/Git/BIOSCI738/module2/index_files/figure-html/raw2-1.png)&lt;!-- --&gt;

--

.footnote[&lt;span style="color:red"&gt;Shows variability in AUC not related with mean `logAUC`&lt;/span&gt;]

---

## Residual plot to evaluate homogeneity of variances assumption

Plot residuals against the estimated values
of the treatment means (Predicted Values)

--

If the variability of the observations around
the treatment means differs between
groups, this will be reflected in the residual
plot

---

## Residual plot to evaluate homogeneity of variances assumption


```r
crd.lm &lt;- lm(AUC ~ Surgery, data = rats)
anova(crd.lm)
```

```
## Analysis of Variance Table
## 
## Response: AUC
##           Df      Sum Sq     Mean Sq F value  Pr(&gt;F)  
## Surgery    2 43315374897 21657687448       4 0.05716 .
## Residuals  9 48729375923  5414375103                  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```



---

## Residual plot to evaluate homogeneity of variances assumption



```r
rats$Residuals &lt;- residuals(crd.lm)
# Extract predicted (fitted) values and add to the data frame
rats$Fitted.Values &lt;- fitted.values(crd.lm)
rats
```

```
## # A tibble: 12 x 6
##    Surgery   Rat logAUC     AUC Residuals Fitted.Values
##    &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;
##  1 C           1   8.49   4866.     -255.         5120.
##  2 C           2   8.2    3641.    -1480.         5120.
##  3 C           3   9.08   8778.     3657.         5120.
##  4 C           4   8.07   3197.    -1923.         5120.
##  5 P           1  10.2   28001.    16363.        11638.
##  6 P           2   7.72   2253.    -9385.        11638.
##  7 P           3   9.34  11384.     -254.        11638.
##  8 P           4   8.5    4915.    -6724.        11638.
##  9 S           1  11.3   81634.   -54069.       135703.
## 10 S           2  12.7  324487.   188783.       135703.
## 11 S           3  11.4   86682.   -49022.       135703.
## 12 S           4  10.8   50011.   -85692.       135703.
```

---

### Is the homogeneity of variance assumption satisfied?

<div class="countdown" id="timer_602b4eb8" style="right:0;bottom:0;font-size:48px;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">02</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

.panelset[
 .panel[.panel-name[Plot]
 
![](/home/charlotte/Git/BIOSCI738/module2/index_files/figure-html/unnamed-chunk-17-1.png)&lt;!-- --&gt;
]
.panel[.panel-name[Code]

```r
ggplot(rats, aes(x = Fitted.Values, y = Residuals)) +
  theme_bw() + 
  geom_point(size = 4, shape = 18) +
  geom_hline( yintercept = 0, col = 2, size = 2) +
  geom_abline( intercept = 0, slope = +1.5, col = 4, linetype = 2 ,size = 2) +
 geom_abline( intercept = 0, slope = -1.5, col = 4, linetype = 2 ,size = 2)
```
]
]

--

Residuals increase in variability with predicted mean values so our equal variance assumption is **violated**

---

#### Other diagnostic plots


```r
gglm::gglm(crd.lm) # Plot the four main diagnostic plots
```

![](/home/charlotte/Git/BIOSCI738/module2/index_files/figure-html/qqnorm-1.png)&lt;!-- --&gt;

---

## QQplot

+ Normal quantile-quantile (QQ) plot

  â€¢ Plot sorted residuals versus expected order statistics from a standard normal distribution

+ Samples should be close to a line

+ Points moving away from 45 degree line at the tails suggest the data are from a skewed distribution, but it is difficult to be confident with so few data points


---
##  Outliers 
### Causes

+ Errors in collecting and/or recording of data
+ Mistakes in technique
+ Treatment and/or environment
+ Affect statistical inference
+ Inflation of estimated experimental error variance
+ Influence estimate of treatment mean
+ Investigate cause before discarding data
+ Discarded data results in loss of information

---
###  Looking for outliers with residuals 

+ Large positive or negative values far removed from the 1-to-1 line in the normal QQ plot
	+ Points far removed from upper and lower boundaries of the Residuals versus Predicted values plot

---
###  Variance stabilizing transformations 

+ Used to change the scale of the observations
	+ To conform more closely with the ANOVA assumptions
	+ To provide more valid inferences from ANOVA
	+ Significance levels ($\alpha$) don't apply to original data
	+ Conduct analysis and make all inferences on transformed
	+ Present summary tables on the original measurement scale

---

### logAUC


```r
rats_lm &lt;- lm(logAUC ~ Surgery, data = rats)
anova(rats_lm)
```

```
## Analysis of Variance Table
## 
## Response: logAUC
##           Df  Sum Sq Mean Sq F value   Pr(&gt;F)   
## Surgery    2 22.0263 11.0132  16.359 0.001006 **
## Residuals  9  6.0591  0.6732                    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

---


.panelset[.panel[.panel-name[Before transform]

```r
gglm::gglm(crd.lm)
```

![](/home/charlotte/Git/BIOSCI738/module2/index_files/figure-html/unnamed-chunk-19-1.png)&lt;!-- --&gt;
]
.panel[.panel-name[After transform]

```r
gglm::gglm(rats_lm)
```

![](/home/charlotte/Git/BIOSCI738/module2/index_files/figure-html/unnamed-chunk-20-1.png)&lt;!-- --&gt;
]
]

---

.panelset[.panel[.panel-name[Before differences]

```r
pm.lm &lt;- predictmeans(crd.lm, modelterm="Surgery", pairwise = TRUE, plot = FALSE)
pwLSDs.pmlm &lt;- pm.lm$"Pairwise LSDs"
diffs &lt;- pwLSDs.pmlm[upper.tri(pwLSDs.pmlm)]
LSDs &lt;- pwLSDs.pmlm[lower.tri(pwLSDs.pmlm)]
diffNames &lt;- c("C-P", "C-S", "P-S")
pValues.pmlm &lt;- pm.lm$`Pairwise p-value`
before &lt;- data.frame(diff = diffNames, Difference = diffs,
           SED = pm.lm$"Standard Error of Differences",
           LSD = LSDs, lowerLimit = diffs-LSDs, upperLimit=diffs+LSDs,
           "p-value" = pValues.pmlm[lower.tri(pValues.pmlm)])
knitr::kable(before)
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; diff &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Difference &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; SED &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; LSD &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; lowerLimit &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; upperLimit &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Max.SED &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; C-P &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -6517.845 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 52030.64 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 117701.5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -124219.3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 111183.642 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.9031 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Min.SED &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; C-S &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -130582.934 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 52030.64 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 117701.5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -248284.4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -12881.448 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0333 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Aveg.SED &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; P-S &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -124065.089 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 52030.64 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 117701.5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -241766.6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -6363.603 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0409 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

]
.panel[.panel-name[After differences]

```r
pm.lm &lt;- predictmeans(rats_lm, modelterm="Surgery", pairwise = TRUE, plot = FALSE)
pwLSDs.pmlm &lt;- pm.lm$"Pairwise LSDs"
diffs &lt;- pwLSDs.pmlm[upper.tri(pwLSDs.pmlm)]
LSDs &lt;- pwLSDs.pmlm[lower.tri(pwLSDs.pmlm)]
diffNames &lt;- c("C-P", "C-S", "P-S")
pValues.pmlm &lt;- pm.lm$`Pairwise p-value`
after &lt;- data.frame(diff = diffNames, Difference = diffs,
           SED = pm.lm$"Standard Error of Differences",
           LSD = LSDs, lowerLimit = diffs-LSDs, upperLimit=diffs+LSDs,
           "p-value" = pValues.pmlm[lower.tri(pValues.pmlm)])
knitr::kable(after)
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; diff &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Difference &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; SED &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; LSD &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; lowerLimit &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; upperLimit &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Max.SED &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; C-P &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.4900 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.5801856 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.31247 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.80247 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.82247 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.4202 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Min.SED &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; C-S &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -3.0875 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.5801856 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.31247 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -4.39997 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.77503 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0005 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Aveg.SED &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; P-S &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -2.5975 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.5801856 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.31247 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -3.90997 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.28503 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0015 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
]]

---

## Interpreting results

+ Data on the original scale
  `\(y_{c1}, y_{c2}, y_{c3}, y_{c4},...,y_{s4}\)`
  
+ On the log-scale
  `\(\text{log}(y_{c1}), \text{log}(y_{c2}), \text{log}(y_{c3}), \text{log}(y_{c4}),...,\text{log}(y_{s4})\)`
  
+ Mean of C group
  `\(\frac{\text{log}(y_{c1}) + \text{log}(y_{c2}) + \text{log}(y_{c3}) + \text{log}(y_{c4})}{4}\)` = `\(\frac{1}{4}(\text{log}(y_{c1}) + \text{log}(y_{c2}) + \text{log}(y_{c3}) + \text{log}(y_{c4}))\)`
  

---

## Interpreting results
### Useful log rules

+ `\(\text{log}(A) + \text{log}(B) = \text{log}(AB)\)`
+ `\(\text{log}(A) - \text{log}(B) = \text{log}(\frac{A}{B})\)`
+ `\(n\text{log}(A) = \text{log}(A^n)\)`

---

## Interpreting results

Mean of C group `\(\bar{x_c}\)`

`\(\bar{x_c} = \frac{1}{4}(\text{log}(y_{c1}) + \text{log}(y_{c2}) + \text{log}(y_{c3}) + \text{log}(y_{c4})) = \text{log}(y_{c1} \times y_{c2} \times y_{c3} \times y_{c4})^\frac{1}{4}\)`

Back-transform to original scale (*geometric mean*)

`\(e^\bar{x_c}\)` = `\((y_{c1} \times y_{c2} \times y_{c3} \times y_{c4})^\frac{1}{4}\)`

---

## Interpreting results

Difference between two group means (C and P) `\(\bar{x_p} - \bar{x_c}\)`
  
Back-transform to original scale `\(e^{\bar{x_p} - \bar{x_c}} = \frac{e^{\bar{x_p}}}{e^\bar{x_c}}\)` (*ratio of two geometric means*)

 + Ratio of 1 implies that the average abundance of protein X in P group, relative to C group, is the same
 
 + If however, the ratio were 1.5 then the average abundance of protein X in P group is 50% higher than in the C group
 

---
 
 ## Interpreting results
 

```r
knitr::kable(after)
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; diff &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Difference &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; SED &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; LSD &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; lowerLimit &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; upperLimit &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Max.SED &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; C-P &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.4900 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.5801856 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.31247 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.80247 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.82247 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.4202 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Min.SED &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; C-S &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -3.0875 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.5801856 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.31247 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -4.39997 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.77503 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0005 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Aveg.SED &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; P-S &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -2.5975 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.5801856 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.31247 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -3.90997 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.28503 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0015 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;table class="table" style="font-size: 18px; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Log-scale &lt;/th&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Original scale &lt;/th&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Comparison &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Estimated Difference &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Standard Error &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Ratio &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 95% CI &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; C â€“ P &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -0.4900 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.5802 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.61 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; (0.16, 2.27) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; C â€“ S &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -3.0875 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.5802 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.05 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; (0.01 0.17) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; P â€“ S &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -2.5975 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.5802 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.07 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; (0.02, 0.28) &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---
class: inverse, middle, center

## Errors in hypothesis testing and the multiple testing problem


---
##  Example: Proteome of lymph 

### There are 3 group means

+ Which pairs, if any, are different from one another?
+ Carry out multiple comparisons/hypothesis tests
	
	
     + `\(H_{01}\)`: `\(\mu_C\)` = `\(\mu_P\)`
	   + `\(H_{02}\)`:  `\(\mu_C\)` = `\(\mu_S\)`
	   + `\(H_{03}\)`:  `\(\mu_P\)` = `\(\mu_S\)`

---
##  Statistical hypothesis testing 

Two competing claims, or hypotheses
+ Assertions about the true value of some population characteristic.
	+ e.g., population mean, `\(\mu\)`

+ Claim 1: **Null**  hypothesis, `\(H_0\)`  (no difference)
versus
+ Claim 2: **Alternative**  hypothesis, `\(H_A\)`  (there is a difference)
	+ Usually the claim the researcher wants to validate

---

## For any single hypothesis: Type I error
+ Declare a difference (i.e. reject `\(H_0\)` ) when there is no difference (i.e. `\(H_0\)` is true)
	+ Also known as a false positive or false discovery
	+ Risk of the Type I error is determined by the â€œlevel of significanceâ€, i.e.
	+ `\(\alpha\)` = Pr (Type I error) = Pr (false discovery)
	+ We choose this!
		+ `\(\alpha\)` = 0.05 is a popular choice by biologists ðŸ˜± âš ï¸ ðŸ˜±
---
class: inverse

![](https://raw.githubusercontent.com/allisonhorst/stats-illustrations/master/other-stats-artwork/type_1_errors.png)
---
##  Type II error 

+ Difference not declared (i.e. `\(H_0\)` not  rejected) when there is a difference (i.e. `\(H_0\)` is false)
	+ Also known as a false negative
	+  `\(\beta\)` = Pr (do not reject `\(H_0\)` when `\(H_0\)` is false)
	+ So, what does 1 âˆ’ `\(\beta\)` tell us?
1 âˆ’ `\(\beta\)` 	= Pr (reject `\(H_0\)` when `\(H_0\)` is false)
= Pr (a true positive)
+ This is the statistical power of the test!!

---
class: inverse

![](https://raw.githubusercontent.com/allisonhorst/stats-illustrations/master/other-stats-artwork/type_2_errors.png) 
---
##  Relationship between Type I and Type II errors 

+ Reducing the chance of a Type I error increases the chance of a Type II error
	+ They are inversely related
	+ Type II error rate is determined by a combination of:
		+ Size of difference (of biological significance) between the true population means
		+ Experimental error variance
		+ Sample size, and
		+ Choice of Type I error rate ($\alpha$)

---
class: inverse

## Power and significance

**Type I** error (false positive): declare a difference (i.e., reject `\(H_0\)`) when there is no difference (i.e. `\(H_0\)` is true). Risk of the Type I error is determined by the *level of significance* (which we set!) (i.e., `\(\alpha =\text{ P(Type I error)} = \text{P(false positive)}\)`.

**Type II** error (false negative): difference not declared (i.e., `\(H_0\)` not rejected) when there is a difference (i.e., `\(H_0\)` is false). Let `\(\beta =\)` P(do not reject `\(H_0\)` when `\(H_0\)` is false); so, `\(1-\beta\)` = P(reject `\(H_0\)` when `\(H_0\)` is false) = P(a true positive), which is the statistical **power** of the test.

---
class: inverse

## Multiple comparisons

**Each** time we carry out a hypothesis test the probability we get a false positive result (type I error) is given by `\(\alpha\)` (the *level of significance* we choose).

When we have **multiple comparisons** to make we should then control the **Type I** error rate across the entire *family* of tests under consideration, i.e., control the Family-Wise Error Rate (FWER); this ensures that the risk of making at least one **Type I** error among the family of comparisons in the experiment is `\(\alpha\)`.

---
#  Risks associated with hypothesis testing 



|State of Nature  | Don't reject `\(H_0\)` | reject `\(H_0\)` |
|---              |---                |---            |
| `\(H_0\)` is true |  âœ… | Type I error  |
| `\(H_0\)` is false  | Type II error  | âœ… |

---

### Simulate the situation

![](/home/charlotte/Git/BIOSCI738/module2/index_files/figure-html/t-test1-1.png)&lt;!-- --&gt;

---

### Simulate the situation

![](/home/charlotte/Git/BIOSCI738/module2/index_files/figure-html/t-test2-1.png)&lt;!-- --&gt;


---

### Simulate the situation

![](/home/charlotte/Git/BIOSCI738/module2/index_files/figure-html/t-test3-1.png)&lt;!-- --&gt;

### Simulate the situation

![](/home/charlotte/Git/BIOSCI738/module2/index_files/figure-html/t-test4-1.png)&lt;!-- --&gt;

---


```r
ggpubr::compare_means(val ~ group, data = sim, method = "t.test")
```

```
## Registered S3 methods overwritten by 'car':
##   method                          from
##   influence.merMod                lme4
##   cooks.distance.influence.merMod lme4
##   dfbeta.influence.merMod         lme4
##   dfbetas.influence.merMod        lme4
```

```
## # A tibble: 1 x 8
##   .y.   group1 group2       p  p.adj p.format p.signif method
##   &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; 
## 1 val   A      B      0.00768 0.0077 0.0077   **       T-test
```

---


![](/home/charlotte/Git/BIOSCI738/module2/index_files/figure-html/between-1.png)&lt;!-- --&gt;

---

class: inverse, center, middle

## p-value indicates we have evidence to reject the null hupothesis at the 0.1% level

--

## But the samples were taken from the **SAME** population

---
![](/home/charlotte/Git/BIOSCI738/module2/index_files/figure-html/t-test5-1.png)&lt;!-- --&gt;
---

### Pairwise comparison of 3 groups

What's the error rate in 1000 trials? i.e. How often is a difference wrongly declared?


```r
pval &lt;- matrix(numeric(3000), ncol = 3)
for (i in 1:nrow(pval)){
   sim &lt;- data.frame(group = rep(c("A","B","C"), each = 10), 
                    val = c(sample(pop,10, replace = FALSE),
                            sample(pop,10, replace = FALSE),
                            sample(pop,10, replace = FALSE)))
   test01 &lt;- t.test(val ~ group, data = sim[sim$group != "A",])$p.value
   test02 &lt;- t.test(val ~ group, data = sim[sim$group != "B",])$p.value
   test03 &lt;- t.test(val ~ group, data = sim[sim$group != "C",])$p.value
   pval[i,] &lt;- c(test01,test02,test03)
}
```

---

```r
p &lt;- c(pval) &lt;= 0.05 ## when do they fail?
prop.table(table(p))
```

```
## p
##      FALSE       TRUE 
## 0.95133333 0.04866667
```

```r
p &lt;- c(pval) &lt;= 0.01 ## when do they fail?
prop.table(table(p))
```

```
## p
##       FALSE        TRUE 
## 0.991666667 0.008333333
```

```r
p &lt;- c(pval) &lt;= 0.2 ## when do they fail?
prop.table(table(p))
```

```
## p
##     FALSE      TRUE 
## 0.7983333 0.2016667
```

---

## Probability of commiting no Type I errors

`\((1 - \alpha)^m\)` where `\(m\)` is the number of independent tests


```r
(1 - 0.05)^3
```

```
## [1] 0.857375
```

---

FWER = `\(\alpha_F\)` `\(1 - (1 - \alpha)^m\)` where `\(m\)` is the number of independent tests

Probability of committing at least one Type I error

1 minus the probability of committing no Type I errors


```r
1 - (1 - 0.05)^3
```

```
## [1] 0.142625
```

---

PCER when `\(\alpha_F\)` is fixed

We need to find the PCER when the FWER is `\(\alpha_F\)`


```r
1 - (1 - 0.05)^1/3
```

```
## [1] 0.6833333
```

---


```
## Loading required package: patchwork
```

![](/home/charlotte/Git/BIOSCI738/module2/index_files/figure-html/alp-1.png)&lt;!-- --&gt;


---

### So what can we do?

+ Control the Type I error rate across the
entire â€œfamilyâ€ of tests u```{r type 1}nder consideration.
Familywise error rate

+ The risk of making at least one Type I error
among the family of comparisons in the
experiment

+ Also known as the *experimentwise* error
rate

---

### Familywise error rate (FWER)

â€¢ Suppose we conduct m independent t-tests
â€¢ Assume H 0 is true for all m tests
â€¢ For any single test, let...
Pr(commit a Type 1 error) = `\(\alpha\)` C
â€“ Known as the per comparison error rate (PCER)
â€¢ Whatâ€™s the probability a correct decision is
made?
Pr(do not commit a Type 1 error) = 1 âˆ’ ï¡ C
---
---
#  Familywise error rate (FWER) 

+ Probability of committing no Type I errors
(assuming m  independent  t -tests)
+ Probability of committing at least one Type I error
	+ 1 minus the probability of committing no Type I errors
+ denotes the upper limit of the FWER, i.e.
FWER â‰¤
+ We need to find the PCER when the FWER is

---
#  Familywise error rate (FWER) 

+ If we set FWER = , from previous slide:
(assuming m  independent  t -tests)
+ We need to make the subject in the above equation, i.e.

---
#  Familywise error rate (FWER) 

Why is ï¡ F the upper limit?
+ We assumed the tests are independent!
	+ Are the tests independent of one another?
	+ Consider again the CRD


---
#  Familywise error rate (FWER) 

Why is ï¡ F the upper limit?
+ There are three pairwise comparisons
	+ Null hypotheses: ï­ P â€’ ï­ C = 0; ï­ S  â€’ ï­ C  = 0; ï­ P  â€’ ï­ S = 0
	+ What are the t -statistics for testing these null hypotheses?
	+ Letâ€™s look at one of these, e.g. H 0 : ï­ P â€’ ï­ C = 0
where is the SED of .

---
#  Familywise error rate (FWER) 

Why is ï¡ F the upper limit?
+ Consider the for the other hypothesis tests
	+ Do any of them share information?
		+ Some of the t -statistics share the same sample mean in their numerator; Sham vs Control, Sham vs Pancreatitis
		+ They all share the s 2 (from the ANOVAâ€™s Residual MS)
	+ Letâ€™s look at what happens to:
		+ the FWER, ï¡ F , when fix the PCER, ï¡ C ,  at 0.05, 0.01 and 0.10
		+ the PCER, ï¡ C  , when fix the FWER, ï¡ F ,  at 0.05, 0.01 and 0.10

---
#  ï¡F when ï¡C = 0.01, 0.05, 0.10  

Each test conducted with a PCER of ï¡ C = 0.01, 0.05 , 0.10 .
Risk of at least one Type I error escalates as  the number of tests increases.
![](assets/img/image37.emf)

---
#  ï¡C when ï¡F = 0.01, 0.05, 0.10  

Each test conducted with an FWER of ï¡ F = 0.01, 0.05 , 0.10 .
When we want to keep the risk of committing at least one Type I error fixed at ï¡ F = 0.05 , the PCER for m = 10 tests is 0.005 .
So what can we do?
![](assets/img/image38.emf)

---
#  Adjustments for multiple testing 

Fisherâ€™s, Least Significant Difference, LSD
Bonferroni correction, i.e.
where = number of pairwise comparisons
Multiple comparison procedures
Tukeyâ€™s Honest Significant Difference (HSD)
Dunnettâ€™s test
Lots more
False Discovery Rate (FDR)

---
#  Fisherâ€™s LSD 

The idea:
Carry out post-hoc tests only if the ANOVA omnibus F -test is significant
Declare significant 100x Î± % any pairwise difference &gt; LSD
Note: If Î± = 0.5, then 100 x Î± % = 5%
Does not control the FWER

---
#  Bonferroni correction 

We reject all null hypotheses for which the p-value
where
= FWER
= number of pairwise comparisons
Extremely general and simple , but often not powerful
It makes no assumptions about independence between tests
This means is invalid if this assumption is violated

---
#  Bonferroni inequality 

PCER, ï¡ C = 0.05 (Fisherâ€™s LSD)
PCER, ï¡ B  = ï¡ C  /3 = 0.05/3 = 0.0167 ( Bonferroni correction)
Since we have only 3 means, and so only 3 comparisons division of PCER by 3
Does not change any of our conclusions â€“ width of CIâ€™s does not increase very much
.pull-left[![](assets/img/image39.png)]

.pull-right[![](assets/img/image41.png)]

---
#  Tukeyâ€™s Honest Significant Difference (HSD) 

+ Compares the mean of every treatment with the mean of every other treatment
	+ CRD example:
H 01 : ï­ P âˆ’ ï­ c = 0
H 02 : ï­ P âˆ’ ï­ S  = 0
H 03 : ï­ S âˆ’ ï­ c  = 0
+ Uses Studentized range distribution, compared with t -distribution for Fisherâ€™s LSD and Bonferroni correction:

---
#  Tukeyâ€™s Honest Significant Difference (HSD) 

+ Idea: If we pick out the tallest and shortest members of a class, they may appear to have significantly different heights, but in fact they are from the same population
	+ Somebody has to be the most extreme
	+ Tukeyâ€™s Honest Significant Difference (HSD ) adjusts for this
	+ Looks at the range, Biggest â€“ Smallest, of a set of means
	+ Could these have come from the same underlying distribution?

---
#  Tukeyâ€™s Honest Significant Difference (HSD) 

+ Uses Tukeyâ€™s  Studentised Range
where m is the is ï¡-level critical value for m 	means.
$`Pairwise p-value`
C       P       S
C 0.0000 -0.8446 -5.3216
P 0.6863  0.0000 -4.4770
S 0.0012  0.0039  0.0000
"The matrix has t-value above the diagonal,
p-value (adjusted by â€˜ tukey â€™ method) below the diagonal"

---
#  Tukeyâ€™s Honest Significant Difference (HSD) 



---
#  Bonferroni vs Tukeyâ€™s HSD 

+ As the number of means, m, increases
	+ The number of pairs of means m(m-1)/2 = increases very rapidly
	+ Tukeyâ€™s HSD does not increase so rapidly
	+ Suppose SED = 1, PCER a = 0.05

&lt;table class="table" style="font-size: 18px; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; m &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Comparisons &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Bonferroni &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Tukey HSD &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1.96 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1.96 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.39 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.35 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 10 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.81 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.73 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 10 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 45 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3.26 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3.17 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---


## PDQR R-function

![](https://raw.githubusercontent.com/cmjt/statbiscuits/master/figs_n_gifs/pdqr.png)


---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"slideNumberFormat": "<div class=\"progress-bar-container\">\n  <div class=\"progress-bar\" style=\"width: calc(%current% / %total% * 100%);\">\n  </div>\n</div>\n"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
